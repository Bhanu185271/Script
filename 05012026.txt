k
        print("ERROR:", traceback.format_exc())
        return jsonify({"error": f"Internal error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR

@app.route("/d_ffn_insight_config_key_obj", methods=["POST"])
@cross_origin()
def insight_key_obj_post():
    try:
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing Username"}), status.HTTP_400_BAD_REQUEST

        page = int(request.args.get("page", 1))
        limit = int(request.args.get("limit", 50))
        offset = (page - 1) * limit

        valid_markets_brands = _get_valid_markets_brands(force_refresh=True)
        valid_market_set = {m.upper() for m in valid_markets_brands.get("markets", [])}
        valid_brand_set = {b.upper() for b in valid_markets_brands.get("brands", [])}

        rows = []
        is_excel_mode = False
        validation_errors = []

        if "file" in request.files:
            is_excel_mode = True
            f = request.files["file"]
            
            if not (f.filename or "").lower().endswith(".xlsx"):
                return jsonify({"error": "Only .xlsx files"}), status.HTTP_400_BAD_REQUEST

            try:
                rows, _ = _process_template_excel_key_obj(f)
            except Exception as e:
                return jsonify({"error": str(e)}), status.HTTP_400_BAD_REQUEST
        else:
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand = _clean(rec.get("BRAND", ""))
                objection = _clean(rec.get("OBJECTION", ""))

                if not market or not brand or not objection:
                    return jsonify({"error": f"Item {i}: Missing required fields (MARKET, BRAND, OBJECTION)"}), status.HTTP_400_BAD_REQUEST

                rows.append((i, {
                    "MARKET": market,
                    "BRAND": brand,
                    "OBJECTION_GROUP": _clean(rec.get("OBJECTION_GROUP", "")),
                    "OBJECTION": objection,
                    "ACKNOWLEDGE": _clean(rec.get("ACKNOWLEDGE", "")),
                    "PROBE": _clean(rec.get("PROBE", "")),
                    "ANSWER": _clean(rec.get("ANSWER", "")),
                    "CONFIRMATION": _clean(rec.get("CONFIRMATION", "")),
                    "TRANSITION": _clean(rec.get("TRANSITION", ""))
                }))

        if not rows:
            return jsonify({"error": "No rows"}), status.HTTP_400_BAD_REQUEST

        # Row-by-row validation - don't break execution
        validated_rows = []
        for r in rows:
            row_num, row_data = r
            market_upper = row_data["MARKET"].upper()
            brand_upper = row_data["BRAND"].upper()
            
            # Check if market is valid
            if market_upper not in valid_market_set:
                validation_errors.append({
                    "excel_row": row_num,
                    "MARKET": row_data["MARKET"],
                    "BRAND": row_data["BRAND"],
                    "reason": f"Invalid MARKET: '{row_data['MARKET']}'"
                })
                continue
            
            # Check if brand is valid
            if brand_upper not in valid_brand_set:
                validation_errors.append({
                    "excel_row": row_num,
                    "MARKET": row_data["MARKET"],
                    "BRAND": row_data["BRAND"],
                    "reason": f"Invalid BRAND: '{row_data['BRAND']}'"
                })
                continue
            
            # If validation passed, add to validated rows
            validated_rows.append((row_num, row_data))

        # If no valid rows after validation, return error
        if not validated_rows:
            return jsonify({
                "status": "error",
                "message": "No valid rows to process after validation",
                "validation_errors": validation_errors
            }), status.HTTP_400_BAD_REQUEST

        # batch duplicate check for validated rows only
        check_vals = []
        for r in validated_rows:
            row_data = r[1]
            check_vals.append(
                f"(UPPER(TRIM(MARKET)) = '{_sq(row_data['MARKET'].upper())}' AND "
                f"UPPER(TRIM(BRAND)) = '{_sq(row_data['BRAND'].upper())}' AND "
                f"TRIM(OBJECTION) = '{_sq(row_data['OBJECTION'])}')"
            )

        check_query = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET, UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(OBJECTION) as OBJECTION
            FROM {INSIGHT_KEY_OBJ_TABLE}
            WHERE {' OR '.join(check_vals)}
        """
        
        existing = dc.execute_query(check_query)
        existing_set = set()
        if existing is not None and not existing.empty:
            for _, row in existing.iterrows():
                existing_set.add((row['MARKET'], row['BRAND'], row['OBJECTION']))

        inserted_rows, duplicates, errors = [], [], []
        batch_inserts = []

        for r in validated_rows:
            try:
                row_num, row_data = r
                market = row_data["MARKET"].upper()
                brand = row_data["BRAND"].upper()
                
                key = (market, brand, row_data["OBJECTION"])

                if key in existing_set:
                    duplicates.append({
                        "excel_row": row_num,
                        "MARKET": market,
                        "BRAND": brand,
                        "reason": "Duplicate entry"
                    })
                    continue

                obj_grp_val = "NULL" if not row_data.get("OBJECTION_GROUP") else f"'{_sq(row_data['OBJECTION_GROUP'])}'"
                ack_val = "NULL" if not row_data.get("ACKNOWLEDGE") else f"'{_sq(row_data['ACKNOWLEDGE'])}'"
                probe_val = "NULL" if not row_data.get("PROBE") else f"'{_sq(row_data['PROBE'])}'"
                ans_val = "NULL" if not row_data.get("ANSWER") else f"'{_sq(row_data['ANSWER'])}'"
                conf_val = "NULL" if not row_data.get("CONFIRMATION") else f"'{_sq(row_data['CONFIRMATION'])}'"
                trans_val = "NULL" if not row_data.get("TRANSITION") else f"'{_sq(row_data['TRANSITION'])}'"

                batch_inserts.append(
                    f"('{_sq(market)}', '{_sq(brand)}', {obj_grp_val}, '{_sq(row_data['OBJECTION'])}', "
                    f"{ack_val}, {probe_val}, {ans_val}, {conf_val}, {trans_val}, '{_sq(added_by)}', CURRENT_TIMESTAMP())"
                )

                inserted_rows.append({"excel_row": row_num, "MARKET": market, "BRAND": brand})

            except Exception as e:
                errors.append({"excel_row": row_num, "error": str(e)})

        # batch insert
        if batch_inserts:
            insert_query = f"""
                INSERT INTO {INSIGHT_KEY_OBJ_TABLE}
                (MARKET, BRAND, OBJECTION_GROUP, OBJECTION, ACKNOWLEDGE, PROBE, ANSWER, CONFIRMATION, TRANSITION, ADDED_BY, DATE_ADDED)
                VALUES {','.join(batch_inserts)}
            """
            dc.execute_non_query(insert_query)

        # pagination
        total_rows = len(rows)
        total_pages = math.ceil(total_rows / limit)
        start, end = offset, min(offset + limit, total_rows)

        response = {
            "status": "success" if inserted_rows else "partial" if (duplicates or validation_errors) else "no-change",
            "mode": "excel-template" if is_excel_mode else "json",
            "page": page,
            "limit": limit,
            "total_rows": total_rows,
            "total_pages": total_pages,
            "inserted_count": len(inserted_rows),
            "duplicate_count": len(duplicates),
            "validation_errors_count": len(validation_errors),
            "inserted_rows": inserted_rows[start:end],
            "duplicates": duplicates[start:end],
            "validation_errors": validation_errors[start:end],
            "errors": errors[start:end]
        }

        response = {k: v for k, v in response.items() if v is not None and not (isinstance(v, list) and len(v) == 0)}

        return jsonify(response), status.HTTP_201_CREATED if inserted_rows else status.HTTP_200_OK

    except Exception as e:
        import traceback
        print("ERROR:", traceback.format_exc())
        return jsonify({"error": f"Internal error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR

###############################################
# INSIGHT CONFIG - Brand Studies (Updated)
###############################################

# NEW TABLE for Brand Studies
INSIGHT_BRAND_STUDY_TABLE = "hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_brand_studies_config"
MARKET_BRAND_TABLE = "hive_metastore.fieldforce_navigator_deployment.audio_market_brand_config"

# UPDATED MAPPING - Only 3 columns now
EXCEL_TO_DB_MAPPING_BRAND_STUDY = {
    "Market": "MARKET",
    "Brand": "BRAND",
    "Study Name": "STUDY_NAME"
}

TARGET_SHEET_NAME_BRAND_STUDY = "Brand - Studies"

def _process_template_excel_brand_study(file_obj):
    try:
        excel_file = pd.ExcelFile(file_obj, engine="openpyxl")
        
        if TARGET_SHEET_NAME_BRAND_STUDY not in excel_file.sheet_names:
            raise ValueError(f"Sheet '{TARGET_SHEET_NAME_BRAND_STUDY}' not found")
        
        df = pd.read_excel(file_obj, sheet_name=TARGET_SHEET_NAME_BRAND_STUDY, header=HEADER_ROW_INDEX, engine="openpyxl")
        
        if len(df.columns) > 0:
            df = df.iloc[:, 1:]
        
        df.columns = [str(c).strip() for c in df.columns]
        df = df.replace({pd.NA: "", None: ""}).fillna("")
        
        df['_excel_row_num'] = range(4, 4 + len(df))
        df = df[df.drop('_excel_row_num', axis=1).astype(str).apply(lambda x: x.str.strip().str.len().sum(), axis=1) > 0]
        
        # Updated mandatory columns as per new mapping
        mandatory_cols = ["Market", "Brand", "Study Name"]
        missing = [col for col in mandatory_cols if col not in df.columns]
        
        if missing:
            raise ValueError(f"Missing columns: {', '.join(missing)}")
        
        mapped_data = []
        
        for idx, row in df.iterrows():
            excel_row_num = int(row['_excel_row_num'])
            mapped_row = {}
            
            for excel_col, db_col in EXCEL_TO_DB_MAPPING_BRAND_STUDY.items():
                if excel_col in df.columns:
                    mapped_row[db_col] = _clean(row.get(excel_col, ""))
                else:
                    mapped_row[db_col] = ""
            
            # Check if all required fields are present
            if mapped_row.get("MARKET") and mapped_row.get("BRAND") and mapped_row.get("STUDY_NAME"):
                mapped_data.append((excel_row_num, mapped_row))
        
        return mapped_data, None
        
    except Exception as e:
        raise Exception(f"Error processing Excel: {str(e)}")


###############################################
# ENDPOINT 1: RETRIEVE DATA (GET-like using POST)
# Purpose: Fetch existing brand study records filtered by MARKET and BRAND
# Method: POST
# URL: /d_ffn_insight_config_brand_study/get_data
# Payload: {"MARKET": "...", "BRAND": "..."}  (both optional)
###############################################
@app.route("/d_ffn_insight_config_brand_study/get_data", methods=["POST"])
@cross_origin()
def insight_brand_study_get_data():
    """
    Retrieves brand study data from the database based on MARKET and BRAND filters.
    Both parameters are OPTIONAL in the POST body.
    
    Example POST body:
    {
        "MARKET": "GBR",
        "BRAND": "AREXVY"
    }
    
    Or just one filter:
    {
        "BRAND": "AREXVY"
    }
    
    Or no filters (returns all data):
    {}
    """
    try:
        # Get brand and market from POST request body
        body = request.get_json(force=True, silent=True) or {}
        brand_param = _clean(body.get("BRAND", ""))
        market_param = _clean(body.get("MARKET", ""))
        
        # Validate BRAND and MARKET against master table (only if provided)
        valid_markets_brands = _get_valid_markets_brands(force_refresh=True)
        
        brand_upper = brand_param.upper() if brand_param else ""
        market_upper = market_param.upper() if market_param else ""
        
        valid_market_set = {m.upper() for m in valid_markets_brands.get("markets", [])}
        valid_brand_set = {b.upper() for b in valid_markets_brands.get("brands", [])}
        
        # Only validate if parameters are provided
        if brand_param and brand_upper not in valid_brand_set:
            return jsonify({"error": f"Invalid BRAND: '{brand_param}' is not available in the database"}), status.HTTP_400_BAD_REQUEST
        
        if market_param and market_upper not in valid_market_set:
            return jsonify({"error": f"Invalid MARKET: '{market_param}' is not available in the database"}), status.HTTP_400_BAD_REQUEST
        
        # Build query with optional WHERE clause based on parameters
        query = f"SELECT * FROM {INSIGHT_BRAND_STUDY_TABLE}"
        where_conditions = []
        
        if brand_param:
            where_conditions.append(f"UPPER(TRIM(BRAND)) = '{_sq(brand_upper)}'")
        
        if market_param:
            where_conditions.append(f"UPPER(TRIM(MARKET)) = '{_sq(market_upper)}'")
        
        if where_conditions:
            query += " WHERE " + " AND ".join(where_conditions)
        
        print(f"Executing query: {query}")
        
        # Execute query to get all matching rows
        sample_df = dc.execute_query(query)
        
        # Initialize result array
        result = []
        
        # If data exists, convert each row to JSON with id
        if sample_df is not None and not sample_df.empty:
            for idx, row in sample_df.iterrows():
                # Create row object with id as first field using OrderedDict
                from collections import OrderedDict
                row_obj = OrderedDict()
                row_obj["id"] = str(idx + 1)
                
                # Add all columns from the table
                for col in sample_df.columns:
                    value = row[col]
                    # Convert NaN, None, or "null" to empty string
                    if pd.isna(value) or value == "" or str(value).strip().lower() == "null":
                        row_obj[col] = ""
                    else:
                        row_obj[col] = str(value)
                
                result.append(row_obj)
        
        return jsonify(result), status.HTTP_200_OK
        
    except Exception as e:
        import traceback
        print("ERROR:", traceback.format_exc())
        return jsonify({"error": f"Internal error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR


###############################################
# ENDPOINT 2: INSERT/UPLOAD DATA
# Method: POST
# URL: /d_ffn_insight_config_brand_study
# Payload: JSON object(s) with MARKET, BRAND, STUDY_NAME (all required)
###############################################
@app.route("/d_ffn_insight_config_brand_study", methods=["POST"])
@cross_origin()
def insight_brand_study_post():
    """
    Inserts new brand study data into the database.
    Requires MARKET, BRAND, and STUDY_NAME in the POST body.
    
    Example POST body for single record:
    {
        "MARKET": "US",
        "BRAND": "AREXVY",
        "STUDY_NAME": "Key Brand Study Name"
    }
    
    Or array of records:
    [
        {
            "MARKET": "US",
            "BRAND": "AREXVY",
            "STUDY_NAME": "Study 1"
        },
        {
            "MARKET": "EU",
            "BRAND": "SHINGRIX",
            "STUDY_NAME": "Study 2"
        }
    ]
    """
    try:
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing Username"}), status.HTTP_400_BAD_REQUEST

        page = int(request.args.get("page", 1))
        limit = int(request.args.get("limit", 50))
        offset = (page - 1) * limit

        valid_markets_brands = _get_valid_markets_brands(force_refresh=True)

        rows = []
        is_excel_mode = False

        # Check if file upload (Excel) or JSON body
        if "file" in request.files:
            is_excel_mode = True
            f = request.files["file"]
            
            if not (f.filename or "").lower().endswith(".xlsx"):
                return jsonify({"error": "Only .xlsx files"}), status.HTTP_400_BAD_REQUEST

            try:
                rows, _ = _process_template_excel_brand_study(f)
            except Exception as e:
                return jsonify({"error": str(e)}), status.HTTP_400_BAD_REQUEST
        else:
            # JSON body mode - Requires MARKET, BRAND, and STUDY_NAME
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand = _clean(rec.get("BRAND", ""))
                study_name = _clean(rec.get("STUDY_NAME", ""))

                # Validate all three required fields
                if not market or not brand or not study_name:
                    return jsonify({"error": f"Item {i}: Missing required fields (MARKET, BRAND, STUDY_NAME)"}), status.HTTP_400_BAD_REQUEST

                rows.append((i, {
                    "MARKET": market,
                    "BRAND": brand,
                    "STUDY_NAME": study_name
                }))

        if not rows:
            return jsonify({"error": "No rows"}), status.HTTP_400_BAD_REQUEST

        # Vectorized validation
        all_markets = {r[1]["MARKET"].upper() for r in rows}
        all_brands = {r[1]["BRAND"].upper() for r in rows}

        valid_market_set = {m.upper() for m in valid_markets_brands.get("markets", [])}
        valid_brand_set = {b.upper() for b in valid_markets_brands.get("brands", [])}

        invalid_mkts = [{"MARKET": m, "reason": "Invalid MARKET"} for m in all_markets if m not in valid_market_set]
        invalid_brnds = [{"BRAND": b, "reason": "Invalid BRAND"} for b in all_brands if b not in valid_brand_set]

        if invalid_mkts or invalid_brnds:
            return jsonify({"status": "error", "invalid_markets": invalid_mkts, "invalid_brands": invalid_brnds}), status.HTTP_400_BAD_REQUEST

        # Batch duplicate check
        check_vals = []
        for r in rows:
            row_data = r[1]
            check_vals.append(
                f"(UPPER(TRIM(MARKET)) = '{_sq(row_data['MARKET'].upper())}' AND "
                f"UPPER(TRIM(BRAND)) = '{_sq(row_data['BRAND'].upper())}' AND "
                f"TRIM(STUDY_NAME) = '{_sq(row_data['STUDY_NAME'])}')"
            )

        check_query = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET, UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(STUDY_NAME) as STUDY_NAME
            FROM {INSIGHT_BRAND_STUDY_TABLE}
            WHERE {' OR '.join(check_vals)}
        """
        
        existing = dc.execute_query(check_query)
        existing_set = set()
        if existing is not None and not existing.empty:
            for _, row in existing.iterrows():
                existing_set.add((row['MARKET'], row['BRAND'], row['STUDY_NAME']))

        inserted_rows, duplicates, errors = [], [], []
        batch_inserts = []

        for r in rows:
            try:
                row_num, row_data = r
                market = row_data["MARKET"].upper()
                brand = row_data["BRAND"].upper()
                
                key = (market, brand, row_data["STUDY_NAME"])

                if key in existing_set:
                    duplicates.append({"excel_row": row_num, "MARKET": market, "BRAND": brand})
                    continue

                # Build insert values - only MARKET, BRAND, STUDY_NAME, and DATE_ADDED
                batch_inserts.append(
                    f"('{_sq(market)}', '{_sq(brand)}', '{_sq(row_data['STUDY_NAME'])}', '{_sq(added_by)}',  CURRENT_TIMESTAMP())"
                )

                inserted_rows.append({"excel_row": row_num, "MARKET": market, "BRAND": brand})

            except Exception as e:
                errors.append({"excel_row": row_num, "error": str(e)})

        # Batch insert
        if batch_inserts:
            # Updated insert query with new table and columns
            insert_query = f"""
                INSERT INTO {INSIGHT_BRAND_STUDY_TABLE}
                (MARKET, BRAND, STUDY_NAME, ADDED_BY, DATE_ADDED)
                VALUES {','.join(batch_inserts)}
            """
            dc.execute_non_query(insert_query)

        # Pagination
        total_rows = len(rows)
        total_pages = math.ceil(total_rows / limit)
        start, end = offset, min(offset + limit, total_rows)

        response = {
            "status": "success" if inserted_rows else "no-change",
            "mode": "excel-template" if is_excel_mode else "json",
            "page": page,
            "limit": limit,
            "total_rows": total_rows,
            "total_pages": total_pages,
            "inserted_count": len(inserted_rows),
            "duplicate_count": len(duplicates),
            "inserted_rows": inserted_rows[start:end],
            "duplicates": duplicates[start:end],
            "errors": errors[start:end]
        }

        response = {k: v for k, v in response.items() if v is not None and not (isinstance(v, list) and len(v) == 0)}

        return jsonify(response), status.HTTP_201_CREATED if inserted_rows else status.HTTP_200_OK

    except Exception as e:
        import traceback
        print("ERROR:", traceback.format_exc())
        return jsonify({"error": f"Internal error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR


        
###############################################
# INSIGHT CONFIG - Medical Terms (Updated)
###############################################

# NEW TABLE for Medical Terms
INSIGHT_MEDICAL_TERM_TABLE = "hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_medical_term_config"
MARKET_BRAND_TABLE = "hive_metastore.fieldforce_navigator_deployment.audio_market_brand_config"

# UPDATED MAPPING - Only 3 columns now
EXCEL_TO_DB_MAPPING_MEDICAL_TERM = {
    "MARKET": "MARKET",
    "BRAND": "BRAND",
    "Term": "TERM"
}

TARGET_SHEET_NAME_MEDICAL_TERM = "Brand - Medical Terms"

def _process_template_excel_medical_term(file_obj):
    try:
        excel_file = pd.ExcelFile(file_obj, engine="openpyxl")
        
        if TARGET_SHEET_NAME_MEDICAL_TERM not in excel_file.sheet_names:
            raise ValueError(f"Sheet '{TARGET_SHEET_NAME_MEDICAL_TERM}' not found")
        
        df = pd.read_excel(file_obj, sheet_name=TARGET_SHEET_NAME_MEDICAL_TERM, header=HEADER_ROW_INDEX, engine="openpyxl")
        
        if len(df.columns) > 0:
            df = df.iloc[:, 1:]
        
        df.columns = [str(c).strip() for c in df.columns]
        df = df.replace({pd.NA: "", None: ""}).fillna("")
        
        df['_excel_row_num'] = range(4, 4 + len(df))
        df = df[df.drop('_excel_row_num', axis=1).astype(str).apply(lambda x: x.str.strip().str.len().sum(), axis=1) > 0]
        
        # Updated mandatory columns as per new mapping
        mandatory_cols = ["MARKET", "BRAND", "Term"]
        missing = [col for col in mandatory_cols if col not in df.columns]
        
        if missing:
            raise ValueError(f"Missing columns: {', '.join(missing)}")
        
        mapped_data = []
        
        for idx, row in df.iterrows():
            excel_row_num = int(row['_excel_row_num'])
            mapped_row = {}
            
            for excel_col, db_col in EXCEL_TO_DB_MAPPING_MEDICAL_TERM.items():
                if excel_col in df.columns:
                    mapped_row[db_col] = _clean(row.get(excel_col, ""))
                else:
                    mapped_row[db_col] = ""
            
            # Check if all required fields are present
            if mapped_row.get("MARKET") and mapped_row.get("BRAND") and mapped_row.get("TERM"):
                mapped_data.append((excel_row_num, mapped_row))
        
        return mapped_data, None
        
    except Exception as e:
        raise Exception(f"Error processing Excel: {str(e)}")


###############################################
# ENDPOINT 1: RETRIEVE DATA 
# Purpose: Fetch existing medical term records filtered by MARKET and BRAND
# Method: POST
# URL: /d_ffn_insight_config_medical_term/get_data
# Payload: {"MARKET": "...", "BRAND": "..."}  
###############################################
@app.route("/d_ffn_insight_config_medical_term/get_data", methods=["POST"])
@cross_origin()
def insight_medical_term_get_data():
    """
    Retrieves medical term data from the database based on MARKET and BRAND filters.
    Both parameters are OPTIONAL in the POST body.
    
    Example POST body:
    {
        "MARKET": "US",
        "BRAND": "AREXVY"
    }
    
    Or just one filter:
    {
        "BRAND": "AREXVY"
    }
    
    Or no filters (returns all data):
    {}
    """
    try:
        # Get brand and market from POST request body
        body = request.get_json(force=True, silent=True) or {}
        brand_param = _clean(body.get("BRAND", ""))
        market_param = _clean(body.get("MARKET", ""))
        
        # Validate BRAND and MARKET against master table (only if provided)
        valid_markets_brands = _get_valid_markets_brands(force_refresh=True)
        
        brand_upper = brand_param.upper() if brand_param else ""
        market_upper = market_param.upper() if market_param else ""
        
        valid_market_set = {m.upper() for m in valid_markets_brands.get("markets", [])}
        valid_brand_set = {b.upper() for b in valid_markets_brands.get("brands", [])}
        
        # Only validate if parameters are provided
        if brand_param and brand_upper not in valid_brand_set:
            return jsonify({"error": f"Invalid BRAND: '{brand_param}' is not available in the database"}), status.HTTP_400_BAD_REQUEST
        
        if market_param and market_upper not in valid_market_set:
            return jsonify({"error": f"Invalid MARKET: '{market_param}' is not available in the database"}), status.HTTP_400_BAD_REQUEST
        
        # Build query with optional WHERE clause based on parameters
        query = f"SELECT * FROM {INSIGHT_MEDICAL_TERM_TABLE}"
        where_conditions = []
        
        if brand_param:
            where_conditions.append(f"UPPER(TRIM(BRAND)) = '{_sq(brand_upper)}'")
        
        if market_param:
            where_conditions.append(f"UPPER(TRIM(MARKET)) = '{_sq(market_upper)}'")
        
        if where_conditions:
            query += " WHERE " + " AND ".join(where_conditions)
        
        print(f"Executing query: {query}")
        
        # Execute query to get all matching rows
        sample_df = dc.execute_query(query)
        
        # Initialize result array
        result = []
        
        # If data exists, convert each row to JSON with id
        if sample_df is not None and not sample_df.empty:
            for idx, row in sample_df.iterrows():
                # Create row object with id as first field using OrderedDict
                from collections import OrderedDict
                row_obj = OrderedDict()
                row_obj["id"] = str(idx + 1)
                
                # Add all columns from the table
                for col in sample_df.columns:
                    value = row[col]
                    # Convert NaN, None, or "null" to empty string
                    if pd.isna(value) or value == "" or str(value).strip().lower() == "null":
                        row_obj[col] = ""
                    else:
                        row_obj[col] = str(value)
                
                result.append(row_obj)
        
        return jsonify(result), status.HTTP_200_OK
        
    except Exception as e:
        import traceback
        print("ERROR:", traceback.format_exc())
        return jsonify({"error": f"Internal error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR


###############################################
# ENDPOINT 2: INSERT/UPLOAD DATA
# Purpose: Add new medical term records to the database
# Method: POST
# URL: /d_ffn_insight_config_medical_term
# Payload: JSON object(s) with MARKET, BRAND, TERM (all required)
###############################################
@app.route("/d_ffn_insight_config_medical_term", methods=["POST"])
@cross_origin()
def insight_medical_term_post():
    """
    Inserts new medical term data into the database.
    Requires MARKET, BRAND, and TERM in the POST body.
    
    Example POST body for single record:
    {
        "MARKET": "US",
        "BRAND": "AREXVY",
        "TERM": "Medical Term"
    }
    
    Or array of records:
    [
        {
            "MARKET": "US",
            "BRAND": "AREXVY",
            "TERM": "Term 1"
        },
        {
            "MARKET": "EU",
            "BRAND": "SHINGRIX",
            "TERM": "Term 2"
        }
    ]
    """
    try:
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing Username"}), status.HTTP_400_BAD_REQUEST

        page = int(request.args.get("page", 1))
        limit = int(request.args.get("limit", 50))
        offset = (page - 1) * limit

        valid_markets_brands = _get_valid_markets_brands(force_refresh=True)

        rows = []
        is_excel_mode = False

        # Check if file upload (Excel) or JSON body
        if "file" in request.files:
            is_excel_mode = True
            f = request.files["file"]
            
            if not (f.filename or "").lower().endswith(".xlsx"):
                return jsonify({"error": "Only .xlsx files"}), status.HTTP_400_BAD_REQUEST

            try:
                rows, _ = _process_template_excel_medical_term(f)
            except Exception as e:
                return jsonify({"error": str(e)}), status.HTTP_400_BAD_REQUEST
        else:
            # JSON body mode - Requires MARKET, BRAND, and TERM
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand = _clean(rec.get("BRAND", ""))
                term = _clean(rec.get("TERM", ""))

                # Validate all three required fields
                if not market or not brand or not term:
                    return jsonify({"error": f"Item {i}: Missing required fields (MARKET, BRAND, TERM)"}), status.HTTP_400_BAD_REQUEST

                rows.append((i, {
                    "MARKET": market,
                    "BRAND": brand,
                    "TERM": term
                }))

        if not rows:
            return jsonify({"error": "No rows"}), status.HTTP_400_BAD_REQUEST

        # Vectorized validation
        all_markets = {r[1]["MARKET"].upper() for r in rows}
        all_brands = {r[1]["BRAND"].upper() for r in rows}

        valid_market_set = {m.upper() for m in valid_markets_brands.get("markets", [])}
        valid_brand_set = {b.upper() for b in valid_markets_brands.get("brands", [])}

        invalid_mkts = [{"MARKET": m, "reason": "Invalid MARKET"} for m in all_markets if m not in valid_market_set]
        invalid_brnds = [{"BRAND": b, "reason": "Invalid BRAND"} for b in all_brands if b not in valid_brand_set]

        if invalid_mkts or invalid_brnds:
            return jsonify({"status": "error", "invalid_markets": invalid_mkts, "invalid_brands": invalid_brnds}), status.HTTP_400_BAD_REQUEST

        # Batch duplicate check
        check_vals = []
        for r in rows:
            row_data = r[1]
            check_vals.append(
                f"(UPPER(TRIM(MARKET)) = '{_sq(row_data['MARKET'].upper())}' AND "
                f"UPPER(TRIM(BRAND)) = '{_sq(row_data['BRAND'].upper())}' AND "
                f"TRIM(TERM) = '{_sq(row_data['TERM'])}')"
            )

        check_query = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET, UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(TERM) as TERM
            FROM {INSIGHT_MEDICAL_TERM_TABLE}
            WHERE {' OR '.join(check_vals)}
        """
        
        existing = dc.execute_query(check_query)
        existing_set = set()
        if existing is not None and not existing.empty:
            for _, row in existing.iterrows():
                existing_set.add((row['MARKET'], row['BRAND'], row['TERM']))

        inserted_rows, duplicates, errors = [], [], []
        batch_inserts = []

        for r in rows:
            try:
                row_num, row_data = r
                market = row_data["MARKET"].upper()
                brand = row_data["BRAND"].upper()
                
                key = (market, brand, row_data["TERM"])

                if key in existing_set:
                    duplicates.append({"excel_row": row_num, "MARKET": market, "BRAND": brand})
                    continue

                # Build insert values - only MARKET, BRAND, TERM, and DATE_ADDED
                batch_inserts.append(
                    f"('{_sq(market)}', '{_sq(brand)}', '{_sq(row_data['TERM'])}', '{_sq(added_by)}', CURRENT_TIMESTAMP())"
                )

                inserted_rows.append({"excel_row": row_num, "MARKET": market, "BRAND": brand})

            except Exception as e:
                errors.append({"excel_row": row_num, "error": str(e)})

        # Batch insert
        if batch_inserts:
            # Updated insert query with new table and columns
            insert_query = f"""
                INSERT INTO {INSIGHT_MEDICAL_TERM_TABLE}
                (MARKET, BRAND, TERM, ADDED_BY, DATE_ADDED)
                VALUES {','.join(batch_inserts)}
            """
            dc.execute_non_query(insert_query)

        # Pagination
        total_rows = len(rows)
        total_pages = math.ceil(total_rows / limit)
        start, end = offset, min(offset + limit, total_rows)

        response = {
            "status": "success" if inserted_rows else "no-change",
            "mode": "excel-template" if is_excel_mode else "json",
            "page": page,
            "limit": limit,
            "total_rows": total_rows,
            "total_pages": total_pages,
            "inserted_count": len(inserted_rows),
            "duplicate_count": len(duplicates),
            "inserted_rows": inserted_rows[start:end],
            "duplicates": duplicates[start:end],
            "errors": errors[start:end]
        }

        response = {k: v for k, v in response.items() if v is not None and not (isinstance(v, list) and len(v) == 0)}

        return jsonify(response), status.HTTP_201_CREATED if inserted_rows else status.HTTP_200_OK

    except Exception as e:
        import traceback
        print("ERROR:", traceback.format_exc())
        return jsonify({"error": f"Internal error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR
	

###############################################
# INSIGHT CONFIG - Brand Abbreviations (Updated)
###############################################

# NEW TABLE for Brand Abbreviations
INSIGHT_BRAND_ABBRV_TABLE = "hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_brand_abbrv_config"
MARKET_BRAND_TABLE = "hive_metastore.fieldforce_navigator_deployment.audio_market_brand_config"

# UPDATED MAPPING - Now 4 columns
EXCEL_TO_DB_MAPPING_ABBRV = {
    "Market": "MARKET",
    "Brand": "BRAND",
    "Term": "TERM",
    "Definition": "DEFINITION"
}

TARGET_SHEET_NAME_ABBRV = "Brand - Acronyms"

def _process_template_excel_abbrv(file_obj):
    try:
        excel_file = pd.ExcelFile(file_obj, engine="openpyxl")
        
        if TARGET_SHEET_NAME_ABBRV not in excel_file.sheet_names:
            raise ValueError(f"Sheet '{TARGET_SHEET_NAME_ABBRV}' not found")
        
        df = pd.read_excel(file_obj, sheet_name=TARGET_SHEET_NAME_ABBRV, header=HEADER_ROW_INDEX, engine="openpyxl")
        
        if len(df.columns) > 0:
            df = df.iloc[:, 1:]
        
        df.columns = [str(c).strip() for c in df.columns]
        df = df.replace({pd.NA: "", None: ""}).fillna("")
        
        df['_excel_row_num'] = range(4, 4 + len(df))
        df = df[df.drop('_excel_row_num', axis=1).astype(str).apply(lambda x: x.str.strip().str.len().sum(), axis=1) > 0]
        
        # Updated mandatory columns as per new mapping
        mandatory_cols = ["Market", "Brand", "Term", "Definition"]
        missing = [col for col in mandatory_cols if col not in df.columns]
        
        if missing:
            raise ValueError(f"Missing columns: {', '.join(missing)}")
        
        mapped_data = []
        
        for idx, row in df.iterrows():
            excel_row_num = int(row['_excel_row_num'])
            mapped_row = {}
            
            for excel_col, db_col in EXCEL_TO_DB_MAPPING_ABBRV.items():
                if excel_col in df.columns:
                    mapped_row[db_col] = _clean(row.get(excel_col, ""))
                else:
                    mapped_row[db_col] = ""
            
            # Check if all required fields are present
            if (mapped_row.get("MARKET") and mapped_row.get("BRAND") and 
                mapped_row.get("TERM") and mapped_row.get("DEFINITION")):
                mapped_data.append((excel_row_num, mapped_row))
        
        return mapped_data, None
        
    except Exception as e:
        raise Exception(f"Error processing Excel: {str(e)}")


###############################################
# ENDPOINT 1: RETRIEVE DATA 
# Purpose: Fetch existing brand abbreviation records filtered by MARKET and BRAND
# Method: POST
# URL: /d_ffn_insight_config_abbrv/get_data
# Payload: {"MARKET": "...", "BRAND": "..."}  
###############################################
@app.route("/d_ffn_insight_config_abbrv/get_data", methods=["POST"])
@cross_origin()
def insight_abbrv_get_data():
    """
    Retrieves brand abbreviation data from the database based on MARKET and BRAND filters.
    Both parameters are OPTIONAL in the POST body.
    
    Example POST body:
    {
        "MARKET": "GBR",
        "BRAND": "AREXVY"
    }
    
    Or just one filter:
    {
        "BRAND": "AREXVY"
    }
    
    Or no filters (returns all data):
    {}
    """
    try:
        # Get brand and market from POST request body
        body = request.get_json(force=True, silent=True) or {}
        brand_param = _clean(body.get("BRAND", ""))
        market_param = _clean(body.get("MARKET", ""))
        
        # Validate BRAND and MARKET against master table (only if provided)
        valid_markets_brands = _get_valid_markets_brands(force_refresh=True)
        
        brand_upper = brand_param.upper() if brand_param else ""
        market_upper = market_param.upper() if market_param else ""
        
        valid_market_set = {m.upper() for m in valid_markets_brands.get("markets", [])}
        valid_brand_set = {b.upper() for b in valid_markets_brands.get("brands", [])}
        
        # Only validate if parameters are provided
        if brand_param and brand_upper not in valid_brand_set:
            return jsonify({"error": f"Invalid BRAND: '{brand_param}' is not available in the database"}), status.HTTP_400_BAD_REQUEST
        
        if market_param and market_upper not in valid_market_set:
            return jsonify({"error": f"Invalid MARKET: '{market_param}' is not available in the database"}), status.HTTP_400_BAD_REQUEST
        
        # Build query with optional WHERE clause based on parameters
        query = f"SELECT * FROM {INSIGHT_BRAND_ABBRV_TABLE}"
        where_conditions = []
        
        if brand_param:
            where_conditions.append(f"UPPER(TRIM(BRAND)) = '{_sq(brand_upper)}'")
        
        if market_param:
            where_conditions.append(f"UPPER(TRIM(MARKET)) = '{_sq(market_upper)}'")
        
        if where_conditions:
            query += " WHERE " + " AND ".join(where_conditions)
        
        print(f"Executing query: {query}")
        
        # Execute query to get all matching rows
        sample_df = dc.execute_query(query)
        
        # Initialize result array
        result = []
        
        # If data exists, convert each row to JSON with id
        if sample_df is not None and not sample_df.empty:
            for idx, row in sample_df.iterrows():
                # Create row object with id as first field using OrderedDict
                from collections import OrderedDict
                row_obj = OrderedDict()
                row_obj["id"] = str(idx + 1)
                
                # Add all columns from the table
                for col in sample_df.columns:
                    value = row[col]
                    # Convert NaN, None, or "null" to empty string
                    if pd.isna(value) or value == "" or str(value).strip().lower() == "null":
                        row_obj[col] = ""
                    else:
                        row_obj[col] = str(value)
                
                result.append(row_obj)
        
        return jsonify(result), status.HTTP_200_OK
        
    except Exception as e:
        import traceback
        print("ERROR:", traceback.format_exc())
        return jsonify({"error": f"Internal error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR


###############################################
# ENDPOINT 2: INSERT/UPLOAD DATA
# Purpose: Add new brand abbreviation records to the database
# Method: POST
# URL: /d_ffn_insight_config_abbrv
# Payload: JSON object(s) with MARKET, BRAND, TERM, DEFINITION (all required)
###############################################
@app.route("/d_ffn_insight_config_abbrv", methods=["POST"])
@cross_origin()
def insight_abbrv_post():
    """
    Inserts new brand abbreviation data into the database.
    Requires MARKET, BRAND, TERM, and DEFINITION in the POST body.
    
    Example POST body for single record:
    {
        "MARKET": "US",
        "BRAND": "AREXVY",
        "TERM": "RSV",
        "DEFINITION": "Respiratory Syncytial Virus"
    }
    
    Or array of records:
    [
        {
            "MARKET": "GBR",
            "BRAND": "AREXVY",
            "TERM": "RSV",
            "DEFINITION": "Respiratory Syncytial Virus"
        },
        {
            "MARKET": "GBR",
            "BRAND": "SHINGRIX",
            "TERM": "VZV",
            "DEFINITION": "Varicella Zoster Virus"
        }
    ]
    """
    try:
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing Username"}), status.HTTP_400_BAD_REQUEST

        page = int(request.args.get("page", 1))
        limit = int(request.args.get("limit", 50))
        offset = (page - 1) * limit

        valid_markets_brands = _get_valid_markets_brands(force_refresh=True)

        rows = []
        is_excel_mode = False

        # Check if file upload (Excel) or JSON body
        if "file" in request.files:
            is_excel_mode = True
            f = request.files["file"]
            
            if not (f.filename or "").lower().endswith(".xlsx"):
                return jsonify({"error": "Only .xlsx files"}), status.HTTP_400_BAD_REQUEST

            try:
                rows, _ = _process_template_excel_abbrv(f)
            except Exception as e:
                return jsonify({"error": str(e)}), status.HTTP_400_BAD_REQUEST
        else:
            # JSON body mode - Requires MARKET, BRAND, TERM, and DEFINITION
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand = _clean(rec.get("BRAND", ""))
                term = _clean(rec.get("TERM", ""))
                definition = _clean(rec.get("DEFINITION", ""))

                # Validate all four required fields
                if not market or not brand or not term or not definition:
                    return jsonify({"error": f"Item {i}: Missing required fields (MARKET, BRAND, TERM, DEFINITION)"}), status.HTTP_400_BAD_REQUEST

                rows.append((i, {
                    "MARKET": market,
                    "BRAND": brand,
                    "TERM": term,
                    "DEFINITION": definition
                }))

        if not rows:
            return jsonify({"error": "No rows"}), status.HTTP_400_BAD_REQUEST

        # Vectorized validation
        all_markets = {r[1]["MARKET"].upper() for r in rows}
        all_brands = {r[1]["BRAND"].upper() for r in rows}

        valid_market_set = {m.upper() for m in valid_markets_brands.get("markets", [])}
        valid_brand_set = {b.upper() for b in valid_markets_brands.get("brands", [])}

        invalid_mkts = [{"MARKET": m, "reason": "Invalid MARKET"} for m in all_markets if m not in valid_market_set]
        invalid_brnds = [{"BRAND": b, "reason": "Invalid BRAND"} for b in all_brands if b not in valid_brand_set]

        if invalid_mkts or invalid_brnds:
            return jsonify({"status": "error", "invalid_markets": invalid_mkts, "invalid_brands": invalid_brnds}), status.HTTP_400_BAD_REQUEST

        # Batch duplicate check
        check_vals = []
        for r in rows:
            row_data = r[1]
            check_vals.append(
                f"(UPPER(TRIM(MARKET)) = '{_sq(row_data['MARKET'].upper())}' AND "
                f"UPPER(TRIM(BRAND)) = '{_sq(row_data['BRAND'].upper())}' AND "
                f"TRIM(TERM) = '{_sq(row_data['TERM'])}' AND "
                f"TRIM(DEFINITION) = '{_sq(row_data['DEFINITION'])}')"
            )

        check_query = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET, UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(TERM) as TERM, TRIM(DEFINITION) as DEFINITION
            FROM {INSIGHT_BRAND_ABBRV_TABLE}
            WHERE {' OR '.join(check_vals)}
        """
        
        existing = dc.execute_query(check_query)
        existing_set = set()
        if existing is not None and not existing.empty:
            for _, row in existing.iterrows():
                existing_set.add((row['MARKET'], row['BRAND'], row['TERM'], row['DEFINITION']))

        inserted_rows, duplicates, errors = [], [], []
        batch_inserts = []

        for r in rows:
            try:
                row_num, row_data = r
                market = row_data["MARKET"].upper()
                brand = row_data["BRAND"].upper()
                
                key = (market, brand, row_data["TERM"], row_data["DEFINITION"])

                if key in existing_set:
                    duplicates.append({"excel_row": row_num, "MARKET": market, "BRAND": brand})
                    continue

                # Build insert values - MARKET, BRAND, TERM, DEFINITION, and DATE_ADDED
                batch_inserts.append(
                    f"('{_sq(market)}', '{_sq(brand)}', '{_sq(row_data['TERM'])}', "
                    f"'{_sq(row_data['DEFINITION'])}', '{_sq(added_by)}', CURRENT_TIMESTAMP())"
                )

                inserted_rows.append({"excel_row": row_num, "MARKET": market, "BRAND": brand})

            except Exception as e:
                errors.append({"excel_row": row_num, "error": str(e)})

        # Batch insert
        if batch_inserts:
            # Updated insert query with new table and columns
            insert_query = f"""
                INSERT INTO {INSIGHT_BRAND_ABBRV_TABLE}
                (MARKET, BRAND, TERM, DEFINITION, ADDED_BY, DATE_ADDED)
                VALUES {','.join(batch_inserts)}
            """
            dc.execute_non_query(insert_query)

        # Pagination
        total_rows = len(rows)
        total_pages = math.ceil(total_rows / limit)
        start, end = offset, min(offset + limit, total_rows)

        response = {
            "status": "success" if inserted_rows else "no-change",
            "mode": "excel-template" if is_excel_mode else "json",
            "page": page,
            "limit": limit,
            "total_rows": total_rows,
            "total_pages": total_pages,
            "inserted_count": len(inserted_rows),
            "duplicate_count": len(duplicates),
            "inserted_rows": inserted_rows[start:end],
            "duplicates": duplicates[start:end],
            "errors": errors[start:end]
        }

        response = {k: v for k, v in response.items() if v is not None and not (isinstance(v, list) and len(v) == 0)}

        return jsonify(response), status.HTTP_201_CREATED if inserted_rows else status.HTTP_200_OK

    except Exception as e:
        import traceback
        print("ERROR:", traceback.format_exc())
        return jsonify({"error": f"Internal error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR 
