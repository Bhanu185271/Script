# Cell 1: Configuration and Setup
country = "YOUR_COUNTRY"  # Set your country
version = ""  # or "Test"

version_suffix = "_test" if version == "Test" else ""

mktShareFlexibility = __getConfigValue(
    country=country, reqdConfig="MktShareFlexibility"
)

fcstInputFullPath = (
    __getConfigValue(country=country, reqdConfig="FcstInputFullPath") + version_suffix
)

print(f"Country: {country}")
print(f"Version: {version}")
print(f"MktShareFlexibility: {mktShareFlexibility}")
print(f"FcstInputFullPath: {fcstInputFullPath}")


# Cell 2: Load Data and Create Temp View
sdf = spark.read.format("delta").load(fcstInputFullPath)

print(f"Data loaded: {sdf.count()} records")
print("\nSample data:")
sdf.show(5)

vw_Name = f"vw_{country}_fcstInputFull"
sdf.createOrReplaceTempView(vw_Name)
print(f"\nTemp view created: {vw_Name}")

result = spark.sql(
    f"select max(period) as maxPeriod,add_months(max(period),-12) as sply from {vw_Name}"
).collect()
maxPeriod = result[0][0]

print(f"Max Period: {maxPeriod}")


# Cell 3: 3-Month Rolling Calculations
sdf = (
    sdf.withColumn(
        "rollingSum3MBS",
        sum(sdf.BrandSize).over(
            Window.partitionBy("xBrandId").orderBy("Period").rowsBetween(-2, 0)
        ),
    )
    .withColumn(
        "rollingSum3MMS",
        sum(sdf.MarketSize).over(
            Window.partitionBy("xBrandId").orderBy("Period").rowsBetween(-2, 0)
        ),
    )
    .withColumn("marketShare3M", col("rollingSum3MBS") / col("rollingSum3MMS"))
    .withColumn(
        "rowNumber3M",
        row_number().over(
            Window.partitionBy("xBrandId").orderBy(col("Period").desc())
        ),
    )
)

print(f"3M Rolling calculated: {sdf.count()} records")
null_ms3 = sdf.filter(col("marketShare3M").isNull()).count()
zero_ms3 = sdf.filter(col("rollingSum3MMS") == 0).count()
print(f"NULL marketShare3M: {null_ms3}")
print(f"Zero rollingSum3MMS: {zero_ms3}")
print("\nSample data:")
sdf.select("xBrandId", "Period", "BrandSize", "MarketSize", "rollingSum3MBS", "rollingSum3MMS", "marketShare3M").show(5)


# Cell 4: 6-Month Rolling Calculations
sdf = (
    sdf.withColumn(
        "rollingSum6MBS",
        sum(sdf.BrandSize).over(
            Window.partitionBy("xBrandId").orderBy("Period").rowsBetween(-5, 0)
        ),
    )
    .withColumn(
        "rollingSum6MMS",
        sum(sdf.MarketSize).over(
            Window.partitionBy("xBrandId").orderBy("Period").rowsBetween(-5, 0)
        ),
    )
    .withColumn("marketShare6M", col("rollingSum6MBS") / col("rollingSum6MMS"))
    .withColumn(
        "rowNumber6M",
        row_number().over(
            Window.partitionBy("xBrandId").orderBy(col("Period").desc())
        ),
    )
)

print(f"6M Rolling calculated: {sdf.count()} records")
null_ms6 = sdf.filter(col("marketShare6M").isNull()).count()
zero_ms6 = sdf.filter(col("rollingSum6MMS") == 0).count()
print(f"NULL marketShare6M: {null_ms6}")
print(f"Zero rollingSum6MMS: {zero_ms6}")
print("\nSample data:")
sdf.select("xBrandId", "Period", "rollingSum6MBS", "rollingSum6MMS", "marketShare6M").show(5)


# Cell 5: 12-Month Rolling Calculations
sdf = (
    sdf.withColumn(
        "rollingSum12MBS",
        sum(sdf.BrandSize).over(
            Window.partitionBy("xBrandId").orderBy("Period").rowsBetween(-11, 0)
        ),
    )
    .withColumn(
        "rollingSum12MMS",
        sum(sdf.MarketSize).over(
            Window.partitionBy("xBrandId").orderBy("Period").rowsBetween(-11, 0)
        ),
    )
    .withColumn("marketShare12M", col("rollingSum12MBS") / col("rollingSum12MMS"))
    .withColumn(
        "rowNumber12M",
        row_number().over(
            Window.partitionBy("xBrandId").orderBy(col("Period").desc())
        ),
    )
)

print(f"12M Rolling calculated: {sdf.count()} records")
null_ms12 = sdf.filter(col("marketShare12M").isNull()).count()
zero_ms12 = sdf.filter(col("rollingSum12MMS") == 0).count()
print(f"NULL marketShare12M: {null_ms12}")
print(f"Zero rollingSum12MMS: {zero_ms12}")
print("\nSample data:")
sdf.select("xBrandId", "Period", "rollingSum12MBS", "rollingSum12MMS", "marketShare12M").show(5)


# Cell 6: Apply Flexibility Bounds and MSApp3Final
sdf = sdf.withColumn(
    "lowerBound3M", sdf.marketShare3M - mktShareFlexibility
).withColumn("upperBound3M", sdf.marketShare3M + mktShareFlexibility)

sdf = (
    sdf.withColumn(
        "avgMSApp3", greatest("marketShare3M", "marketShare6M", "marketShare12M")
    )
    .withColumn(
        "adjustedApp3UB",
        when(col("avgMSApp3") > col("upperBound3M"), col("upperBound3M")).otherwise(
            col("avgMSApp3")
        ),
    )
    .withColumn(
        "MSApp3Final",
        when(
            col("adjustedApp3UB") < col("lowerBound3M"), col("lowerBound3M")
        ).otherwise(col("adjustedApp3UB")),
    )
)

print(f"Flexibility bounds applied: {sdf.count()} records")
null_msapp3 = sdf.filter(col("MSApp3Final").isNull()).count()
print(f"NULL MSApp3Final: {null_msapp3}")
print("\nSample data:")
sdf.select("xBrandId", "marketShare3M", "marketShare6M", "marketShare12M", "avgMSApp3", "MSApp3Final").show(5)


# Cell 7: Filter to Max Period
mktShareApproachesSDF = sdf.where(col("Period") == maxPeriod)

print(f"Filtered to Period={maxPeriod}: {mktShareApproachesSDF.count()} records")
print("\nSample data:")
mktShareApproachesSDF.show(5)


# Cell 8: Load Metrics and Join
metricsPath = __getConfigValue(country=country, reqdConfig="MetricsPath") + version_suffix
metricsSDF = spark.read.format("delta").load(metricsPath)

print(f"Metrics loaded: {metricsSDF.count()} records")
print("\nSample metrics data:")
metricsSDF.show(5)

computeBrandSizeForApproachesSDF = (
    metricsSDF.alias("m")
    .join(other=mktShareApproachesSDF.alias("msa"), on="xBrandId")
    .select(
        "xBrandId",
        "m." + granularity_level,
        "m.Brand",
        "m.L6Avg",
        "m.L12Avg",
        "m.BL_SPLY",
        "m.SPLY",
        "m.SPLYGSK",
        "m.capped_Baseline",
        "msa.Period",
        "msa.BrandSize",
        "msa.MarketSize",
        "msa.rollingSum3MBS",
        "msa.rollingSum3MMS",
        "msa.marketShare3M",
        "msa.rollingSum6MBS",
        "msa.rollingSum6MMS",
        "msa.marketShare6M",
        "msa.rollingSum12MBS",
        "msa.rollingSum12MMS",
        "msa.marketShare12M",
        "msa.MSApp3Final",
    )
    .withColumn("BrandSizeApp3", round(col("capped_Baseline") * col("MSApp3Final"),0))
)

print(f"\nAfter join: {computeBrandSizeForApproachesSDF.count()} records")
null_forecast = computeBrandSizeForApproachesSDF.filter(col("BrandSizeApp3").isNull()).count()
print(f"NULL BrandSizeApp3: {null_forecast}")
print("\nSample joined data:")
computeBrandSizeForApproachesSDF.select("xBrandId", "Brand", "capped_Baseline", "MSApp3Final", "BrandSizeApp3").show(5)


# Cell 9: Write to Delta Lake
MktSharePerXPath = (
    __getConfigValue(country=country, reqdConfig="MktSharePerXPath") + version_suffix
)

recursive_delete(MktSharePerXPath)
dbutils.fs.rm(MktSharePerXPath, True)

print(f"Writing to {MktSharePerXPath}")
(
    computeBrandSizeForApproachesSDF.write.format("delta")
    .mode("overwrite")
    .partitionBy("Brand")
    .save(f"{MktSharePerXPath}")
)
print("Write complete")


# Cell 10: Create Delta Table
dsDb = __getConfigValue(country=country, reqdConfig="DsDb")
MktSharePerXTable = (
    __getConfigValue(country=country, reqdConfig="MktSharePerXTable") + version_suffix
)
MktSharePerXTableName = f"{dsDb}.{MktSharePerXTable}"

print(f"Creating table {MktSharePerXTableName}")
spark.sql(f"DROP TABLE IF EXISTS {MktSharePerXTableName}")
spark.sql(
    f"CREATE TABLE {MktSharePerXTableName} USING DELTA LOCATION '{MktSharePerXPath}'"
)
print(f"Table created successfully")

print("\nVerifying table:")
spark.sql(f"SELECT COUNT(*) as count FROM {MktSharePerXTableName}").show()
