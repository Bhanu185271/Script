# d_ffn_insight_config — Individual Endpoint

INSIGHT_TABLE = "hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config"
MARKET_BRAND_TABLE = "hive_metastore.fieldforce_navigator_deployment.audio_market_brand_config"

# Cache for valid markets and brands 
_valid_markets_brands_cache = None
_cache_timestamp = None

import time
from datetime import datetime, timedelta
import pandas as pd

# Excel column to database column mapping for Insight Config
EXCEL_TO_DB_MAPPING_INSIGHT = {
    "Market": "MARKET",
    "Brand": "BRAND",
    "Message": "VALUE_1"
}

# Fixed values for Insight Config
FIXED_INSIGHT_VALUE = "mkt-key-messages-categorization-insight"
FIXED_TYPE_VALUE = "Market_Intelligence"

# Target sheet name in the template
TARGET_SHEET_NAME_INSIGHT = "Brand - Key Messages"
HEADER_ROW_INDEX = 2  # 0-indexed, so row 3 is index 2

def _clean(s):  
    return " ".join(str(s).strip().split()) if s is not None else ""

def _sq(s):     
    return str(s).replace("'", "''")

def _is_nullish(v):
    """Return True if value should be treated as SQL NULL."""
    if v is None:
        return True
    t = str(v).strip()
    if t == "":
        return True
    return t.lower() == "null"

def _get_valid_markets_brands(force_refresh=False):
    """
    Get valid markets and brands from audio_market_brand_config table.
    Cache the results to avoid repeated queries.
    Returns a dict with 'markets' and 'brands' sets.
    """
    global _valid_markets_brands_cache, _cache_timestamp
    
    # Refresh cache every 5 minutes or on force_refresh
    cache_duration = timedelta(minutes=5)
    current_time = datetime.now()
    
    if (not force_refresh and 
        _valid_markets_brands_cache is not None and 
        _cache_timestamp is not None and 
        current_time - _cache_timestamp < cache_duration):
        return _valid_markets_brands_cache
    
    try:
        # Query distinct markets and brands
        sql = f"""
            SELECT 
                DISTINCT UPPER(TRIM(MARKET)) as MARKET,
                UPPER(TRIM(BRAND)) as BRAND
            FROM {MARKET_BRAND_TABLE}
            WHERE MARKET IS NOT NULL AND BRAND IS NOT NULL
        """
        
        df = dc.execute_query(sql)
        
        if df is not None and not df.empty:
            # lookup
            valid_markets = set(df['MARKET'].dropna().unique())
            valid_brands = set(df['BRAND'].dropna().unique())
            
            # Also create a set of valid combinations
            valid_combinations = set()
            for _, row in df.iterrows():
                if pd.notna(row['MARKET']) and pd.notna(row['BRAND']):
                    valid_combinations.add((row['MARKET'], row['BRAND']))
            
            _valid_markets_brands_cache = {
                'markets': valid_markets,
                'brands': valid_brands,
                'combinations': valid_combinations
            }
            _cache_timestamp = current_time
        else:
            # Empty table or query failed
            _valid_markets_brands_cache = {
                'markets': set(),
                'brands': set(),
                'combinations': set()
            }
            _cache_timestamp = current_time
            
    except Exception as e:
        print(f"Error fetching valid markets/brands: {str(e)}")
        # Return empty sets on error
        return {'markets': set(), 'brands': set(), 'combinations': set()}
    
    return _valid_markets_brands_cache

def _validate_market_brand_separately(market, brand):
    """
    Check if market AND brand exist separately in the audio_market_brand_config table
    """
    valid_data = _get_valid_markets_brands()
    
    market_upper = market.upper().strip()
    brand_upper = brand.upper().strip()
    
    market_valid = market_upper in valid_data['markets']
    brand_valid = brand_upper in valid_data['brands']
    
    return market_valid, brand_valid

def _batch_check_duplicates_insight(rows):
    """
    Highly optimized batch duplicate check using chunked queries for Insight Config.
    Checks on: BRAND, MARKET, INSIGHT, TYPE, VALUE_1
    Returns a set of tuples representing duplicates.
    """
    if not rows:
        return set()
    
    duplicate_set = set()
    
    # Process in chunks to avoid SQL query too large and timeout issues
    chunk_size = 25  # Reduced chunk size for faster query execution
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        conditions = []
        
        for r in chunk_rows:
            market = _sq(r["MARKET"].upper())
            brand = _sq(r["BRAND"].upper())
            insight = _sq(r["INSIGHT"])
            type_val = _sq(r["TYPE"])
            value_1 = _sq(r["VALUE_1"])
            
            conditions.append(
                f"(UPPER(TRIM(MARKET)) = '{market}' AND "
                f"UPPER(TRIM(BRAND)) = '{brand}' AND "
                f"TRIM(INSIGHT) = '{insight}' AND "
                f"TRIM(TYPE) = '{type_val}' AND "
                f"TRIM(VALUE_1) = '{value_1}')"
            )
        
        # Query each chunk
        sql = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET,
                   UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(INSIGHT) as INSIGHT,
                   TRIM(TYPE) as TYPE,
                   TRIM(VALUE_1) as VALUE_1
            FROM {INSIGHT_TABLE}
            WHERE {' OR '.join(conditions)}
        """
        
        try:
            df = dc.execute_query(sql)
            if df is not None and not df.empty:
                for _, row in df.iterrows():
                    duplicate_set.add((
                        row['MARKET'], 
                        row['BRAND'], 
                        row['INSIGHT'],
                        row['TYPE'],
                        row['VALUE_1']
                    ))
        except Exception as e:
            print(f"Batch duplicate check error (chunk {chunk_start//chunk_size + 1}): {str(e)}")
    
    return duplicate_set

def _batch_insert_rows_insight(rows, added_by):
    """
    Highly optimized batch insert using chunked execution to prevent timeouts.
    MARKET and BRAND are converted to UPPER CASE during insertion.
    """
    if not rows:
        return 0
    
    total_inserted = 0
    
    # Process in smaller chunks to avoid timeout
    chunk_size = 25
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        values_clauses = []
        
        for r in chunk_rows:
            # Convert MARKET and BRAND to UPPER CASE for insertion
            market_upper = r['MARKET'].upper()
            brand_upper = r['BRAND'].upper()
            
            # VALUE_2, VALUE_3, VALUE_4 are always NULL
            sql = f"""
                INSERT INTO {INSIGHT_TABLE}
                    (BRAND, MARKET, INSIGHT, TYPE, VALUE_1, VALUE_2, VALUE_3, VALUE_4, ADDED_BY, ENTRY_TIME)
                VALUES
                    ('{_sq(brand_upper)}', '{_sq(market_upper)}', '{_sq(r['INSIGHT'])}', '{_sq(r['TYPE'])}',
                     '{_sq(r['VALUE_1'])}', NULL, NULL, NULL,
                     '{_sq(added_by)}', CURRENT_TIMESTAMP())
            """
            values_clauses.append(sql)
        
        # Execute all inserts in the chunk
        for sql in values_clauses:
            try:
                dc.execute_non_query(sql)
                total_inserted += 1
            except Exception as e:
                print(f"Insert error: {str(e)}")
                raise
    
    return total_inserted

def _process_template_excel_insight(file_obj):
    """
    Process the template Excel file for Insight Config.
    Headers start from Column B (index 1), Row 3 (index 2).
    Data starts from Row 4, Column B onwards.
    Returns a tuple: (list of tuples (excel_row_num, row_dict), list of missing columns or None)
    """
    try:
        # Read all sheet names to verify target sheet exists
        excel_file = pd.ExcelFile(file_obj, engine="openpyxl")
        
        if TARGET_SHEET_NAME_INSIGHT not in excel_file.sheet_names:
            raise ValueError(
                f"Required sheet '{TARGET_SHEET_NAME_INSIGHT}' not found. "
                f"Available sheets: {', '.join(excel_file.sheet_names)}"
            )
        
        # Read the target sheet with header at row 3 (index 2)
        df = pd.read_excel(
            file_obj, 
            sheet_name=TARGET_SHEET_NAME_INSIGHT,
            header=HEADER_ROW_INDEX,
            engine="openpyxl"
        )
        
        # Drop the first column (Column A) if it exists
        if len(df.columns) > 0:
            df = df.iloc[:, 1:]  # Skip first column (Column A)
        
        # Clean column names (strip whitespace)
        df.columns = [str(c).strip() for c in df.columns]
        
        # Replace NaN/None with empty strings FIRST
        df = df.replace({pd.NA: "", None: ""}).fillna("")
        
        # Filter out completely empty rows but keep track of original row numbers
        # Row numbers in Excel: header at row 3, data starts at row 4
        df['_excel_row_num'] = range(4, 4 + len(df))  # Excel row numbers starting from 4
        df = df[df.drop('_excel_row_num', axis=1).astype(str).apply(lambda x: x.str.strip().str.len().sum(), axis=1) > 0]
        
        # Check which Excel columns are actually present
        available_excel_cols = set(df.columns) - {'_excel_row_num'}
        expected_excel_cols = set(EXCEL_TO_DB_MAPPING_INSIGHT.keys())
        found_cols = available_excel_cols.intersection(expected_excel_cols)
        missing_cols = expected_excel_cols - available_excel_cols
        
        # Track missing columns for warning
        missing_columns_list = list(missing_cols) if missing_cols else None
        
        # Log what was found and what's missing
        if missing_cols:
            print(f"Info: Missing Excel columns (will cause error): {missing_cols}")
        if found_cols:
            print(f"Info: Found Excel columns: {found_cols}")
        
        # Mandatory columns check - all 3 columns are mandatory
        mandatory_excel_cols = ["Market", "Brand", "Message"]
        missing_mandatory = [col for col in mandatory_excel_cols if col not in df.columns]
        
        if missing_mandatory:
            raise ValueError(
                f"Missing mandatory columns in '{TARGET_SHEET_NAME_INSIGHT}' sheet: {', '.join(missing_mandatory)}. "
                f"Found columns: {list(df.columns)}"
            )
        
        # Apply mapping
        mapped_data = []
        
        for idx, row in df.iterrows():
            excel_row_num = int(row['_excel_row_num'])
            mapped_row = {}
            
            # Map each Excel column to database column
            for excel_col, db_col in EXCEL_TO_DB_MAPPING_INSIGHT.items():
                if excel_col in df.columns:
                    # Column exists in Excel, map the value
                    value = row.get(excel_col, "")
                    mapped_row[db_col] = _clean(value)
                else:
                    # Column missing - should not happen as we check mandatory above
                    mapped_row[db_col] = ""
            
            # Add fixed values
            mapped_row["INSIGHT"] = FIXED_INSIGHT_VALUE
            mapped_row["TYPE"] = FIXED_TYPE_VALUE
            mapped_row["VALUE_2"] = None
            mapped_row["VALUE_3"] = None
            mapped_row["VALUE_4"] = None
            
            # Validate - all 3 mandatory fields must have values
            if (mapped_row.get("MARKET") and 
                mapped_row.get("BRAND") and 
                mapped_row.get("VALUE_1")):
                # Store tuple of (excel_row_number, data_dict)
                mapped_data.append((excel_row_num, mapped_row))
        
        # After processing all rows, check if we found any valid data
        if not mapped_data:
            raise ValueError(
                f"No valid data rows found in sheet '{TARGET_SHEET_NAME_INSIGHT}'. "
                f"Ensure mandatory fields (Market, Brand, Message) have values. "
                f"Found columns: {list(df.columns)}"
            )
        
        # Return both mapped data (with row numbers) AND missing columns info
        return mapped_data, missing_columns_list
        
    except Exception as e:
        raise Exception(f"Error processing template Excel: {str(e)}")

# GET: column definitions + simple post template 
@app.route("/d_ffn_insight_config_key_msg", methods=["GET"])
def insight_get():
    try:
        definitions = {
            "Description": {
                "BRAND": {
                    "Example Values": ["AREXVY", "SHINGRIX"],
                    "Description": "Name of Brand to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "MARKET": {
                    "Example Values": ["GBR", "THA", "ITA", "PRT"],
                    "Description": "Name of the Market to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "INSIGHT": {
                    "Example Values": ["mkt-key-messages-categorization-insight"],
                    "Description": "Fixed value for insight type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "TYPE": {
                    "Example Values": ["Market_Intelligence"],
                    "Description": "Fixed value for type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "VALUE_1": {
                    "Example Values": ["Key message text"],
                    "Description": "The actual key message content",
                    "Parameter_type": "Mandatory"
                },
                "VALUE_2": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_3": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_4": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "ADDED_BY": {
                    "Example Values": ["abc.x.abc@gsk.com"],
                    "Description": "Auto-picked from 'Username' header in POST",
                    "Parameter_type": "Mandatory (via header)"
                },
                "ENTRY_TIME": {
                    "Example Values": ["current_timestamp()"],
                    "Description": "Set by system at insert time",
                    "Parameter_type": "Auto-generated"
                }
            },
            "sample_json": {
                "MARKET": "THA",
                "BRAND": "AREXVY",
                "VALUE_1": "This is a key message"
            },
            "excel_upload_instructions": {
                "expected_extension": ".xlsx",
                "template_sheet_name": TARGET_SHEET_NAME_INSIGHT,
                "header_row": HEADER_ROW_INDEX + 1,
                "form_field_name": "file",
                "required_columns": ["Market", "Brand", "Message"],
                "column_mapping": EXCEL_TO_DB_MAPPING_INSIGHT,
                "fixed_values": {
                    "INSIGHT": FIXED_INSIGHT_VALUE,
                    "TYPE": FIXED_TYPE_VALUE,
                    "VALUE_2": "NULL",
                    "VALUE_3": "NULL",
                    "VALUE_4": "NULL"
                },
                "notes": [
                    "Tablename: hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config",
                    f"The Excel file must contain a sheet named '{TARGET_SHEET_NAME_INSIGHT}'",
                    f"Headers must be in row {HEADER_ROW_INDEX + 1} of the sheet, starting from Column B",
                    "Data rows should start from row 4 onwards, Column B onwards",
                    "Column A is ignored during processing",
                    "INSIGHT and TYPE are automatically set to fixed values",
                    "VALUE_2, VALUE_3, VALUE_4 are automatically set to NULL"
                ]
            }
        }
        return jsonify(definitions), status.HTTP_200_OK
    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR

# POST: JSON (single/list) OR Excel (.xlsx) 
@app.route("/d_ffn_insight_config_key_msg", methods=["POST"])
def insight_post():
    try:
        # capture inserting user
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing 'Username' in headers."}), status.HTTP_400_BAD_REQUEST

        # Refresh cache at the start of POST request to get latest valid values
        _get_valid_markets_brands(force_refresh=True)

        rows = []  # list of tuples (row_num, dict)
        is_excel_mode = False
        missing_columns_warning = None

        if "file" in request.files:
            # Excel (multipart/form-data) - TEMPLATE BASED
            is_excel_mode = True
            f = request.files["file"]
            fname = (f.filename or "").lower().strip()
            
            if not fname.endswith(".xlsx"):
                return jsonify({
                    "error": "Only .xlsx files are supported for bulk upload."
                }), status.HTTP_400_BAD_REQUEST

            try:
                # Process template Excel with mapping - returns (rows, missing_columns)
                rows, missing_columns_warning = _process_template_excel_insight(f)
                
                if not rows:
                    return jsonify({
                        "error": f"No valid data rows found in sheet '{TARGET_SHEET_NAME_INSIGHT}'. "
                                 f"Ensure headers are in row {HEADER_ROW_INDEX + 1}, starting from Column B, and data starts from row 4."
                    }), status.HTTP_400_BAD_REQUEST
                
            except ValueError as ve:
                return jsonify({"error": str(ve)}), status.HTTP_400_BAD_REQUEST
            except Exception as e:
                return jsonify({
                    "error": f"Failed to process Excel template: {str(e)}"
                }), status.HTTP_400_BAD_REQUEST

        else:
            # JSON (single object or list)
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand  = _clean(rec.get("BRAND", ""))
                value_1 = _clean(rec.get("VALUE_1", ""))

                # Check all mandatory fields
                if not market or not brand or not value_1:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Missing required field(s). MARKET, BRAND, and VALUE_1 are mandatory."
                    }), status.HTTP_400_BAD_REQUEST

                # Validate MARKET and BRAND separately
                market_valid, brand_valid = _validate_market_brand_separately(market, brand)
                
                if not market_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid MARKET '{market}'. This market does not exist."
                    }), status.HTTP_400_BAD_REQUEST
                
                if not brand_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid BRAND '{brand}'. This brand does not exist."
                    }), status.HTTP_400_BAD_REQUEST

                # For JSON, use item number as row identifier
                rows.append((i, {
                    "MARKET": market,
                    "BRAND": brand,
                    "INSIGHT": FIXED_INSIGHT_VALUE,
                    "TYPE": FIXED_TYPE_VALUE,
                    "VALUE_1": value_1,
                    "VALUE_2": None,
                    "VALUE_3": None,
                    "VALUE_4": None
                }))

        if not rows:
            return jsonify({"error": "No valid rows to process."}), status.HTTP_400_BAD_REQUEST

        # Validate all rows upfront - DON'T stop at first error, collect ALL errors
        invalid_markets = []
        invalid_brands = []
        valid_rows = []
        
        valid_data = _get_valid_markets_brands()
        
        # rows is now a list of tuples: (row_number, data_dict)
        for row_num, r in rows:
            market = r["MARKET"]
            brand = r["BRAND"]
            
            market_valid = market.upper().strip() in valid_data['markets']
            brand_valid = brand.upper().strip() in valid_data['brands']
            
            if not market_valid:
                invalid_markets.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid MARKET - not found in audio_market_brand_config"
                })
                continue
            
            if not brand_valid:
                invalid_brands.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid BRAND - not found in audio_market_brand_config"
                })
                continue
            
            # This row is valid
            valid_rows.append((row_num, r))
        
        # Process valid rows even if there are some invalid ones
        # Batch duplicate check (optimized)
        duplicate_set = _batch_check_duplicates_insight([r for _, r in valid_rows])
        
        # Separate duplicates from insertable rows
        duplicates = []
        insertable_rows = []
        
        for row_num, r in valid_rows:
            # Check duplicate based on MARKET, BRAND, INSIGHT, TYPE, VALUE_1
            key = (
                r["MARKET"].upper().strip(),
                r["BRAND"].upper().strip(),
                r["INSIGHT"].strip(),
                r["TYPE"].strip(),
                r["VALUE_1"].strip()
            )
            
            if key in duplicate_set:
                duplicates.append({
                    "excel_row": row_num,
                    "MARKET": r["MARKET"],
                    "BRAND": r["BRAND"],
                    "VALUE_1": r["VALUE_1"],
                    "reason": "Duplicate - already exists in database"
                })
            else:
                insertable_rows.append(r)
        
        # Batch insert with optimized chunking
        inserted = 0
        errors = []
        
        if insertable_rows:
            try:
                # Smaller batch size (25) for faster execution and timeout prevention
                inserted = _batch_insert_rows_insight(insertable_rows, added_by)
            except Exception as e:
                errors.append({
                    "error": f"Batch insert failed: {str(e)}"
                })

        response = {
            "status": "success" if inserted and not errors else ("partial" if inserted else "no-change"),
            "mode": "excel-template" if is_excel_mode else "json",
            "sheet_processed": TARGET_SHEET_NAME_INSIGHT if is_excel_mode else None,
            "total_rows_processed": len(rows),
            "inserted_count": inserted,
            "duplicate_count": len(duplicates),
            "validation_errors_count": len(invalid_markets) + len(invalid_brands),
            "skipped_duplicates": duplicates,
            "invalid_markets": invalid_markets,
            "invalid_brands": invalid_brands,
            "row_errors": errors
        }
        
        # Add warning if columns were missing in Excel template
        if is_excel_mode and missing_columns_warning:
            response["warning"] = {
                "message": "The uploaded Excel template is missing some required columns.",
                "missing_columns": missing_columns_warning,
                "note": "All three columns (Market, Brand, Message) are required for Insight Config."
            }
        
        # Remove None and empty lists from response 
        response = {k: v for k, v in response.items() 
                   if v is not None and not (isinstance(v, list) and len(v) == 0)}
        
        # Determine HTTP status code based on results
        if inserted > 0:
            status_code = status.HTTP_201_CREATED
        elif invalid_markets or invalid_brands:
            status_code = status.HTTP_207_MULTI_STATUS  # Partial success
        else:
            status_code = status.HTTP_200_OK
        
        return jsonify(response), status_code

    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR
        
        
        
        
####### Key objection


# d_ffn_insight_config — Individual Endpoint

INSIGHT_TABLE = "hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config"
MARKET_BRAND_TABLE = "hive_metastore.fieldforce_navigator_deployment.audio_market_brand_config"

# Cache for valid markets and brands 
_valid_markets_brands_cache = None
_cache_timestamp = None

import time
from datetime import datetime, timedelta
import pandas as pd

# Excel column to database column mapping for Insight Config
EXCEL_TO_DB_MAPPING_INSIGHT = {
    "Market": "MARKET",
    "Brand": "BRAND",
    "Objection": "VALUE_1"
}

# Fixed values for Insight Config
FIXED_INSIGHT_VALUE = "mkt-objections-categorization-insight"
FIXED_TYPE_VALUE = "Market_Intelligence"

# Target sheet name in the template
TARGET_SHEET_NAME_INSIGHT = "Brand - Objections & Handlers"
HEADER_ROW_INDEX = 2  # 0-indexed, so row 3 is index 2

def _clean(s):  
    return " ".join(str(s).strip().split()) if s is not None else ""

def _sq(s):     
    return str(s).replace("'", "''")

def _is_nullish(v):
    """Return True if value should be treated as SQL NULL."""
    if v is None:
        return True
    t = str(v).strip()
    if t == "":
        return True
    return t.lower() == "null"

def _get_valid_markets_brands(force_refresh=False):
    """
    Get valid markets and brands from audio_market_brand_config table.
    Cache the results to avoid repeated queries.
    Returns a dict with 'markets' and 'brands' sets.
    """
    global _valid_markets_brands_cache, _cache_timestamp
    
    # Refresh cache every 5 minutes or on force_refresh
    cache_duration = timedelta(minutes=5)
    current_time = datetime.now()
    
    if (not force_refresh and 
        _valid_markets_brands_cache is not None and 
        _cache_timestamp is not None and 
        current_time - _cache_timestamp < cache_duration):
        return _valid_markets_brands_cache
    
    try:
        # Query distinct markets and brands
        sql = f"""
            SELECT 
                DISTINCT UPPER(TRIM(MARKET)) as MARKET,
                UPPER(TRIM(BRAND)) as BRAND
            FROM {MARKET_BRAND_TABLE}
            WHERE MARKET IS NOT NULL AND BRAND IS NOT NULL
        """
        
        df = dc.execute_query(sql)
        
        if df is not None and not df.empty:
            # lookup
            valid_markets = set(df['MARKET'].dropna().unique())
            valid_brands = set(df['BRAND'].dropna().unique())
            
            # Also create a set of valid combinations
            valid_combinations = set()
            for _, row in df.iterrows():
                if pd.notna(row['MARKET']) and pd.notna(row['BRAND']):
                    valid_combinations.add((row['MARKET'], row['BRAND']))
            
            _valid_markets_brands_cache = {
                'markets': valid_markets,
                'brands': valid_brands,
                'combinations': valid_combinations
            }
            _cache_timestamp = current_time
        else:
            # Empty table or query failed
            _valid_markets_brands_cache = {
                'markets': set(),
                'brands': set(),
                'combinations': set()
            }
            _cache_timestamp = current_time
            
    except Exception as e:
        print(f"Error fetching valid markets/brands: {str(e)}")
        # Return empty sets on error
        return {'markets': set(), 'brands': set(), 'combinations': set()}
    
    return _valid_markets_brands_cache

def _validate_market_brand_separately(market, brand):
    """
    Check if market AND brand exist separately in the audio_market_brand_config table
    """
    valid_data = _get_valid_markets_brands()
    
    market_upper = market.upper().strip()
    brand_upper = brand.upper().strip()
    
    market_valid = market_upper in valid_data['markets']
    brand_valid = brand_upper in valid_data['brands']
    
    return market_valid, brand_valid

def _batch_check_duplicates_insight(rows):
    """
    Highly optimized batch duplicate check using chunked queries for Insight Config.
    Checks on: BRAND, MARKET, INSIGHT, TYPE, VALUE_1
    Returns a set of tuples representing duplicates.
    """
    if not rows:
        return set()
    
    duplicate_set = set()
    
    # Process in chunks to avoid SQL query too large and timeout issues
    chunk_size = 25  # Reduced chunk size for faster query execution
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        conditions = []
        
        for r in chunk_rows:
            market = _sq(r["MARKET"].upper())
            brand = _sq(r["BRAND"].upper())
            insight = _sq(r["INSIGHT"])
            type_val = _sq(r["TYPE"])
            value_1 = _sq(r["VALUE_1"])
            
            conditions.append(
                f"(UPPER(TRIM(MARKET)) = '{market}' AND "
                f"UPPER(TRIM(BRAND)) = '{brand}' AND "
                f"TRIM(INSIGHT) = '{insight}' AND "
                f"TRIM(TYPE) = '{type_val}' AND "
                f"TRIM(VALUE_1) = '{value_1}')"
            )
        
        # Query each chunk
        sql = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET,
                   UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(INSIGHT) as INSIGHT,
                   TRIM(TYPE) as TYPE,
                   TRIM(VALUE_1) as VALUE_1
            FROM {INSIGHT_TABLE}
            WHERE {' OR '.join(conditions)}
        """
        
        try:
            df = dc.execute_query(sql)
            if df is not None and not df.empty:
                for _, row in df.iterrows():
                    duplicate_set.add((
                        row['MARKET'], 
                        row['BRAND'], 
                        row['INSIGHT'],
                        row['TYPE'],
                        row['VALUE_1']
                    ))
        except Exception as e:
            print(f"Batch duplicate check error (chunk {chunk_start//chunk_size + 1}): {str(e)}")
    
    return duplicate_set

def _batch_insert_rows_insight(rows, added_by):
    """
    Highly optimized batch insert using chunked execution to prevent timeouts.
    MARKET and BRAND are converted to UPPER CASE during insertion.
    """
    if not rows:
        return 0
    
    total_inserted = 0
    
    # Process in smaller chunks to avoid timeout
    chunk_size = 25
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        values_clauses = []
        
        for r in chunk_rows:
            # Convert MARKET and BRAND to UPPER CASE for insertion
            market_upper = r['MARKET'].upper()
            brand_upper = r['BRAND'].upper()
            
            # VALUE_2, VALUE_3, VALUE_4 are always NULL
            sql = f"""
                INSERT INTO {INSIGHT_TABLE}
                    (BRAND, MARKET, INSIGHT, TYPE, VALUE_1, VALUE_2, VALUE_3, VALUE_4, ADDED_BY, ENTRY_TIME)
                VALUES
                    ('{_sq(brand_upper)}', '{_sq(market_upper)}', '{_sq(r['INSIGHT'])}', '{_sq(r['TYPE'])}',
                     '{_sq(r['VALUE_1'])}', NULL, NULL, NULL,
                     '{_sq(added_by)}', CURRENT_TIMESTAMP())
            """
            values_clauses.append(sql)
        
        # Execute all inserts in the chunk
        for sql in values_clauses:
            try:
                dc.execute_non_query(sql)
                total_inserted += 1
            except Exception as e:
                print(f"Insert error: {str(e)}")
                raise
    
    return total_inserted

def _process_template_excel_insight(file_obj):
    """
    Process the template Excel file for Insight Config.
    Headers start from Column B (index 1), Row 3 (index 2).
    Data starts from Row 4, Column B onwards.
    Returns a tuple: (list of tuples (excel_row_num, row_dict), list of missing columns or None)
    """
    try:
        # Read all sheet names to verify target sheet exists
        excel_file = pd.ExcelFile(file_obj, engine="openpyxl")
        
        if TARGET_SHEET_NAME_INSIGHT not in excel_file.sheet_names:
            raise ValueError(
                f"Required sheet '{TARGET_SHEET_NAME_INSIGHT}' not found. "
                f"Available sheets: {', '.join(excel_file.sheet_names)}"
            )
        
        # Read the target sheet with header at row 3 (index 2)
        df = pd.read_excel(
            file_obj, 
            sheet_name=TARGET_SHEET_NAME_INSIGHT,
            header=HEADER_ROW_INDEX,
            engine="openpyxl"
        )
        
        # Drop the first column (Column A) if it exists
        if len(df.columns) > 0:
            df = df.iloc[:, 1:]  # Skip first column (Column A)
        
        # Clean column names (strip whitespace)
        df.columns = [str(c).strip() for c in df.columns]
        
        # Replace NaN/None with empty strings FIRST
        df = df.replace({pd.NA: "", None: ""}).fillna("")
        
        # Filter out completely empty rows but keep track of original row numbers
        # Row numbers in Excel: header at row 3, data starts at row 4
        df['_excel_row_num'] = range(4, 4 + len(df))  # Excel row numbers starting from 4
        df = df[df.drop('_excel_row_num', axis=1).astype(str).apply(lambda x: x.str.strip().str.len().sum(), axis=1) > 0]
        
        # Check which Excel columns are actually present
        available_excel_cols = set(df.columns) - {'_excel_row_num'}
        expected_excel_cols = set(EXCEL_TO_DB_MAPPING_INSIGHT.keys())
        found_cols = available_excel_cols.intersection(expected_excel_cols)
        missing_cols = expected_excel_cols - available_excel_cols
        
        # Track missing columns for warning
        missing_columns_list = list(missing_cols) if missing_cols else None
        
        # Log what was found and what's missing
        if missing_cols:
            print(f"Info: Missing Excel columns (will cause error): {missing_cols}")
        if found_cols:
            print(f"Info: Found Excel columns: {found_cols}")
        
        # Mandatory columns check - all 3 columns are mandatory
        mandatory_excel_cols = ["Market", "Brand", "Objection"]
        missing_mandatory = [col for col in mandatory_excel_cols if col not in df.columns]
        
        if missing_mandatory:
            raise ValueError(
                f"Missing mandatory columns in '{TARGET_SHEET_NAME_INSIGHT}' sheet: {', '.join(missing_mandatory)}. "
                f"Found columns: {list(df.columns)}"
            )
        
        # Apply mapping
        mapped_data = []
        
        for idx, row in df.iterrows():
            excel_row_num = int(row['_excel_row_num'])
            mapped_row = {}
            
            # Map each Excel column to database column
            for excel_col, db_col in EXCEL_TO_DB_MAPPING_INSIGHT.items():
                if excel_col in df.columns:
                    # Column exists in Excel, map the value
                    value = row.get(excel_col, "")
                    mapped_row[db_col] = _clean(value)
                else:
                    # Column missing - should not happen as we check mandatory above
                    mapped_row[db_col] = ""
            
            # Add fixed values
            mapped_row["INSIGHT"] = FIXED_INSIGHT_VALUE
            mapped_row["TYPE"] = FIXED_TYPE_VALUE
            mapped_row["VALUE_2"] = None
            mapped_row["VALUE_3"] = None
            mapped_row["VALUE_4"] = None
            
            # Validate - all 3 mandatory fields must have values
            if (mapped_row.get("MARKET") and 
                mapped_row.get("BRAND") and 
                mapped_row.get("VALUE_1")):
                # Store tuple of (excel_row_number, data_dict)
                mapped_data.append((excel_row_num, mapped_row))
        
        # After processing all rows, check if we found any valid data
        if not mapped_data:
            raise ValueError(
                f"No valid data rows found in sheet '{TARGET_SHEET_NAME_INSIGHT}'. "
                f"Ensure mandatory fields (Market, Brand, Objection) have values. "
                f"Found columns: {list(df.columns)}"
            )
        
        # Return both mapped data (with row numbers) AND missing columns info
        return mapped_data, missing_columns_list
        
    except Exception as e:
        raise Exception(f"Error processing template Excel: {str(e)}")

# GET: column definitions + simple post template 
@app.route("/d_ffn_insight_config_key_obj", methods=["GET"])
def insight_key_obj_get():
    try:
        definitions = {
            "Description": {
                "BRAND": {
                    "Example Values": ["AREXVY", "SHINGRIX"],
                    "Description": "Name of Brand to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "MARKET": {
                    "Example Values": ["GBR", "THA", "ITA", "PRT"],
                    "Description": "Name of the Market to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "INSIGHT": {
                    "Example Values": ["mkt-objections-categorization-insight"],
                    "Description": "Fixed value for insight type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "TYPE": {
                    "Example Values": ["Market_Intelligence"],
                    "Description": "Fixed value for type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "VALUE_1": {
                    "Example Values": ["Key Objection text"],
                    "Description": "The actual key Objection content",
                    "Parameter_type": "Mandatory"
                },
                "VALUE_2": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_3": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_4": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "ADDED_BY": {
                    "Example Values": ["abc.x.abc@gsk.com"],
                    "Description": "Auto-picked from 'Username' header in POST",
                    "Parameter_type": "Mandatory (via header)"
                },
                "ENTRY_TIME": {
                    "Example Values": ["current_timestamp()"],
                    "Description": "Set by system at insert time",
                    "Parameter_type": "Auto-generated"
                }
            },
            "sample_json": {
                "MARKET": "THA",
                "BRAND": "AREXVY",
                "VALUE_1": "This is a key Objection"
            },
            "excel_upload_instructions": {
                "expected_extension": ".xlsx",
                "template_sheet_name": TARGET_SHEET_NAME_INSIGHT,
                "header_row": HEADER_ROW_INDEX + 1,
                "form_field_name": "file",
                "required_columns": ["Market", "Brand", "Objection"],
                "column_mapping": EXCEL_TO_DB_MAPPING_INSIGHT,
                "fixed_values": {
                    "INSIGHT": FIXED_INSIGHT_VALUE,
                    "TYPE": FIXED_TYPE_VALUE,
                    "VALUE_2": "NULL",
                    "VALUE_3": "NULL",
                    "VALUE_4": "NULL"
                },
                "notes": [
                    "Tablename: hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config",
                    f"The Excel file must contain a sheet named '{TARGET_SHEET_NAME_INSIGHT}'",
                    f"Headers must be in row {HEADER_ROW_INDEX + 1} of the sheet, starting from Column B",
                    "Data rows should start from row 4 onwards, Column B onwards",
                    "Column A is ignored during processing",
                    "INSIGHT and TYPE are automatically set to fixed values",
                    "VALUE_2, VALUE_3, VALUE_4 are automatically set to NULL"
                ]
            }
        }
        return jsonify(definitions), status.HTTP_200_OK
    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR

# POST: JSON (single/list) OR Excel (.xlsx) 
@app.route("/d_ffn_insight_config_key_obj", methods=["POST"])
def insight_key_obj_post():
    try:
        # capture inserting user
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing 'Username' in headers."}), status.HTTP_400_BAD_REQUEST

        # Refresh cache at the start of POST request to get latest valid values
        _get_valid_markets_brands(force_refresh=True)

        rows = []  # list of tuples (row_num, dict)
        is_excel_mode = False
        missing_columns_warning = None

        if "file" in request.files:
            # Excel (multipart/form-data) - TEMPLATE BASED
            is_excel_mode = True
            f = request.files["file"]
            fname = (f.filename or "").lower().strip()
            
            if not fname.endswith(".xlsx"):
                return jsonify({
                    "error": "Only .xlsx files are supported for bulk upload."
                }), status.HTTP_400_BAD_REQUEST

            try:
                # Process template Excel with mapping - returns (rows, missing_columns)
                rows, missing_columns_warning = _process_template_excel_insight(f)
                
                if not rows:
                    return jsonify({
                        "error": f"No valid data rows found in sheet '{TARGET_SHEET_NAME_INSIGHT}'. "
                                 f"Ensure headers are in row {HEADER_ROW_INDEX + 1}, starting from Column B, and data starts from row 4."
                    }), status.HTTP_400_BAD_REQUEST
                
            except ValueError as ve:
                return jsonify({"error": str(ve)}), status.HTTP_400_BAD_REQUEST
            except Exception as e:
                return jsonify({
                    "error": f"Failed to process Excel template: {str(e)}"
                }), status.HTTP_400_BAD_REQUEST

        else:
            # JSON (single object or list)
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand  = _clean(rec.get("BRAND", ""))
                value_1 = _clean(rec.get("VALUE_1", ""))

                # Check all mandatory fields
                if not market or not brand or not value_1:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Missing required field(s). MARKET, BRAND, and VALUE_1 are mandatory."
                    }), status.HTTP_400_BAD_REQUEST

                # Validate MARKET and BRAND separately
                market_valid, brand_valid = _validate_market_brand_separately(market, brand)
                
                if not market_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid MARKET '{market}'. This market does not exist."
                    }), status.HTTP_400_BAD_REQUEST
                
                if not brand_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid BRAND '{brand}'. This brand does not exist."
                    }), status.HTTP_400_BAD_REQUEST

                # For JSON, use item number as row identifier
                rows.append((i, {
                    "MARKET": market,
                    "BRAND": brand,
                    "INSIGHT": FIXED_INSIGHT_VALUE,
                    "TYPE": FIXED_TYPE_VALUE,
                    "VALUE_1": value_1,
                    "VALUE_2": None,
                    "VALUE_3": None,
                    "VALUE_4": None
                }))

        if not rows:
            return jsonify({"error": "No valid rows to process."}), status.HTTP_400_BAD_REQUEST

        # Validate all rows upfront - DON'T stop at first error, collect ALL errors
        invalid_markets = []
        invalid_brands = []
        valid_rows = []
        
        valid_data = _get_valid_markets_brands()
        
        # rows is now a list of tuples: (row_number, data_dict)
        for row_num, r in rows:
            market = r["MARKET"]
            brand = r["BRAND"]
            
            market_valid = market.upper().strip() in valid_data['markets']
            brand_valid = brand.upper().strip() in valid_data['brands']
            
            if not market_valid:
                invalid_markets.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid MARKET - not found in audio_market_brand_config"
                })
                continue
            
            if not brand_valid:
                invalid_brands.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid BRAND - not found in audio_market_brand_config"
                })
                continue
            
            # This row is valid
            valid_rows.append((row_num, r))
        
        # Process valid rows even if there are some invalid ones
        # Batch duplicate check (optimized)
        duplicate_set = _batch_check_duplicates_insight([r for _, r in valid_rows])
        
        # Separate duplicates from insertable rows
        duplicates = []
        insertable_rows = []
        
        for row_num, r in valid_rows:
            # Check duplicate based on MARKET, BRAND, INSIGHT, TYPE, VALUE_1
            key = (
                r["MARKET"].upper().strip(),
                r["BRAND"].upper().strip(),
                r["INSIGHT"].strip(),
                r["TYPE"].strip(),
                r["VALUE_1"].strip()
            )
            
            if key in duplicate_set:
                duplicates.append({
                    "excel_row": row_num,
                    "MARKET": r["MARKET"],
                    "BRAND": r["BRAND"],
                    "VALUE_1": r["VALUE_1"],
                    "reason": "Duplicate - already exists in database"
                })
            else:
                insertable_rows.append(r)
        
        # Batch insert with optimized chunking
        inserted = 0
        errors = []
        
        if insertable_rows:
            try:
                # Smaller batch size (25) for faster execution and timeout prevention
                inserted = _batch_insert_rows_insight(insertable_rows, added_by)
            except Exception as e:
                errors.append({
                    "error": f"Batch insert failed: {str(e)}"
                })

        response = {
            "status": "success" if inserted and not errors else ("partial" if inserted else "no-change"),
            "mode": "excel-template" if is_excel_mode else "json",
            "sheet_processed": TARGET_SHEET_NAME_INSIGHT if is_excel_mode else None,
            "total_rows_processed": len(rows),
            "inserted_count": inserted,
            "duplicate_count": len(duplicates),
            "validation_errors_count": len(invalid_markets) + len(invalid_brands),
            "skipped_duplicates": duplicates,
            "invalid_markets": invalid_markets,
            "invalid_brands": invalid_brands,
            "row_errors": errors
        }
        
        # Add warning if columns were missing in Excel template
        if is_excel_mode and missing_columns_warning:
            response["warning"] = {
                "message": "The uploaded Excel template is missing some required columns.",
                "missing_columns": missing_columns_warning,
                "note": "All three columns (Market, Brand, Objection) are required for Insight Config."
            }
        
        # Remove None and empty lists from response 
        response = {k: v for k, v in response.items() 
                   if v is not None and not (isinstance(v, list) and len(v) == 0)}
        
        # Determine HTTP status code based on results
        if inserted > 0:
            status_code = status.HTTP_201_CREATED
        elif invalid_markets or invalid_brands:
            status_code = status.HTTP_207_MULTI_STATUS  # Partial success
        else:
            status_code = status.HTTP_200_OK
        
        return jsonify(response), status_code

    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR
    


        
        
####### Brand Studies


# d_ffn_insight_config — Individual Endpoint

INSIGHT_TABLE = "hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config"
MARKET_BRAND_TABLE = "hive_metastore.fieldforce_navigator_deployment.audio_market_brand_config"

# Cache for valid markets and brands 
_valid_markets_brands_cache = None
_cache_timestamp = None

import time
from datetime import datetime, timedelta
import pandas as pd

# Excel column to database column mapping for Insight Config
EXCEL_TO_DB_MAPPING_INSIGHT = {
    "Market": "MARKET",
    "Brand": "BRAND",
    "Study Name": "VALUE_1"
}

# Fixed values for Insight Config
FIXED_INSIGHT_VALUE = "medical-word-reassignment"
FIXED_TYPE_VALUE = "STUDY"

# Target sheet name in the template
TARGET_SHEET_NAME_INSIGHT = "Brand - Studies"
HEADER_ROW_INDEX = 2  # 0-indexed, so row 3 is index 2

def _clean(s):  
    return " ".join(str(s).strip().split()) if s is not None else ""

def _sq(s):     
    return str(s).replace("'", "''")

def _is_nullish(v):
    """Return True if value should be treated as SQL NULL."""
    if v is None:
        return True
    t = str(v).strip()
    if t == "":
        return True
    return t.lower() == "null"

def _get_valid_markets_brands(force_refresh=False):
    """
    Get valid markets and brands from audio_market_brand_config table.
    Cache the results to avoid repeated queries.
    Returns a dict with 'markets' and 'brands' sets.
    """
    global _valid_markets_brands_cache, _cache_timestamp
    
    # Refresh cache every 5 minutes or on force_refresh
    cache_duration = timedelta(minutes=5)
    current_time = datetime.now()
    
    if (not force_refresh and 
        _valid_markets_brands_cache is not None and 
        _cache_timestamp is not None and 
        current_time - _cache_timestamp < cache_duration):
        return _valid_markets_brands_cache
    
    try:
        # Query distinct markets and brands
        sql = f"""
            SELECT 
                DISTINCT UPPER(TRIM(MARKET)) as MARKET,
                UPPER(TRIM(BRAND)) as BRAND
            FROM {MARKET_BRAND_TABLE}
            WHERE MARKET IS NOT NULL AND BRAND IS NOT NULL
        """
        
        df = dc.execute_query(sql)
        
        if df is not None and not df.empty:
            # lookup
            valid_markets = set(df['MARKET'].dropna().unique())
            valid_brands = set(df['BRAND'].dropna().unique())
            
            # Also create a set of valid combinations
            valid_combinations = set()
            for _, row in df.iterrows():
                if pd.notna(row['MARKET']) and pd.notna(row['BRAND']):
                    valid_combinations.add((row['MARKET'], row['BRAND']))
            
            _valid_markets_brands_cache = {
                'markets': valid_markets,
                'brands': valid_brands,
                'combinations': valid_combinations
            }
            _cache_timestamp = current_time
        else:
            # Empty table or query failed
            _valid_markets_brands_cache = {
                'markets': set(),
                'brands': set(),
                'combinations': set()
            }
            _cache_timestamp = current_time
            
    except Exception as e:
        print(f"Error fetching valid markets/brands: {str(e)}")
        # Return empty sets on error
        return {'markets': set(), 'brands': set(), 'combinations': set()}
    
    return _valid_markets_brands_cache

def _validate_market_brand_separately(market, brand):
    """
    Check if market AND brand exist separately in the audio_market_brand_config table
    """
    valid_data = _get_valid_markets_brands()
    
    market_upper = market.upper().strip()
    brand_upper = brand.upper().strip()
    
    market_valid = market_upper in valid_data['markets']
    brand_valid = brand_upper in valid_data['brands']
    
    return market_valid, brand_valid

def _batch_check_duplicates_insight(rows):
    """
    Highly optimized batch duplicate check using chunked queries for Insight Config.
    Checks on: BRAND, MARKET, INSIGHT, TYPE, VALUE_1
    Returns a set of tuples representing duplicates.
    """
    if not rows:
        return set()
    
    duplicate_set = set()
    
    # Process in chunks to avoid SQL query too large and timeout issues
    chunk_size = 25  # Reduced chunk size for faster query execution
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        conditions = []
        
        for r in chunk_rows:
            market = _sq(r["MARKET"].upper())
            brand = _sq(r["BRAND"].upper())
            insight = _sq(r["INSIGHT"])
            type_val = _sq(r["TYPE"])
            value_1 = _sq(r["VALUE_1"])
            
            conditions.append(
                f"(UPPER(TRIM(MARKET)) = '{market}' AND "
                f"UPPER(TRIM(BRAND)) = '{brand}' AND "
                f"TRIM(INSIGHT) = '{insight}' AND "
                f"TRIM(TYPE) = '{type_val}' AND "
                f"TRIM(VALUE_1) = '{value_1}')"
            )
        
        # Query each chunk
        sql = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET,
                   UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(INSIGHT) as INSIGHT,
                   TRIM(TYPE) as TYPE,
                   TRIM(VALUE_1) as VALUE_1
            FROM {INSIGHT_TABLE}
            WHERE {' OR '.join(conditions)}
        """
        
        try:
            df = dc.execute_query(sql)
            if df is not None and not df.empty:
                for _, row in df.iterrows():
                    duplicate_set.add((
                        row['MARKET'], 
                        row['BRAND'], 
                        row['INSIGHT'],
                        row['TYPE'],
                        row['VALUE_1']
                    ))
        except Exception as e:
            print(f"Batch duplicate check error (chunk {chunk_start//chunk_size + 1}): {str(e)}")
    
    return duplicate_set

def _batch_insert_rows_insight(rows, added_by):
    """
    Highly optimized batch insert using chunked execution to prevent timeouts.
    MARKET and BRAND are converted to UPPER CASE during insertion.
    """
    if not rows:
        return 0
    
    total_inserted = 0
    
    # Process in smaller chunks to avoid timeout
    chunk_size = 25
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        values_clauses = []
        
        for r in chunk_rows:
            # Convert MARKET and BRAND to UPPER CASE for insertion
            market_upper = r['MARKET'].upper()
            brand_upper = r['BRAND'].upper()
            
            # VALUE_2, VALUE_3, VALUE_4 are always NULL
            sql = f"""
                INSERT INTO {INSIGHT_TABLE}
                    (BRAND, MARKET, INSIGHT, TYPE, VALUE_1, VALUE_2, VALUE_3, VALUE_4, ADDED_BY, ENTRY_TIME)
                VALUES
                    ('{_sq(brand_upper)}', '{_sq(market_upper)}', '{_sq(r['INSIGHT'])}', '{_sq(r['TYPE'])}',
                     '{_sq(r['VALUE_1'])}', NULL, NULL, NULL,
                     '{_sq(added_by)}', CURRENT_TIMESTAMP())
            """
            values_clauses.append(sql)
        
        # Execute all inserts in the chunk
        for sql in values_clauses:
            try:
                dc.execute_non_query(sql)
                total_inserted += 1
            except Exception as e:
                print(f"Insert error: {str(e)}")
                raise
    
    return total_inserted

def _process_template_excel_insight(file_obj):
    """
    Process the template Excel file for Insight Config.
    Headers start from Column B (index 1), Row 3 (index 2).
    Data starts from Row 4, Column B onwards.
    Returns a tuple: (list of tuples (excel_row_num, row_dict), list of missing columns or None)
    """
    try:
        # Read all sheet names to verify target sheet exists
        excel_file = pd.ExcelFile(file_obj, engine="openpyxl")
        
        if TARGET_SHEET_NAME_INSIGHT not in excel_file.sheet_names:
            raise ValueError(
                f"Required sheet '{TARGET_SHEET_NAME_INSIGHT}' not found. "
                f"Available sheets: {', '.join(excel_file.sheet_names)}"
            )
        
        # Read the target sheet with header at row 3 (index 2)
        df = pd.read_excel(
            file_obj, 
            sheet_name=TARGET_SHEET_NAME_INSIGHT,
            header=HEADER_ROW_INDEX,
            engine="openpyxl"
        )
        
        # Drop the first column (Column A) if it exists
        if len(df.columns) > 0:
            df = df.iloc[:, 1:]  # Skip first column (Column A)
        
        # Clean column names (strip whitespace)
        df.columns = [str(c).strip() for c in df.columns]
        
        # Replace NaN/None with empty strings FIRST
        df = df.replace({pd.NA: "", None: ""}).fillna("")
        
        # Filter out completely empty rows but keep track of original row numbers
        # Row numbers in Excel: header at row 3, data starts at row 4
        df['_excel_row_num'] = range(4, 4 + len(df))  # Excel row numbers starting from 4
        df = df[df.drop('_excel_row_num', axis=1).astype(str).apply(lambda x: x.str.strip().str.len().sum(), axis=1) > 0]
        
        # Check which Excel columns are actually present
        available_excel_cols = set(df.columns) - {'_excel_row_num'}
        expected_excel_cols = set(EXCEL_TO_DB_MAPPING_INSIGHT.keys())
        found_cols = available_excel_cols.intersection(expected_excel_cols)
        missing_cols = expected_excel_cols - available_excel_cols
        
        # Track missing columns for warning
        missing_columns_list = list(missing_cols) if missing_cols else None
        
        # Log what was found and what's missing
        if missing_cols:
            print(f"Info: Missing Excel columns (will cause error): {missing_cols}")
        if found_cols:
            print(f"Info: Found Excel columns: {found_cols}")
        
        # Mandatory columns check - all 3 columns are mandatory
        mandatory_excel_cols = ["Market", "Brand", "Study Name"]
        missing_mandatory = [col for col in mandatory_excel_cols if col not in df.columns]
        
        if missing_mandatory:
            raise ValueError(
                f"Missing mandatory columns in '{TARGET_SHEET_NAME_INSIGHT}' sheet: {', '.join(missing_mandatory)}. "
                f"Found columns: {list(df.columns)}"
            )
        
        # Apply mapping
        mapped_data = []
        
        for idx, row in df.iterrows():
            excel_row_num = int(row['_excel_row_num'])
            mapped_row = {}
            
            # Map each Excel column to database column
            for excel_col, db_col in EXCEL_TO_DB_MAPPING_INSIGHT.items():
                if excel_col in df.columns:
                    # Column exists in Excel, map the value
                    value = row.get(excel_col, "")
                    mapped_row[db_col] = _clean(value)
                else:
                    # Column missing - should not happen as we check mandatory above
                    mapped_row[db_col] = ""
            
            # Add fixed values
            mapped_row["INSIGHT"] = FIXED_INSIGHT_VALUE
            mapped_row["TYPE"] = FIXED_TYPE_VALUE
            mapped_row["VALUE_2"] = None
            mapped_row["VALUE_3"] = None
            mapped_row["VALUE_4"] = None
            
            # Validate - all 3 mandatory fields must have values
            if (mapped_row.get("MARKET") and 
                mapped_row.get("BRAND") and 
                mapped_row.get("VALUE_1")):
                # Store tuple of (excel_row_number, data_dict)
                mapped_data.append((excel_row_num, mapped_row))
        
        # After processing all rows, check if we found any valid data
        if not mapped_data:
            raise ValueError(
                f"No valid data rows found in sheet '{TARGET_SHEET_NAME_INSIGHT}'. "
                f"Ensure mandatory fields (Market, Brand, Study Name) have values. "
                f"Found columns: {list(df.columns)}"
            )
        
        # Return both mapped data (with row numbers) AND missing columns info
        return mapped_data, missing_columns_list
        
    except Exception as e:
        raise Exception(f"Error processing template Excel: {str(e)}")

# GET: column definitions + simple post template 
@app.route("/d_ffn_insight_config_brand_study", methods=["GET"])
def insight_brand_study_get():
    try:
        definitions = {
            "Description": {
                "BRAND": {
                    "Example Values": ["AREXVY", "SHINGRIX"],
                    "Description": "Name of Brand to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "MARKET": {
                    "Example Values": ["GBR", "THA", "ITA", "PRT"],
                    "Description": "Name of the Market to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "INSIGHT": {
                    "Example Values": ["medical-word-reassignment"],
                    "Description": "Fixed value for insight type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "TYPE": {
                    "Example Values": ["STUDY"],
                    "Description": "Fixed value for type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "VALUE_1": {
                    "Example Values": ["Key Study Name  text"],
                    "Description": "The actual key Study Name content",
                    "Parameter_type": "Mandatory"
                },
                "VALUE_2": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_3": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_4": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "ADDED_BY": {
                    "Example Values": ["abc.x.abc@gsk.com"],
                    "Description": "Auto-picked from 'Username' header in POST",
                    "Parameter_type": "Mandatory (via header)"
                },
                "ENTRY_TIME": {
                    "Example Values": ["current_timestamp()"],
                    "Description": "Set by system at insert time",
                    "Parameter_type": "Auto-generated"
                }
            },
            "sample_json": {
                "MARKET": "THA",
                "BRAND": "AREXVY",
                "VALUE_1": "This is a key Study Name"
            },
            "excel_upload_instructions": {
                "expected_extension": ".xlsx",
                "template_sheet_name": TARGET_SHEET_NAME_INSIGHT,
                "header_row": HEADER_ROW_INDEX + 1,
                "form_field_name": "file",
                "required_columns": ["Market", "Brand", "Study Name"],
                "column_mapping": EXCEL_TO_DB_MAPPING_INSIGHT,
                "fixed_values": {
                    "INSIGHT": FIXED_INSIGHT_VALUE,
                    "TYPE": FIXED_TYPE_VALUE,
                    "VALUE_2": "NULL",
                    "VALUE_3": "NULL",
                    "VALUE_4": "NULL"
                },
                "notes": [
                    "Tablename: hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config",
                    f"The Excel file must contain a sheet named '{TARGET_SHEET_NAME_INSIGHT}'",
                    f"Headers must be in row {HEADER_ROW_INDEX + 1} of the sheet, starting from Column B",
                    "Data rows should start from row 4 onwards, Column B onwards",
                    "Column A is ignored during processing",
                    "INSIGHT and TYPE are automatically set to fixed values",
                    "VALUE_2, VALUE_3, VALUE_4 are automatically set to NULL"
                ]
            }
        }
        return jsonify(definitions), status.HTTP_200_OK
    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR

# POST: JSON (single/list) OR Excel (.xlsx) 
@app.route("/d_ffn_insight_config_brand_study", methods=["POST"])
def insight_brand_study_post():
    try:
        # capture inserting user
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing 'Username' in headers."}), status.HTTP_400_BAD_REQUEST

        # Refresh cache at the start of POST request to get latest valid values
        _get_valid_markets_brands(force_refresh=True)

        rows = []  # list of tuples (row_num, dict)
        is_excel_mode = False
        missing_columns_warning = None

        if "file" in request.files:
            # Excel (multipart/form-data) - TEMPLATE BASED
            is_excel_mode = True
            f = request.files["file"]
            fname = (f.filename or "").lower().strip()
            
            if not fname.endswith(".xlsx"):
                return jsonify({
                    "error": "Only .xlsx files are supported for bulk upload."
                }), status.HTTP_400_BAD_REQUEST

            try:
                # Process template Excel with mapping - returns (rows, missing_columns)
                rows, missing_columns_warning = _process_template_excel_insight(f)
                
                if not rows:
                    return jsonify({
                        "error": f"No valid data rows found in sheet '{TARGET_SHEET_NAME_INSIGHT}'. "
                                 f"Ensure headers are in row {HEADER_ROW_INDEX + 1}, starting from Column B, and data starts from row 4."
                    }), status.HTTP_400_BAD_REQUEST
                
            except ValueError as ve:
                return jsonify({"error": str(ve)}), status.HTTP_400_BAD_REQUEST
            except Exception as e:
                return jsonify({
                    "error": f"Failed to process Excel template: {str(e)}"
                }), status.HTTP_400_BAD_REQUEST

        else:
            # JSON (single object or list)
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand  = _clean(rec.get("BRAND", ""))
                value_1 = _clean(rec.get("VALUE_1", ""))

                # Check all mandatory fields
                if not market or not brand or not value_1:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Missing required field(s). MARKET, BRAND, and VALUE_1 are mandatory."
                    }), status.HTTP_400_BAD_REQUEST

                # Validate MARKET and BRAND separately
                market_valid, brand_valid = _validate_market_brand_separately(market, brand)
                
                if not market_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid MARKET '{market}'. This market does not exist."
                    }), status.HTTP_400_BAD_REQUEST
                
                if not brand_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid BRAND '{brand}'. This brand does not exist."
                    }), status.HTTP_400_BAD_REQUEST

                # For JSON, use item number as row identifier
                rows.append((i, {
                    "MARKET": market,
                    "BRAND": brand,
                    "INSIGHT": FIXED_INSIGHT_VALUE,
                    "TYPE": FIXED_TYPE_VALUE,
                    "VALUE_1": value_1,
                    "VALUE_2": None,
                    "VALUE_3": None,
                    "VALUE_4": None
                }))

        if not rows:
            return jsonify({"error": "No valid rows to process."}), status.HTTP_400_BAD_REQUEST

        # Validate all rows upfront - DON'T stop at first error, collect ALL errors
        invalid_markets = []
        invalid_brands = []
        valid_rows = []
        
        valid_data = _get_valid_markets_brands()
        
        # rows is now a list of tuples: (row_number, data_dict)
        for row_num, r in rows:
            market = r["MARKET"]
            brand = r["BRAND"]
            
            market_valid = market.upper().strip() in valid_data['markets']
            brand_valid = brand.upper().strip() in valid_data['brands']
            
            if not market_valid:
                invalid_markets.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid MARKET - not found in audio_market_brand_config"
                })
                continue
            
            if not brand_valid:
                invalid_brands.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid BRAND - not found in audio_market_brand_config"
                })
                continue
            
            # This row is valid
            valid_rows.append((row_num, r))
        
        # Process valid rows even if there are some invalid ones
        # Batch duplicate check (optimized)
        duplicate_set = _batch_check_duplicates_insight([r for _, r in valid_rows])
        
        # Separate duplicates from insertable rows
        duplicates = []
        insertable_rows = []
        
        for row_num, r in valid_rows:
            # Check duplicate based on MARKET, BRAND, INSIGHT, TYPE, VALUE_1
            key = (
                r["MARKET"].upper().strip(),
                r["BRAND"].upper().strip(),
                r["INSIGHT"].strip(),
                r["TYPE"].strip(),
                r["VALUE_1"].strip()
            )
            
            if key in duplicate_set:
                duplicates.append({
                    "excel_row": row_num,
                    "MARKET": r["MARKET"],
                    "BRAND": r["BRAND"],
                    "VALUE_1": r["VALUE_1"],
                    "reason": "Duplicate - already exists in database"
                })
            else:
                insertable_rows.append(r)
        
        # Batch insert with optimized chunking
        inserted = 0
        errors = []
        
        if insertable_rows:
            try:
                # Smaller batch size (25) for faster execution and timeout prevention
                inserted = _batch_insert_rows_insight(insertable_rows, added_by)
            except Exception as e:
                errors.append({
                    "error": f"Batch insert failed: {str(e)}"
                })

        response = {
            "status": "success" if inserted and not errors else ("partial" if inserted else "no-change"),
            "mode": "excel-template" if is_excel_mode else "json",
            "sheet_processed": TARGET_SHEET_NAME_INSIGHT if is_excel_mode else None,
            "total_rows_processed": len(rows),
            "inserted_count": inserted,
            "duplicate_count": len(duplicates),
            "validation_errors_count": len(invalid_markets) + len(invalid_brands),
            "skipped_duplicates": duplicates,
            "invalid_markets": invalid_markets,
            "invalid_brands": invalid_brands,
            "row_errors": errors
        }
        
        # Add warning if columns were missing in Excel template
        if is_excel_mode and missing_columns_warning:
            response["warning"] = {
                "message": "The uploaded Excel template is missing some required columns.",
                "missing_columns": missing_columns_warning,
                "note": "All three columns (Market, Brand, Study Name) are required for Insight Config."
            }
        
        # Remove None and empty lists from response 
        response = {k: v for k, v in response.items() 
                   if v is not None and not (isinstance(v, list) and len(v) == 0)}
        
        # Determine HTTP status code based on results
        if inserted > 0:
            status_code = status.HTTP_201_CREATED
        elif invalid_markets or invalid_brands:
            status_code = status.HTTP_207_MULTI_STATUS  # Partial success
        else:
            status_code = status.HTTP_200_OK
        
        return jsonify(response), status_code

    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR        
        
        

       
####### Medical Term


# d_ffn_insight_config — Individual Endpoint

INSIGHT_TABLE = "hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config"
MARKET_BRAND_TABLE = "hive_metastore.fieldforce_navigator_deployment.audio_market_brand_config"

# Cache for valid markets and brands 
_valid_markets_brands_cache = None
_cache_timestamp = None

import time
from datetime import datetime, timedelta
import pandas as pd

# Excel column to database column mapping for Insight Config
EXCEL_TO_DB_MAPPING_INSIGHT = {
    "MARKET": "MARKET",
    "BRAND": "BRAND",
    "Term": "VALUE_1"
}

# Fixed values for Insight Config
FIXED_INSIGHT_VALUE = "medical-word-reassignment"
FIXED_TYPE_VALUE = "TERM"

# Target sheet name in the template
TARGET_SHEET_NAME_INSIGHT = "Brand - Medical Terms"
HEADER_ROW_INDEX = 2  # 0-indexed, so row 3 is index 2

def _clean(s):  
    return " ".join(str(s).strip().split()) if s is not None else ""

def _sq(s):     
    return str(s).replace("'", "''")

def _is_nullish(v):
    """Return True if value should be treated as SQL NULL."""
    if v is None:
        return True
    t = str(v).strip()
    if t == "":
        return True
    return t.lower() == "null"

def _get_valid_markets_brands(force_refresh=False):
    """
    Get valid markets and brands from audio_market_brand_config table.
    Cache the results to avoid repeated queries.
    Returns a dict with 'markets' and 'brands' sets.
    """
    global _valid_markets_brands_cache, _cache_timestamp
    
    # Refresh cache every 5 minutes or on force_refresh
    cache_duration = timedelta(minutes=5)
    current_time = datetime.now()
    
    if (not force_refresh and 
        _valid_markets_brands_cache is not None and 
        _cache_timestamp is not None and 
        current_time - _cache_timestamp < cache_duration):
        return _valid_markets_brands_cache
    
    try:
        # Query distinct markets and brands
        sql = f"""
            SELECT 
                DISTINCT UPPER(TRIM(MARKET)) as MARKET,
                UPPER(TRIM(BRAND)) as BRAND
            FROM {MARKET_BRAND_TABLE}
            WHERE MARKET IS NOT NULL AND BRAND IS NOT NULL
        """
        
        df = dc.execute_query(sql)
        
        if df is not None and not df.empty:
            # lookup
            valid_markets = set(df['MARKET'].dropna().unique())
            valid_brands = set(df['BRAND'].dropna().unique())
            
            # Also create a set of valid combinations
            valid_combinations = set()
            for _, row in df.iterrows():
                if pd.notna(row['MARKET']) and pd.notna(row['BRAND']):
                    valid_combinations.add((row['MARKET'], row['BRAND']))
            
            _valid_markets_brands_cache = {
                'markets': valid_markets,
                'brands': valid_brands,
                'combinations': valid_combinations
            }
            _cache_timestamp = current_time
        else:
            # Empty table or query failed
            _valid_markets_brands_cache = {
                'markets': set(),
                'brands': set(),
                'combinations': set()
            }
            _cache_timestamp = current_time
            
    except Exception as e:
        print(f"Error fetching valid markets/brands: {str(e)}")
        # Return empty sets on error
        return {'markets': set(), 'brands': set(), 'combinations': set()}
    
    return _valid_markets_brands_cache

def _validate_market_brand_separately(market, brand):
    """
    Check if market AND brand exist separately in the audio_market_brand_config table
    """
    valid_data = _get_valid_markets_brands()
    
    market_upper = market.upper().strip()
    brand_upper = brand.upper().strip()
    
    market_valid = market_upper in valid_data['markets']
    brand_valid = brand_upper in valid_data['brands']
    
    return market_valid, brand_valid

def _batch_check_duplicates_insight(rows):
    """
    Highly optimized batch duplicate check using chunked queries for Insight Config.
    Checks on: BRAND, MARKET, INSIGHT, TYPE, VALUE_1
    Returns a set of tuples representing duplicates.
    """
    if not rows:
        return set()
    
    duplicate_set = set()
    
    # Process in chunks to avoid SQL query too large and timeout issues
    chunk_size = 25  # Reduced chunk size for faster query execution
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        conditions = []
        
        for r in chunk_rows:
            market = _sq(r["MARKET"].upper())
            brand = _sq(r["BRAND"].upper())
            insight = _sq(r["INSIGHT"])
            type_val = _sq(r["TYPE"])
            value_1 = _sq(r["VALUE_1"])
            
            conditions.append(
                f"(UPPER(TRIM(MARKET)) = '{market}' AND "
                f"UPPER(TRIM(BRAND)) = '{brand}' AND "
                f"TRIM(INSIGHT) = '{insight}' AND "
                f"TRIM(TYPE) = '{type_val}' AND "
                f"TRIM(VALUE_1) = '{value_1}')"
            )
        
        # Query each chunk
        sql = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET,
                   UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(INSIGHT) as INSIGHT,
                   TRIM(TYPE) as TYPE,
                   TRIM(VALUE_1) as VALUE_1
            FROM {INSIGHT_TABLE}
            WHERE {' OR '.join(conditions)}
        """
        
        try:
            df = dc.execute_query(sql)
            if df is not None and not df.empty:
                for _, row in df.iterrows():
                    duplicate_set.add((
                        row['MARKET'], 
                        row['BRAND'], 
                        row['INSIGHT'],
                        row['TYPE'],
                        row['VALUE_1']
                    ))
        except Exception as e:
            print(f"Batch duplicate check error (chunk {chunk_start//chunk_size + 1}): {str(e)}")
    
    return duplicate_set

def _batch_insert_rows_insight(rows, added_by):
    """
    Highly optimized batch insert using chunked execution to prevent timeouts.
    MARKET and BRAND are converted to UPPER CASE during insertion.
    """
    if not rows:
        return 0
    
    total_inserted = 0
    
    # Process in smaller chunks to avoid timeout
    chunk_size = 25
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        values_clauses = []
        
        for r in chunk_rows:
            # Convert MARKET and BRAND to UPPER CASE for insertion
            market_upper = r['MARKET'].upper()
            brand_upper = r['BRAND'].upper()
            
            # VALUE_2, VALUE_3, VALUE_4 are always NULL
            sql = f"""
                INSERT INTO {INSIGHT_TABLE}
                    (BRAND, MARKET, INSIGHT, TYPE, VALUE_1, VALUE_2, VALUE_3, VALUE_4, ADDED_BY, ENTRY_TIME)
                VALUES
                    ('{_sq(brand_upper)}', '{_sq(market_upper)}', '{_sq(r['INSIGHT'])}', '{_sq(r['TYPE'])}',
                     '{_sq(r['VALUE_1'])}', NULL, NULL, NULL,
                     '{_sq(added_by)}', CURRENT_TIMESTAMP())
            """
            values_clauses.append(sql)
        
        # Execute all inserts in the chunk
        for sql in values_clauses:
            try:
                dc.execute_non_query(sql)
                total_inserted += 1
            except Exception as e:
                print(f"Insert error: {str(e)}")
                raise
    
    return total_inserted

def _process_template_excel_insight(file_obj):
    """
    Process the template Excel file for Insight Config.
    Headers start from Column B (index 1), Row 3 (index 2).
    Data starts from Row 4, Column B onwards.
    Returns a tuple: (list of tuples (excel_row_num, row_dict), list of missing columns or None)
    """
    try:
        # Read all sheet names to verify target sheet exists
        excel_file = pd.ExcelFile(file_obj, engine="openpyxl")
        
        if TARGET_SHEET_NAME_INSIGHT not in excel_file.sheet_names:
            raise ValueError(
                f"Required sheet '{TARGET_SHEET_NAME_INSIGHT}' not found. "
                f"Available sheets: {', '.join(excel_file.sheet_names)}"
            )
        
        # Read the target sheet with header at row 3 (index 2)
        df = pd.read_excel(
            file_obj, 
            sheet_name=TARGET_SHEET_NAME_INSIGHT,
            header=HEADER_ROW_INDEX,
            engine="openpyxl"
        )
        
        # Drop the first column (Column A) if it exists
        if len(df.columns) > 0:
            df = df.iloc[:, 1:]  # Skip first column (Column A)
        
        # Clean column names (strip whitespace)
        df.columns = [str(c).strip() for c in df.columns]
        
        # Replace NaN/None with empty strings FIRST
        df = df.replace({pd.NA: "", None: ""}).fillna("")
        
        # Filter out completely empty rows but keep track of original row numbers
        # Row numbers in Excel: header at row 3, data starts at row 4
        df['_excel_row_num'] = range(4, 4 + len(df))  # Excel row numbers starting from 4
        df = df[df.drop('_excel_row_num', axis=1).astype(str).apply(lambda x: x.str.strip().str.len().sum(), axis=1) > 0]
        
        # Check which Excel columns are actually present
        available_excel_cols = set(df.columns) - {'_excel_row_num'}
        expected_excel_cols = set(EXCEL_TO_DB_MAPPING_INSIGHT.keys())
        found_cols = available_excel_cols.intersection(expected_excel_cols)
        missing_cols = expected_excel_cols - available_excel_cols
        
        # Track missing columns for warning
        missing_columns_list = list(missing_cols) if missing_cols else None
        
        # Log what was found and what's missing
        if missing_cols:
            print(f"Info: Missing Excel columns (will cause error): {missing_cols}")
        if found_cols:
            print(f"Info: Found Excel columns: {found_cols}")
        
        # Mandatory columns check - all 3 columns are mandatory
        mandatory_excel_cols = ["MARKET", "BRAND", "Term"]
        missing_mandatory = [col for col in mandatory_excel_cols if col not in df.columns]
        
        if missing_mandatory:
            raise ValueError(
                f"Missing mandatory columns in '{TARGET_SHEET_NAME_INSIGHT}' sheet: {', '.join(missing_mandatory)}. "
                f"Found columns: {list(df.columns)}"
            )
        
        # Apply mapping
        mapped_data = []
        
        for idx, row in df.iterrows():
            excel_row_num = int(row['_excel_row_num'])
            mapped_row = {}
            
            # Map each Excel column to database column
            for excel_col, db_col in EXCEL_TO_DB_MAPPING_INSIGHT.items():
                if excel_col in df.columns:
                    # Column exists in Excel, map the value
                    value = row.get(excel_col, "")
                    mapped_row[db_col] = _clean(value)
                else:
                    # Column missing - should not happen as we check mandatory above
                    mapped_row[db_col] = ""
            
            # Add fixed values
            mapped_row["INSIGHT"] = FIXED_INSIGHT_VALUE
            mapped_row["TYPE"] = FIXED_TYPE_VALUE
            mapped_row["VALUE_2"] = None
            mapped_row["VALUE_3"] = None
            mapped_row["VALUE_4"] = None
            
            # Validate - all 3 mandatory fields must have values
            if (mapped_row.get("MARKET") and 
                mapped_row.get("BRAND") and 
                mapped_row.get("VALUE_1")):
                # Store tuple of (excel_row_number, data_dict)
                mapped_data.append((excel_row_num, mapped_row))
        
        # After processing all rows, check if we found any valid data
        if not mapped_data:
            raise ValueError(
                f"No valid data rows found in sheet '{TARGET_SHEET_NAME_INSIGHT}'. "
                f"Ensure mandatory fields (MARKET, BRAND, Term) have values. "
                f"Found columns: {list(df.columns)}"
            )
        
        # Return both mapped data (with row numbers) AND missing columns info
        return mapped_data, missing_columns_list
        
    except Exception as e:
        raise Exception(f"Error processing template Excel: {str(e)}")

# GET: column definitions + simple post template 
@app.route("/d_ffn_insight_config_medical_term", methods=["GET"])
def insight_medical_term_get():
    try:
        definitions = {
            "Description": {
                "BRAND": {
                    "Example Values": ["AREXVY", "SHINGRIX"],
                    "Description": "Name of Brand to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "MARKET": {
                    "Example Values": ["GBR", "THA", "ITA", "PRT"],
                    "Description": "Name of the Market to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "INSIGHT": {
                    "Example Values": ["medical-word-reassignment"],
                    "Description": "Fixed value for insight type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "TYPE": {
                    "Example Values": ["TERM"],
                    "Description": "Fixed value for type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "VALUE_1": {
                    "Example Values": ["Key Medical Term text"],
                    "Description": "The actual Medical Term content",
                    "Parameter_type": "Mandatory"
                },
                "VALUE_2": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_3": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_4": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "ADDED_BY": {
                    "Example Values": ["abc.x.abc@gsk.com"],
                    "Description": "Auto-picked from 'Username' header in POST",
                    "Parameter_type": "Mandatory (via header)"
                },
                "ENTRY_TIME": {
                    "Example Values": ["current_timestamp()"],
                    "Description": "Set by system at insert time",
                    "Parameter_type": "Auto-generated"
                }
            },
            "sample_json": {
                "MARKET": "THA",
                "BRAND": "AREXVY",
                "VALUE_1": "This is a key Medical Term"
            },
            "excel_upload_instructions": {
                "expected_extension": ".xlsx",
                "template_sheet_name": TARGET_SHEET_NAME_INSIGHT,
                "header_row": HEADER_ROW_INDEX + 1,
                "form_field_name": "file",
                "required_columns": ["MARKET", "BRAND", "Term"],
                "column_mapping": EXCEL_TO_DB_MAPPING_INSIGHT,
                "fixed_values": {
                    "INSIGHT": FIXED_INSIGHT_VALUE,
                    "TYPE": FIXED_TYPE_VALUE,
                    "VALUE_2": "NULL",
                    "VALUE_3": "NULL",
                    "VALUE_4": "NULL"
                },
                "notes": [
                    "Tablename: hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config",
                    f"The Excel file must contain a sheet named '{TARGET_SHEET_NAME_INSIGHT}'",
                    f"Headers must be in row {HEADER_ROW_INDEX + 1} of the sheet, starting from Column B",
                    "Data rows should start from row 4 onwards, Column B onwards",
                    "Column A is ignored during processing",
                    "INSIGHT and TYPE are automatically set to fixed values",
                    "VALUE_2, VALUE_3, VALUE_4 are automatically set to NULL"
                ]
            }
        }
        return jsonify(definitions), status.HTTP_200_OK
    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR

# POST: JSON (single/list) OR Excel (.xlsx) 
@app.route("/d_ffn_insight_config_medical_term", methods=["POST"])
def insight_medical_term_post():
    try:
        # capture inserting user
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing 'Username' in headers."}), status.HTTP_400_BAD_REQUEST

        # Refresh cache at the start of POST request to get latest valid values
        _get_valid_markets_brands(force_refresh=True)

        rows = []  # list of tuples (row_num, dict)
        is_excel_mode = False
        missing_columns_warning = None

        if "file" in request.files:
            # Excel (multipart/form-data) - TEMPLATE BASED
            is_excel_mode = True
            f = request.files["file"]
            fname = (f.filename or "").lower().strip()
            
            if not fname.endswith(".xlsx"):
                return jsonify({
                    "error": "Only .xlsx files are supported for bulk upload."
                }), status.HTTP_400_BAD_REQUEST

            try:
                # Process template Excel with mapping - returns (rows, missing_columns)
                rows, missing_columns_warning = _process_template_excel_insight(f)
                
                if not rows:
                    return jsonify({
                        "error": f"No valid data rows found in sheet '{TARGET_SHEET_NAME_INSIGHT}'. "
                                 f"Ensure headers are in row {HEADER_ROW_INDEX + 1}, starting from Column B, and data starts from row 4."
                    }), status.HTTP_400_BAD_REQUEST
                
            except ValueError as ve:
                return jsonify({"error": str(ve)}), status.HTTP_400_BAD_REQUEST
            except Exception as e:
                return jsonify({
                    "error": f"Failed to process Excel template: {str(e)}"
                }), status.HTTP_400_BAD_REQUEST

        else:
            # JSON (single object or list)
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand  = _clean(rec.get("BRAND", ""))
                value_1 = _clean(rec.get("VALUE_1", ""))

                # Check all mandatory fields
                if not market or not brand or not value_1:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Missing required field(s). MARKET, BRAND, and VALUE_1 are mandatory."
                    }), status.HTTP_400_BAD_REQUEST

                # Validate MARKET and BRAND separately
                market_valid, brand_valid = _validate_market_brand_separately(market, brand)
                
                if not market_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid MARKET '{market}'. This market does not exist."
                    }), status.HTTP_400_BAD_REQUEST
                
                if not brand_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid BRAND '{brand}'. This brand does not exist."
                    }), status.HTTP_400_BAD_REQUEST

                # For JSON, use item number as row identifier
                rows.append((i, {
                    "MARKET": market,
                    "BRAND": brand,
                    "INSIGHT": FIXED_INSIGHT_VALUE,
                    "TYPE": FIXED_TYPE_VALUE,
                    "VALUE_1": value_1,
                    "VALUE_2": None,
                    "VALUE_3": None,
                    "VALUE_4": None
                }))

        if not rows:
            return jsonify({"error": "No valid rows to process."}), status.HTTP_400_BAD_REQUEST

        # Validate all rows upfront - DON'T stop at first error, collect ALL errors
        invalid_markets = []
        invalid_brands = []
        valid_rows = []
        
        valid_data = _get_valid_markets_brands()
        
        # rows is now a list of tuples: (row_number, data_dict)
        for row_num, r in rows:
            market = r["MARKET"]
            brand = r["BRAND"]
            
            market_valid = market.upper().strip() in valid_data['markets']
            brand_valid = brand.upper().strip() in valid_data['brands']
            
            if not market_valid:
                invalid_markets.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid MARKET - not found in audio_market_brand_config"
                })
                continue
            
            if not brand_valid:
                invalid_brands.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid BRAND - not found in audio_market_brand_config"
                })
                continue
            
            # This row is valid
            valid_rows.append((row_num, r))
        
        # Process valid rows even if there are some invalid ones
        # Batch duplicate check (optimized)
        duplicate_set = _batch_check_duplicates_insight([r for _, r in valid_rows])
        
        # Separate duplicates from insertable rows
        duplicates = []
        insertable_rows = []
        
        for row_num, r in valid_rows:
            # Check duplicate based on MARKET, BRAND, INSIGHT, TYPE, VALUE_1
            key = (
                r["MARKET"].upper().strip(),
                r["BRAND"].upper().strip(),
                r["INSIGHT"].strip(),
                r["TYPE"].strip(),
                r["VALUE_1"].strip()
            )
            
            if key in duplicate_set:
                duplicates.append({
                    "excel_row": row_num,
                    "MARKET": r["MARKET"],
                    "BRAND": r["BRAND"],
                    "VALUE_1": r["VALUE_1"],
                    "reason": "Duplicate - already exists in database"
                })
            else:
                insertable_rows.append(r)
        
        # Batch insert with optimized chunking
        inserted = 0
        errors = []
        
        if insertable_rows:
            try:
                # Smaller batch size (25) for faster execution and timeout prevention
                inserted = _batch_insert_rows_insight(insertable_rows, added_by)
            except Exception as e:
                errors.append({
                    "error": f"Batch insert failed: {str(e)}"
                })

        response = {
            "status": "success" if inserted and not errors else ("partial" if inserted else "no-change"),
            "mode": "excel-template" if is_excel_mode else "json",
            "sheet_processed": TARGET_SHEET_NAME_INSIGHT if is_excel_mode else None,
            "total_rows_processed": len(rows),
            "inserted_count": inserted,
            "duplicate_count": len(duplicates),
            "validation_errors_count": len(invalid_markets) + len(invalid_brands),
            "skipped_duplicates": duplicates,
            "invalid_markets": invalid_markets,
            "invalid_brands": invalid_brands,
            "row_errors": errors
        }
        
        # Add warning if columns were missing in Excel template
        if is_excel_mode and missing_columns_warning:
            response["warning"] = {
                "message": "The uploaded Excel template is missing some required columns.",
                "missing_columns": missing_columns_warning,
                "note": "All three columns (MARKET, BRAND, Term) are required for Insight Config."
            }
        
        # Remove None and empty lists from response 
        response = {k: v for k, v in response.items() 
                   if v is not None and not (isinstance(v, list) and len(v) == 0)}
        
        # Determine HTTP status code based on results
        if inserted > 0:
            status_code = status.HTTP_201_CREATED
        elif invalid_markets or invalid_brands:
            status_code = status.HTTP_207_MULTI_STATUS  # Partial success
        else:
            status_code = status.HTTP_200_OK
        
        return jsonify(response), status_code

    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR  
