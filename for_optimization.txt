# Competitor Config â€“ Individual Endpoint 

COMP_TABLE = "hive_metastore.fieldforce_navigator_deployment.d_ffn_competitor_config"
MARKET_BRAND_TABLE = "hive_metastore.fieldforce_navigator_deployment.audio_market_brand_config"

# Cache for valid markets and brands 
_valid_markets_brands_cache = None
_cache_timestamp = None

import time
from datetime import datetime, timedelta
import pandas as pd

#column mapping
EXCEL_TO_DB_MAPPING = {
    "Market": "MARKET",
    "Brand": "BRAND",
    "Disease State": "DISEASE_STATE",
    "Drug Class": "DRUG_CLASS",
    "Brands": "COMPETITOR_BRANDS",
    "Competition Type": "COMPETITION_TYPE",
    "Molecule": "MOLECULE",
    "Other names": "OTHER_NAMES",
    "Combination": "COMBINATION"
}

# Target sheet name 
TARGET_SHEET_NAME = "Brand - Competitors"
HEADER_ROW_INDEX = 2  

def _clean(s):  
    return " ".join(str(s).strip().split()) if s is not None else ""

def _sq(s):     
    return str(s).replace("'", "''")

def _is_nullish(v):
    """Return True if value should be treated as SQL NULL."""
    if v is None:
        return True
    t = str(v).strip()
    if t == "":
        return True
    return t.lower() == "null"

def _get_valid_markets_brands(force_refresh=False):
    """
    Get valid markets and brands from audio_market_brand_config table.
    Cache the results to avoid repeated queries.
    Returns a dict with 'markets' and 'brands' sets.
    """
    global _valid_markets_brands_cache, _cache_timestamp
    
    
    cache_duration = timedelta(minutes=5)
    current_time = datetime.now()
    
    if (not force_refresh and 
        _valid_markets_brands_cache is not None and 
        _cache_timestamp is not None and 
        current_time - _cache_timestamp < cache_duration):
        return _valid_markets_brands_cache
    
    try:
       
        sql = f"""
            SELECT 
                DISTINCT UPPER(TRIM(MARKET)) as MARKET,
                UPPER(TRIM(BRAND)) as BRAND
            FROM {MARKET_BRAND_TABLE}
            WHERE MARKET IS NOT NULL AND BRAND IS NOT NULL
        """
        
        df = dc.execute_query(sql)
        
        if df is not None and not df.empty:
            # lookup
            valid_markets = set(df['MARKET'].dropna().unique())
            valid_brands = set(df['BRAND'].dropna().unique())
            
            
            valid_combinations = set()
            for _, row in df.iterrows():
                if pd.notna(row['MARKET']) and pd.notna(row['BRAND']):
                    valid_combinations.add((row['MARKET'], row['BRAND']))
            
            _valid_markets_brands_cache = {
                'markets': valid_markets,
                'brands': valid_brands,
                'combinations': valid_combinations
            }
            _cache_timestamp = current_time
        else:
            
            _valid_markets_brands_cache = {
                'markets': set(),
                'brands': set(),
                'combinations': set()
            }
            _cache_timestamp = current_time
            
    except Exception as e:
        print(f"Error fetching valid markets/brands: {str(e)}")
        
        return {'markets': set(), 'brands': set(), 'combinations': set()}
    
    return _valid_markets_brands_cache

def _validate_market_brand_separately(market, brand):
    """
    Check if market AND brand exist separately in the audio_market_brand_config table
    """
    valid_data = _get_valid_markets_brands()
    
    market_upper = market.upper().strip()
    brand_upper = brand.upper().strip()
    
    market_valid = market_upper in valid_data['markets']
    brand_valid = brand_upper in valid_data['brands']
    
    return market_valid, brand_valid

def _validate_market_brand_combination(market, brand):
    """
    Check if the specific market-brand combination exists in audio_market_brand_config table.
    """
    valid_data = _get_valid_markets_brands()
    
    market_upper = market.upper().strip()
    brand_upper = brand.upper().strip()
    
    return (market_upper, brand_upper) in valid_data['combinations']

def _exists_cmpt_row(market, brand, disease_class, comp_brand, comp_type, molecule, disease_state):

    

    if _is_nullish(molecule):
        molecule_condition = "MOLECULE IS NULL"
    else:
        molecule_condition = f"TRIM(MOLECULE) = '{_sq(molecule)}'"
    
    
    if _is_nullish(disease_state):
        disease_state_condition = "DISEASE_STATE IS NULL"
    else:
        disease_state_condition = f"TRIM(DISEASE_STATE) = '{_sq(disease_state)}'"
    
    sql = f"""
        SELECT *
        FROM {COMP_TABLE}
        WHERE UPPER(TRIM(MARKET))          = '{_sq(market.upper())}'
          AND UPPER(TRIM(BRAND))           = '{_sq(brand.upper())}'
          AND TRIM(DRUG_CLASS)             = '{_sq(disease_class)}'
          AND TRIM(COMPETITOR_BRANDS)      = '{_sq(comp_brand)}'
          AND TRIM(COMPETITION_TYPE)       = '{_sq(comp_type)}'
          AND {molecule_condition}
          AND {disease_state_condition}
        LIMIT 1
    """
    df = dc.execute_query(sql)
    return df is not None and not df.empty

def _batch_check_duplicates(rows):
    """
    Highly optimized batch duplicate check using chunked queries.
    Checks on: BRAND, MARKET, DRUG_CLASS, COMPETITOR_BRANDS, COMPETITION_TYPE, MOLECULE, DISEASE_STATE
    Returns a set of tuples representing duplicates.
    """
    if not rows:
        return set()
    
    duplicate_set = set()
    
    # Process in chunks
    chunk_size = 25 
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        conditions = []
        
        for r in chunk_rows:
            market = _sq(r["MARKET"].upper())
            brand = _sq(r["BRAND"].upper())
            drug_class = _sq(r["DRUG_CLASS"])
            cbrand = _sq(r["COMPETITOR_BRANDS"])
            ctype = _sq(r["COMPETITION_TYPE"])
            molecule = r.get("MOLECULE", "")
            disease_state = r.get("DISEASE_STATE", "")
            
           
            if _is_nullish(molecule):
                molecule_condition = "MOLECULE IS NULL"
            else:
                molecule_condition = f"TRIM(MOLECULE) = '{_sq(molecule)}'"
            
          
            if _is_nullish(disease_state):
                disease_state_condition = "DISEASE_STATE IS NULL"
            else:
                disease_state_condition = f"TRIM(DISEASE_STATE) = '{_sq(disease_state)}'"
            
            conditions.append(
                f"(UPPER(TRIM(MARKET)) = '{market}' AND "
                f"UPPER(TRIM(BRAND)) = '{brand}' AND "
                f"TRIM(DRUG_CLASS) = '{drug_class}' AND "
                f"TRIM(COMPETITOR_BRANDS) = '{cbrand}' AND "
                f"TRIM(COMPETITION_TYPE) = '{ctype}' AND "
                f"{molecule_condition} AND "
                f"{disease_state_condition})"
            )
        
        # Query each chunk
        sql = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET,
                   UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(DRUG_CLASS) as DRUG_CLASS,
                   TRIM(COMPETITOR_BRANDS) as COMPETITOR_BRANDS,
                   TRIM(COMPETITION_TYPE) as COMPETITION_TYPE,
                   TRIM(MOLECULE) as MOLECULE,
                   TRIM(DISEASE_STATE) as DISEASE_STATE
            FROM {COMP_TABLE}
            WHERE {' OR '.join(conditions)}
        """
        
        try:
            df = dc.execute_query(sql)
            if df is not None and not df.empty:
                for _, row in df.iterrows():
                    mol_val = None if pd.isna(row['MOLECULE']) or row['MOLECULE'] == '' else row['MOLECULE']
                    disease_val = None if pd.isna(row['DISEASE_STATE']) or row['DISEASE_STATE'] == '' else row['DISEASE_STATE']
                    
                    duplicate_set.add((
                        row['MARKET'], 
                        row['BRAND'], 
                        row['DRUG_CLASS'],
                        row['COMPETITOR_BRANDS'], 
                        row['COMPETITION_TYPE'],
                        mol_val,
                        disease_val
                    ))
        except Exception as e:
            print(f"Batch duplicate check error (chunk {chunk_start//chunk_size + 1}): {str(e)}")
    
    return duplicate_set

def _batch_insert_rows(rows, added_by):
    """
    Highly optimized batch insert using chunked execution to prevent timeouts.
    MARKET and BRAND are converted to UPPER CASE during insertion.
    """
    if not rows:
        return 0
    
    total_inserted = 0
    
    # Process in chunks 
    chunk_size = 25
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        values_clauses = []
        
        for r in chunk_rows:
            # Convert MARKET and BRAND to UPPER CASE for insertion
            market_upper = r['MARKET'].upper()
            brand_upper = r['BRAND'].upper()
            
            mol_sql = "NULL" if _is_nullish(r.get("MOLECULE")) else f"'{_sq(r['MOLECULE'])}'"
            oth_sql = "NULL" if _is_nullish(r.get("OTHER_NAMES")) else f"'{_sq(r['OTHER_NAMES'])}'"
            cmb_sql = "NULL" if _is_nullish(r.get("COMBINATION")) else f"'{_sq(r['COMBINATION'])}'"
            
            values_clauses.append(
                f"('{_sq(market_upper)}','{_sq(brand_upper)}','{_sq(r['DISEASE_STATE'])}','{_sq(r['DRUG_CLASS'])}', "
                f"'{_sq(r['COMPETITOR_BRANDS'])}','{_sq(r['COMPETITION_TYPE'])}', "
                f"'{_sq(added_by)}',CURRENT_TIMESTAMP(), "
                f"{mol_sql},{oth_sql},{cmb_sql})"
            )
        
        sql = f"""
            INSERT INTO {COMP_TABLE}
                (MARKET, BRAND, DISEASE_STATE, DRUG_CLASS,
                 COMPETITOR_BRANDS, COMPETITION_TYPE,
                 ADDED_BY, ENTRY_TIME,
                 MOLECULE, OTHER_NAMES, COMBINATION)
            VALUES
                {','.join(values_clauses)}
        """
        
        try:
            dc.execute_non_query(sql)
            total_inserted += len(chunk_rows)
        except Exception as e:
            print(f"Batch insert error (chunk {chunk_start//chunk_size + 1}): {str(e)}")
            raise 
    
    return total_inserted

def _process_template_excel(file_obj):
    """
    Process the template Excel file with specific sheet and header mapping.
    """
    try:
        
        excel_file = pd.ExcelFile(file_obj, engine="openpyxl")
        
        if TARGET_SHEET_NAME not in excel_file.sheet_names:
            raise ValueError(
                f"Required sheet '{TARGET_SHEET_NAME}' not found. "
                f"Available sheets: {', '.join(excel_file.sheet_names)}"
            )
        
        # Read the target sheet
        df = pd.read_excel(
            file_obj, 
            sheet_name=TARGET_SHEET_NAME,
            header=HEADER_ROW_INDEX,
            engine="openpyxl"
        )
        
       
        if len(df.columns) > 0:
            df = df.iloc[:, 1:]  
        
        # Clean column names (strip whitespace)
        df.columns = [str(c).strip() for c in df.columns]
        
        
        df = df.replace({pd.NA: "", None: ""}).fillna("")
        
   
        df = df[df.astype(str).apply(lambda x: x.str.strip().str.len().sum(), axis=1) > 0]
        
        # Check which Excel columns
        available_excel_cols = set(df.columns)
        expected_excel_cols = set(EXCEL_TO_DB_MAPPING.keys())
        found_cols = available_excel_cols.intersection(expected_excel_cols)
        missing_cols = expected_excel_cols - available_excel_cols
        
        # Track missing columns 
        missing_columns_list = list(missing_cols) if missing_cols else None
        
        
        if missing_cols:
            print(f"Info: Missing Excel columns (will be NULL): {missing_cols}")
        if found_cols:
            print(f"Info: Found Excel columns: {found_cols}")
        
        
        mandatory_excel_cols = ["Market", "Brand", "Disease State", "Drug Class", 
                               "Brands", "Competition Type"]
        missing_mandatory = [col for col in mandatory_excel_cols if col not in df.columns]
        
        if missing_mandatory:
            raise ValueError(
                f"Missing mandatory columns in '{TARGET_SHEET_NAME}' sheet: {', '.join(missing_mandatory)}. "
                f"Found columns: {list(df.columns)}"
            )
        
        # Apply  mapping
        mapped_data = []
        
        for idx, row in df.iterrows():
            mapped_row = {}
            
            # Map each Excel column to database column
            for excel_col, db_col in EXCEL_TO_DB_MAPPING.items():
                if excel_col in df.columns:
                    # Column exists in Excel
                    value = row.get(excel_col, "")
                    mapped_row[db_col] = _clean(value)
                else:
                    # Column missing in Excel, will be inserted as NULL
                    mapped_row[db_col] = "" 
            
            # Validate AFTER mapping 
            # Only mandatory fields need to have values
            if (mapped_row.get("MARKET") and 
                mapped_row.get("BRAND") and 
                mapped_row.get("DISEASE_STATE") and 
                mapped_row.get("DRUG_CLASS") and 
                mapped_row.get("COMPETITOR_BRANDS") and 
                mapped_row.get("COMPETITION_TYPE")):
                mapped_data.append(mapped_row)
        
        # After processing all rows
        if not mapped_data:
            raise ValueError(
                f"No valid data rows found in sheet '{TARGET_SHEET_NAME}'. "
                f"Ensure mandatory fields (Market, Brand, Disease State, Drug Class, Brands, Competition Type) have values. "
                f"Found columns: {list(df.columns)}"
            )
        
       
        return mapped_data, missing_columns_list
        
    except Exception as e:
        raise Exception(f"Error processing template Excel: {str(e)}")

# GET: column definitions + simple post template 
@app.route("/d_ffn_competitor_config", methods=["GET"])
def competitor_get():
    try:
        
        valid_data = _get_valid_markets_brands()
        
        definitions = {
            "Description": {
                "MARKET": {
                    "Example Values": ["GBR", "THA", "ITA", "PRT"],
                    "Description": "Name of the Market to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "BRAND": {
                    "Example Values": ["AREXVY", "SHINGRIX"],
                    "Description": "Name of Brand to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "DISEASE_STATE": {
                    "Example Values": ["RSV vaccine", "Shingles"],
                    "Description": "Primary disease state or indication",
                    "Parameter_type": "Mandatory"
                },
                "DRUG_CLASS": {
                    "Example Values": ["Non-adjuvanted", "Adjuvanted"],
                    "Description": "Drug class/category",
                    "Parameter_type": "Mandatory"
                },
                "COMPETITOR_BRANDS": {
                    "Example Values": ["Abrysvo"],
                    "Description": "Competing brand name",
                    "Parameter_type": "Mandatory"
                },
                "COMPETITION_TYPE": {
                    "Example Values": ["COMPETITOR", "ALTERNATIVE"],
                    "Description": "Relationship type vs our brand",
                    "Parameter_type": "Mandatory"
                },
                "MOLECULE": {
                    "Example Values": ["RSVPreF"],
                    "Description": "Active component or molecule",
                    "Parameter_type": "Optional"
                },
                "OTHER_NAMES": {
                    "Example Values": ["null"],
                    "Description": "Aliases / other names",
                    "Parameter_type": "Optional"
                },
                "COMBINATION": {
                    "Example Values": ["null"],
                    "Description": "Combination component if any",
                    "Parameter_type": "Optional"
                },
                "ADDED_BY": {
                    "Example Values": ["abc.x.abc@gsk.com"],
                    "Description": "Auto-picked from 'Username' header in POST",
                    "Parameter_type": "Mandatory (via header)"
                },
                "ENTRY_TIME": {
                    "Example Values": ["current_timestamp()"],
                    "Description": "Set by system at insert time",
                    "Parameter_type": "Auto-generated"
                }
            },
            "sample_json": {
                "MARKET": "THA",
                "BRAND": "AREXVY",
                "DISEASE_STATE": "RSV vaccine",
                "DRUG_CLASS": "Non-adjuvanted",
                "COMPETITOR_BRANDS": "Abrysvo",
                "COMPETITION_TYPE": "COMPETITOR",
                "MOLECULE": "null",
                "OTHER_NAMES": "null",
                "COMBINATION": "null"
            },
            "excel_upload_instructions": {
                "expected_extension": ".xlsx",
                "template_sheet_name": TARGET_SHEET_NAME,
                "form_field_name": "file",
                "required_columns": ["Market", "Brand", "Disease State", "Drug Class", "Brands", "Competition Type"],
                "optional_columns": ["Molecule", "Other names", "Combination"],
                "notes": [
                    "Tablename: hive_metastore.fieldforce_navigator_deployment.d_ffn_competitor_config",
                    f"The Excel file must contain a sheet named '{TARGET_SHEET_NAME}'",
                    "When optional columns are empty, they will be saved as NULL"
                ]
            }
        }
        return jsonify(definitions), status.HTTP_200_OK
    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR

# POST: JSON (single/list) OR Excel (.xlsx) 
@app.route("/d_ffn_competitor_config", methods=["POST"])
def competitor_post():
    try:
        # capture inserting user
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing 'Username' in headers."}), status.HTTP_400_BAD_REQUEST

        # Refresh cache at the start of POST request to get latest valid values
        _get_valid_markets_brands(force_refresh=True)

        rows = []  # list of dicts
        is_excel_mode = False
        missing_columns_warning = None  # Track missing columns for warning

        if "file" in request.files:
            # Excel (multipart/form-data) - TEMPLATE BASED
            is_excel_mode = True
            f = request.files["file"]
            fname = (f.filename or "").lower().strip()
            
            if not fname.endswith(".xlsx"):
                return jsonify({
                    "error": "Only .xlsx files are supported for bulk upload."
                }), status.HTTP_400_BAD_REQUEST

            try:
                # Process template Excel with mapping - returns (rows, missing_columns)
                rows, missing_columns_warning = _process_template_excel(f)
                
                if not rows:
                    return jsonify({
                        "error": f"No valid data rows found in sheet '{TARGET_SHEET_NAME}'. "
                                 f"Ensure headers are in row {HEADER_ROW_INDEX + 1}, starting from Column B, and data starts from row 4."
                    }), status.HTTP_400_BAD_REQUEST
                
            except ValueError as ve:
                return jsonify({"error": str(ve)}), status.HTTP_400_BAD_REQUEST
            except Exception as e:
                return jsonify({
                    "error": f"Failed to process Excel template: {str(e)}"
                }), status.HTTP_400_BAD_REQUEST

        else:
            # JSON (single object or list) 
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand  = _clean(rec.get("BRAND", ""))
                disease = _clean(rec.get("DISEASE_STATE", ""))
                dclass  = _clean(rec.get("DRUG_CLASS", ""))
                cbrand = _clean(rec.get("COMPETITOR_BRANDS", ""))
                ctype  = _clean(rec.get("COMPETITION_TYPE", ""))

                # Check all mandatory fields
                if not market or not brand or not disease or not dclass or not cbrand or not ctype:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Missing required field(s). All of MARKET, BRAND, DISEASE_STATE, DRUG_CLASS, COMPETITOR_BRANDS, and COMPETITION_TYPE are mandatory."
                    }), status.HTTP_400_BAD_REQUEST

                # Validate MARKET and BRAND separately
                market_valid, brand_valid = _validate_market_brand_separately(market, brand)
                
                if not market_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid MARKET '{market}'. This market does not exist."
                    }), status.HTTP_400_BAD_REQUEST
                
                if not brand_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid BRAND '{brand}'. This brand does not exist."
                    }), status.HTTP_400_BAD_REQUEST

                # optional columns
                molec   = _clean(rec.get("MOLECULE", "null"))
                other   = _clean(rec.get("OTHER_NAMES", "null"))
                comb    = _clean(rec.get("COMBINATION", "null"))

                rows.append({
                    "MARKET": market,
                    "BRAND": brand,
                    "DISEASE_STATE": disease,
                    "DRUG_CLASS": dclass,
                    "COMPETITOR_BRANDS": cbrand,
                    "COMPETITION_TYPE": ctype,
                    "MOLECULE": molec,
                    "OTHER_NAMES": other,
                    "COMBINATION": comb
                })

        if not rows:
            return jsonify({"error": "No valid rows to process."}), status.HTTP_400_BAD_REQUEST

        # Validate all rows  
        invalid_markets = []
        invalid_brands = []
        valid_rows = []
        
        valid_data = _get_valid_markets_brands()
        
        for idx, r in enumerate(rows, start=1):
            market = r["MARKET"]
            brand = r["BRAND"]
            
            market_valid = market.upper().strip() in valid_data['markets']
            brand_valid = brand.upper().strip() in valid_data['brands']
            
            if not market_valid:
                invalid_markets.append({
                    "row": idx,
                    "MARKET": market,
                    "reason": "MARKET not found in audio_market_brand_config"
                })
                continue
            
            if not brand_valid:
                invalid_brands.append({
                    "row": idx,
                    "BRAND": brand,
                    "reason": "BRAND not found in audio_market_brand_config"
                })
                continue
            
            valid_rows.append((idx, r))
        
        # If there are validation errors, return them
        if invalid_markets or invalid_brands:
            response = {
                "status": "error",
                "message": "Validation failed for one or more rows",
                "invalid_markets": invalid_markets,
                "invalid_brands": invalid_brands,
                "total_rows": len(rows),
                "failed_rows": len(invalid_markets) + len(invalid_brands)
            }
            return jsonify(response), status.HTTP_400_BAD_REQUEST
        
        # Batch duplicate check 
        duplicate_set = _batch_check_duplicates([r for _, r in valid_rows])
        
        # Separate duplicates from insertable rows
        duplicates = []
        insertable_rows = []
        
        for idx, r in valid_rows:
            # Use None for NULL/empty optional values
            mol_val = None if _is_nullish(r.get("MOLECULE", "")) else r.get("MOLECULE", "")
            disease_val = None if _is_nullish(r.get("DISEASE_STATE", "")) else r.get("DISEASE_STATE", "")
            
            # Only MARKET and BRAND are uppercased, other columns maintain their case
            key = (
                r["MARKET"].upper().strip(),
                r["BRAND"].upper().strip(),
                r["DRUG_CLASS"].strip(),
                r["COMPETITOR_BRANDS"].strip(),
                r["COMPETITION_TYPE"].strip(),
                mol_val,
                disease_val
            )
            
            if key in duplicate_set:
                duplicates.append({
                    "row": idx,
                    "MARKET": r["MARKET"],
                    "BRAND": r["BRAND"],
                    "DRUG_CLASS": r["DRUG_CLASS"],
                    "COMPETITOR_BRANDS": r["COMPETITOR_BRANDS"],
                    "COMPETITION_TYPE": r["COMPETITION_TYPE"],
                    "MOLECULE": r.get("MOLECULE", ""),
                    "DISEASE_STATE": r.get("DISEASE_STATE", ""),
                    "reason": "duplicate"
                })
            else:
                insertable_rows.append(r)
        
        # Batch insert with optimized chunking
        inserted = 0
        errors = []
        
        if insertable_rows:
            try:
                
                inserted = _batch_insert_rows(insertable_rows, added_by)
            except Exception as e:
                errors.append({
                    "error": f"Batch insert failed: {str(e)}"
                })

        response = {
            "status": "success" if inserted and not errors else ("partial" if inserted else "no-change"),
            "mode": "excel-template" if is_excel_mode else "json",
            "sheet_processed": TARGET_SHEET_NAME if is_excel_mode else None,
            "total_rows_processed": len(rows),
            "inserted_count": inserted,
            "duplicate_count": len(duplicates),
            "skipped_duplicates": duplicates,
            "row_errors": errors
        }
        
        # Add warning if columns were missing in Excel template
        if is_excel_mode and missing_columns_warning:
            response["warning"] = {
                "message": "The uploaded Excel template is missing some optional columns. Missing columns have been inserted as NULL in the database",
                "missing_columns": missing_columns_warning,
                "note": "To avoid this warning, ensure your Excel template includes all expected columns: Market, Brand, Disease State, Drug Class, Brands, Competition Type, Molecule, Other names, Combination"
            }
        
        # Remove None and empty lists from response 
        response = {k: v for k, v in response.items() 
                   if v is not None and not (isinstance(v, list) and len(v) == 0)}
        
        return jsonify(response), (status.HTTP_201_CREATED if inserted else status.HTTP_200_OK)

    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR
		
		
		
		
		
		
		
		

# d_ffn_insight_config - Multiple Independent Endpoints

INSIGHT_TABLE = "hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config"
MARKET_BRAND_TABLE = "hive_metastore.fieldforce_navigator_deployment.audio_market_brand_config"

# Shared cache for all endpoints
_valid_markets_brands_cache = None
_cache_timestamp = None

import time
from datetime import datetime, timedelta
import pandas as pd

# ============================================================================
# SHARED UTILITY FUNCTIONS (used by all endpoints)
# ============================================================================

def _clean(s):  
    return " ".join(str(s).strip().split()) if s is not None else ""

def _sq(s):     
    return str(s).replace("'", "''")

def _is_nullish(v):
    if v is None:
        return True
    t = str(v).strip()
    if t == "":
        return True
    return t.lower() == "null"

def _get_valid_markets_brands(force_refresh=False):
    """Get valid markets and brands from audio_market_brand_config, cached for 5 min"""
    global _valid_markets_brands_cache, _cache_timestamp
    
    cache_duration = timedelta(minutes=5)
    current_time = datetime.now()
    
    if (not force_refresh and 
        _valid_markets_brands_cache is not None and 
        _cache_timestamp is not None and 
        current_time - _cache_timestamp < cache_duration):
        return _valid_markets_brands_cache
    
    try:
        sql = f"""
            SELECT 
                DISTINCT UPPER(TRIM(MARKET)) as MARKET,
                UPPER(TRIM(BRAND)) as BRAND
            FROM {MARKET_BRAND_TABLE}
            WHERE MARKET IS NOT NULL AND BRAND IS NOT NULL
        """
        
        df = dc.execute_query(sql)
        
        if df is not None and not df.empty:
            valid_markets = set(df['MARKET'].dropna().unique())
            valid_brands = set(df['BRAND'].dropna().unique())
            
            valid_combinations = set()
            for _, row in df.iterrows():
                if pd.notna(row['MARKET']) and pd.notna(row['BRAND']):
                    valid_combinations.add((row['MARKET'], row['BRAND']))
            
            _valid_markets_brands_cache = {
                'markets': valid_markets,
                'brands': valid_brands,
                'combinations': valid_combinations
            }
            _cache_timestamp = current_time
        else:
            _valid_markets_brands_cache = {
                'markets': set(),
                'brands': set(),
                'combinations': set()
            }
            _cache_timestamp = current_time
            
    except Exception as e:
        print(f"Error fetching valid markets/brands: {str(e)}")
        return {'markets': set(), 'brands': set(), 'combinations': set()}
    
    return _valid_markets_brands_cache

def _validate_market_brand_separately(market, brand):
    """Check if market AND brand exist separately in audio_market_brand_config"""
    valid_data = _get_valid_markets_brands()
    
    market_upper = market.upper().strip()
    brand_upper = brand.upper().strip()
    
    market_valid = market_upper in valid_data['markets']
    brand_valid = brand_upper in valid_data['brands']
    
    return market_valid, brand_valid


# ============================================================================
# ENDPOINT 1: KEY MESSAGES
# ============================================================================

EXCEL_TO_DB_MAPPING_KEY_MSG = {
    "Market": "MARKET",
    "Brand": "BRAND",
    "Message": "VALUE_1"
}

FIXED_INSIGHT_VALUE_KEY_MSG = "mkt-key-messages-categorization-insight"
FIXED_TYPE_VALUE_KEY_MSG = "Market_Intelligence"
TARGET_SHEET_NAME_KEY_MSG = "Brand - Key Messages"
HEADER_ROW_INDEX_KEY_MSG = 2

def _batch_check_duplicates_key_msg(rows):
    """Batch duplicate check for Key Messages endpoint"""
    if not rows:
        return set()
    
    duplicate_set = set()
    chunk_size = 25
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        conditions = []
        
        for r in chunk_rows:
            market = _sq(r["MARKET"].upper())
            brand = _sq(r["BRAND"].upper())
            insight = _sq(r["INSIGHT"])
            type_val = _sq(r["TYPE"])
            value_1 = _sq(r["VALUE_1"])
            
            conditions.append(
                f"(UPPER(TRIM(MARKET)) = '{market}' AND "
                f"UPPER(TRIM(BRAND)) = '{brand}' AND "
                f"TRIM(INSIGHT) = '{insight}' AND "
                f"TRIM(TYPE) = '{type_val}' AND "
                f"TRIM(VALUE_1) = '{value_1}')"
            )
        
        sql = f"""
            SELECT UPPER(TRIM(MARKET)) as MARKET,
                   UPPER(TRIM(BRAND)) as BRAND,
                   TRIM(INSIGHT) as INSIGHT,
                   TRIM(TYPE) as TYPE,
                   TRIM(VALUE_1) as VALUE_1
            FROM {INSIGHT_TABLE}
            WHERE {' OR '.join(conditions)}
        """
        
        try:
            df = dc.execute_query(sql)
            if df is not None and not df.empty:
                for _, row in df.iterrows():
                    duplicate_set.add((
                        row['MARKET'], 
                        row['BRAND'], 
                        row['INSIGHT'],
                        row['TYPE'],
                        row['VALUE_1']
                    ))
        except Exception as e:
            print(f"Batch duplicate check error (chunk {chunk_start//chunk_size + 1}): {str(e)}")
    
    return duplicate_set

def _batch_insert_rows_key_msg(rows, added_by):
    """Batch insert for Key Messages endpoint"""
    if not rows:
        return 0
    
    total_inserted = 0
    chunk_size = 25
    
    for chunk_start in range(0, len(rows), chunk_size):
        chunk_rows = rows[chunk_start:chunk_start + chunk_size]
        
        for r in chunk_rows:
            market_upper = r['MARKET'].upper()
            brand_upper = r['BRAND'].upper()
            
            sql = f"""
                INSERT INTO {INSIGHT_TABLE}
                    (BRAND, MARKET, INSIGHT, TYPE, VALUE_1, VALUE_2, VALUE_3, VALUE_4, ADDED_BY, ENTRY_TIME)
                VALUES
                    ('{_sq(brand_upper)}', '{_sq(market_upper)}', '{_sq(r['INSIGHT'])}', '{_sq(r['TYPE'])}',
                     '{_sq(r['VALUE_1'])}', NULL, NULL, NULL,
                     '{_sq(added_by)}', CURRENT_TIMESTAMP())
            """
            
            try:
                dc.execute_non_query(sql)
                total_inserted += 1
            except Exception as e:
                print(f"Insert error: {str(e)}")
                raise
    
    return total_inserted

def _process_template_excel_key_msg(file_obj):
    """Process Excel template for Key Messages endpoint"""
    try:
        excel_file = pd.ExcelFile(file_obj, engine="openpyxl")
        
        if TARGET_SHEET_NAME_KEY_MSG not in excel_file.sheet_names:
            raise ValueError(
                f"Required sheet '{TARGET_SHEET_NAME_KEY_MSG}' not found. "
                f"Available sheets: {', '.join(excel_file.sheet_names)}"
            )
        
        df = pd.read_excel(
            file_obj, 
            sheet_name=TARGET_SHEET_NAME_KEY_MSG,
            header=HEADER_ROW_INDEX_KEY_MSG,
            engine="openpyxl"
        )
        
        if len(df.columns) > 0:
            df = df.iloc[:, 1:]  # skip first column
        
        df.columns = [str(c).strip() for c in df.columns]
        df = df.replace({pd.NA: "", None: ""}).fillna("")
        
        df['_excel_row_num'] = range(4, 4 + len(df))
        df = df[df.drop('_excel_row_num', axis=1).astype(str).apply(lambda x: x.str.strip().str.len().sum(), axis=1) > 0]
        
        available_excel_cols = set(df.columns) - {'_excel_row_num'}
        expected_excel_cols = set(EXCEL_TO_DB_MAPPING_KEY_MSG.keys())
        missing_cols = expected_excel_cols - available_excel_cols
        
        missing_columns_list = list(missing_cols) if missing_cols else None
        
        mandatory_excel_cols = ["Market", "Brand", "Message"]
        missing_mandatory = [col for col in mandatory_excel_cols if col not in df.columns]
        
        if missing_mandatory:
            raise ValueError(
                f"Missing mandatory columns in '{TARGET_SHEET_NAME_KEY_MSG}' sheet: {', '.join(missing_mandatory)}. "
                f"Found columns: {list(df.columns)}"
            )
        
        mapped_data = []
        
        for idx, row in df.iterrows():
            excel_row_num = int(row['_excel_row_num'])
            mapped_row = {}
            
            for excel_col, db_col in EXCEL_TO_DB_MAPPING_KEY_MSG.items():
                if excel_col in df.columns:
                    value = row.get(excel_col, "")
                    mapped_row[db_col] = _clean(value)
                else:
                    mapped_row[db_col] = ""
            
            mapped_row["INSIGHT"] = FIXED_INSIGHT_VALUE_KEY_MSG
            mapped_row["TYPE"] = FIXED_TYPE_VALUE_KEY_MSG
            mapped_row["VALUE_2"] = None
            mapped_row["VALUE_3"] = None
            mapped_row["VALUE_4"] = None
            
            if (mapped_row.get("MARKET") and 
                mapped_row.get("BRAND") and 
                mapped_row.get("VALUE_1")):
                mapped_data.append((excel_row_num, mapped_row))
        
        if not mapped_data:
            raise ValueError(
                f"No valid data rows found in sheet '{TARGET_SHEET_NAME_KEY_MSG}'. "
                f"Ensure mandatory fields (Market, Brand, Message) have values."
            )
        
        return mapped_data, missing_columns_list
        
    except Exception as e:
        raise Exception(f"Error processing template Excel: {str(e)}")

@app.route("/d_ffn_insight_config_key_msg", methods=["GET"])
def insight_key_msg_get():
    try:
        definitions = {
            "Description": {
                "BRAND": {
                    "Example Values": ["AREXVY", "SHINGRIX"],
                    "Description": "Name of Brand to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "MARKET": {
                    "Example Values": ["GBR", "THA", "ITA", "PRT"],
                    "Description": "Name of the Market to be deployed",
                    "Parameter_type": "Mandatory"
                },
                "INSIGHT": {
                    "Example Values": ["mkt-key-messages-categorization-insight"],
                    "Description": "Fixed value for insight type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "TYPE": {
                    "Example Values": ["Market_Intelligence"],
                    "Description": "Fixed value for type",
                    "Parameter_type": "Auto-filled (Fixed Value)"
                },
                "VALUE_1": {
                    "Example Values": ["Key message text"],
                    "Description": "The actual key message content",
                    "Parameter_type": "Mandatory"
                },
                "VALUE_2": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_3": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "VALUE_4": {
                    "Example Values": ["NULL"],
                    "Description": "Reserved for future use",
                    "Parameter_type": "Auto-filled (NULL)"
                },
                "ADDED_BY": {
                    "Example Values": ["abc.x.abc@gsk.com"],
                    "Description": "Auto-picked from 'Username' header in POST",
                    "Parameter_type": "Mandatory (via header)"
                },
                "ENTRY_TIME": {
                    "Example Values": ["current_timestamp()"],
                    "Description": "Set by system at insert time",
                    "Parameter_type": "Auto-generated"
                }
            },
            "sample_json": {
                "MARKET": "THA",
                "BRAND": "AREXVY",
                "VALUE_1": "This is a key message"
            },
            "excel_upload_instructions": {
                "expected_extension": ".xlsx",
                "template_sheet_name": TARGET_SHEET_NAME_KEY_MSG,
                "form_field_name": "file",
                "required_columns": ["Market", "Brand", "Message"],
                "notes": [
                    "Tablename: hive_metastore.fieldforce_navigator_deployment.d_ffn_insight_config",
                    f"The Excel file must contain a sheet named '{TARGET_SHEET_NAME_KEY_MSG}'",
                    "VALUE_2, VALUE_3, VALUE_4 are automatically set to NULL"
                ]
            }
        }
        return jsonify(definitions), status.HTTP_200_OK
    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR

@app.route("/d_ffn_insight_config_key_msg", methods=["POST"])
def insight_key_msg_post():
    try:
        added_by = _clean(request.headers.get("Username", "") or "")
        if not added_by:
            return jsonify({"error": "Missing 'Username' in headers."}), status.HTTP_400_BAD_REQUEST

        _get_valid_markets_brands(force_refresh=True)

        rows = []
        is_excel_mode = False
        missing_columns_warning = None

        if "file" in request.files:
            is_excel_mode = True
            f = request.files["file"]
            fname = (f.filename or "").lower().strip()
            
            if not fname.endswith(".xlsx"):
                return jsonify({
                    "error": "Only .xlsx files are supported for bulk upload."
                }), status.HTTP_400_BAD_REQUEST

            try:
                rows, missing_columns_warning = _process_template_excel_key_msg(f)
                
                if not rows:
                    return jsonify({
                        "error": f"No valid data rows found in sheet '{TARGET_SHEET_NAME_KEY_MSG}'."
                    }), status.HTTP_400_BAD_REQUEST
                
            except ValueError as ve:
                return jsonify({"error": str(ve)}), status.HTTP_400_BAD_REQUEST
            except Exception as e:
                return jsonify({
                    "error": f"Failed to process Excel template: {str(e)}"
                }), status.HTTP_400_BAD_REQUEST

        else:
            body = request.get_json(force=True, silent=True) or {}
            items = body if isinstance(body, list) else [body]

            for i, rec in enumerate(items, start=1):
                market = _clean(rec.get("MARKET", ""))
                brand  = _clean(rec.get("BRAND", ""))
                value_1 = _clean(rec.get("VALUE_1", ""))

                if not market or not brand or not value_1:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Missing required field(s). MARKET, BRAND, and VALUE_1 are mandatory."
                    }), status.HTTP_400_BAD_REQUEST

                market_valid, brand_valid = _validate_market_brand_separately(market, brand)
                
                if not market_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid MARKET '{market}'. This market does not exist."
                    }), status.HTTP_400_BAD_REQUEST
                
                if not brand_valid:
                    return jsonify({
                        "status": "error",
                        "message": f"Item {i}: Invalid BRAND '{brand}'. This brand does not exist."
                    }), status.HTTP_400_BAD_REQUEST

                rows.append((i, {
                    "MARKET": market,
                    "BRAND": brand,
                    "INSIGHT": FIXED_INSIGHT_VALUE_KEY_MSG,
                    "TYPE": FIXED_TYPE_VALUE_KEY_MSG,
                    "VALUE_1": value_1,
                    "VALUE_2": None,
                    "VALUE_3": None,
                    "VALUE_4": None
                }))

        if not rows:
            return jsonify({"error": "No valid rows to process."}), status.HTTP_400_BAD_REQUEST

        invalid_markets = []
        invalid_brands = []
        valid_rows = []
        
        valid_data = _get_valid_markets_brands()
        
        for row_num, r in rows:
            market = r["MARKET"]
            brand = r["BRAND"]
            
            market_valid = market.upper().strip() in valid_data['markets']
            brand_valid = brand.upper().strip() in valid_data['brands']
            
            if not market_valid:
                invalid_markets.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid MARKET - not found in audio_market_brand_config"
                })
                continue
            
            if not brand_valid:
                invalid_brands.append({
                    "excel_row": row_num,
                    "MARKET": market,
                    "BRAND": brand,
                    "reason": "Invalid BRAND - not found in audio_market_brand_config"
                })
                continue
            
            valid_rows.append((row_num, r))
        
        duplicate_set = _batch_check_duplicates_key_msg([r for _, r in valid_rows])
        
        duplicates = []
        insertable_rows = []
        
        for row_num, r in valid_rows:
            key = (
                r["MARKET"].upper().strip(),
                r["BRAND"].upper().strip(),
                r["INSIGHT"].strip(),
                r["TYPE"].strip(),
                r["VALUE_1"].strip()
            )
            
            if key in duplicate_set:
                duplicates.append({
                    "excel_row": row_num,
                    "MARKET": r["MARKET"],
                    "BRAND": r["BRAND"],
                    "VALUE_1": r["VALUE_1"],
                    "reason": "Duplicate - already exists in database"
                })
            else:
                insertable_rows.append(r)
        
        inserted = 0
        errors = []
        
        if insertable_rows:
            try:
                inserted = _batch_insert_rows_key_msg(insertable_rows, added_by)
            except Exception as e:
                errors.append({
                    "error": f"Batch insert failed: {str(e)}"
                })

        response = {
            "status": "success" if inserted and not errors else ("partial" if inserted else "no-change"),
            "mode": "excel-template" if is_excel_mode else "json",
            "sheet_processed": TARGET_SHEET_NAME_KEY_MSG if is_excel_mode else None,
            "total_rows_processed": len(rows),
            "inserted_count": inserted,
            "duplicate_count": len(duplicates),
            "validation_errors_count": len(invalid_markets) + len(invalid_brands),
            "skipped_duplicates": duplicates,
            "invalid_markets": invalid_markets,
            "invalid_brands": invalid_brands,
            "row_errors": errors
        }
        
        if is_excel_mode and missing_columns_warning:
            response["warning"] = {
                "message": "The uploaded Excel template is missing some required columns.",
                "missing_columns": missing_columns_warning,
                "note": "All three columns (Market, Brand, Message) are required for Insight Config."
            }
        
        response = {k: v for k, v in response.items() 
                   if v is not None and not (isinstance(v, list) and len(v) == 0)}
        
        if inserted > 0:
            status_code = status.HTTP_201_CREATED
        elif invalid_markets or invalid_brands:
            status_code = status.HTTP_207_MULTI_STATUS
        else:
            status_code = status.HTTP_200_OK
        
        return jsonify(response), status_code

    except Exception as e:
        return jsonify({"error": f"Internal server error: {str(e)}"}), status.HTTP_500_INTERNAL_SERVER_ERROR		
		
		
		
		
		
		
		
		
