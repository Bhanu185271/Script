
import pandas as pd

# Define columns to select
excel_columns = ["AudioID", "QuestionID", "QuestionText"]
llm_columns = ["AudioID", "LOCAL_LANGUAGE_AGREED_ACTIONS", "LOCAL_LANGUAGE_SUMMARY", "OVERALL_AREA_OF_INTEREST_INSIGHT", "OVERALL_OBJECTIONS"]
pre_processed_columns = ["AudioID", "BRAND", "MARKET", "TRANSCRIPT_ID"]

# Read Excel file
file_path = "/Workspace/Users/chandrabhanu.x.chandrabhanu@gsk.com/Feedback_Report.xlsx"
df_excel = pd.read_excel(file_path, sheet_name="Feedback Data", nrows=5000)

# Select required columns from Excel
excel_data = df_excel[excel_columns]

# Read Databricks tables as pandas DataFrames
llm_output = spark.table("fieldforce_navigator_uat.llm_audio_insights_output").toPandas()
pre_processed = spark.table("fieldforce_navigator_uat.audio_pre_processed_output").toPandas()

# Select required columns
llm_data = llm_output[llm_columns]
pre_data = pre_processed[pre_processed_columns]

# Join the dataframes
result1 = excel_data.merge(llm_data, on="AudioID", how="inner")
final_result = result1.merge(pre_data, on="AudioID", how="inner")

print(f"Final result shape: {final_result.shape}")
final_result.head()














import pandas as pd
from pyspark.sql import SparkSession

# Define columns
excel_columns = ["AudioID", "QuestionID", "QuestionText"] 
table1_columns = ["AudioID", "LOCAL_LANGUAGE_AGREED_ACTIONS", "LOCAL_LANGUAGE_SUMMARY","OVERALL_AREA_OF_INTEREST_INSIGHT", "OVERALL_OBJECTIONS"] 
table2_columns = ["AudioID", "BRAND", "MARKET","TRANSCRIPT_ID"]

# File path
file_path = "/Workspace/Users/chandrabhanu.x.chandrabhanu@gsk.com/Feedback_Report.xlsx"
temp_csv_path = "/tmp/temp_excel_data.csv"

# Read Excel with pandas and save as CSV for Spark
df_temp = pd.read_excel(file_path, sheet_name="Feedback Data", nrows=5000)
df_temp.to_csv(temp_csv_path, index=False)

# Read CSV with Spark and select columns
spark_df_excel = spark.read.csv(temp_csv_path, header=True, inferSchema=True) \
    .select(excel_columns)

# Read Databricks tables and select specific columns
llm_output = spark.table("fieldforce_navigator_uat.llm_audio_insights_output").select(table1_columns)
pre_processed = spark.table("fieldforce_navigator_uat.audio_pre_processed_output").select(table2_columns)

# Join Excel with llm_output on AudioID
result1 = spark_df_excel.join(llm_output, on="AudioID", how="inner")

# Join the result with pre_processed on AudioID
final_result = result1.join(pre_processed, on="AudioID", how="inner")

# Display results
print(f"Excel rows: {spark_df_excel.count()}")
print(f"LLM output rows: {llm_output.count()}")
print(f"Pre-processed rows: {pre_processed.count()}")
print(f"Final joined rows: {final_result.count()}")

final_result.show()

# Clean up temp file
import os
os.remove(temp_csv_path)











from pyspark.sql import SparkSession

# Define columns
excel_columns =  ["AudioID", "QuestionID", "QuestionText"] 
table1_columns = ["AudioID", "LOCAL_LANGUAGE_AGREED_ACTIONS", "LOCAL_LANGUAGE_SUMMARY","OVERALL_AREA_OF_INTEREST_INSIGHT", "OVERALL_OBJECTIONS"] 
table2_columns = ["AudioID", "BRAND", "MARKET","TRANSCRIPT_ID"]  



file_path =  "/Workspace/Users/chandrabhanu.x.chandrabhanu@gsk.com/Feedback_Report.xlsx"
spark_df_excel = spark.read.format("com.crealytics.spark.excel") \
    .option("dataAddress", "Feedback Data!A1") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load(file_path) \
    .limit(5000) \
    .select(excel_columns)

# Read Databricks tables and select specific columns
llm_output = spark.table("fieldforce_navigator_uat.llm_audio_insights_output").select(table1_columns)
pre_processed = spark.table("fieldforce_navigator_uat.audio_pre_processed_output").select(table2_columns)

# Join Excel with Table1 on AudioID
result1 = spark_df_excel.join(llm_output, on="AudioID", how="inner")

# Join the result with Table2 on AudioID
final_result = result1.join(pre_processed, on="AudioID", how="inner")

# Display results
print(f"Excel rows: {spark_df_excel.count()}")
print(f"Table1 rows: {llm_output.count()}")
print(f"Table2 rows: {pre_processed.count()}")
print(f"Final joined rows: {final_result.count()}")

final_result.show()




















from pyspark.sql import SparkSession

# Define columns to select from each source
excel_columns = ["AudioID", "column1", "column2"]  # Replace with your desired columns
table1_columns = ["AudioID", "column3", "column4"]  # Replace with your desired columns  
table2_columns = ["AudioID", "column5", "column6"]  # Replace with your desired columns

# Read Excel file with PySpark
file_path = "/Workspace/Users/alex@grb.com/sample.xlsx"
spark_df_excel = spark.read.format("com.crealytics.spark.excel") \
    .option("dataAddress", "Feedback Data!A1") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load(file_path) \
    .limit(5000) \
    .select(excel_columns)

# Read Databricks tables and select specific columns
table1 = spark.table("hub_order.Table1").select(table1_columns)
table2 = spark.table("hub_order.Table2").select(table2_columns)

# Join Excel with Table1 on AudioID
result1 = spark_df_excel.join(table1, on="AudioID", how="inner")

# Join the result with Table2 on AudioID
final_result = result1.join(table2, on="AudioID", how="inner")

# Display results
print(f"Excel rows: {spark_df_excel.count()}")
print(f"Table1 rows: {table1.count()}")
print(f"Table2 rows: {table2.count()}")
print(f"Final joined rows: {final_result.count()}")

final_result.show()








import pandas as pd
from pyspark.sql import SparkSession

# Define columns to select from each source
excel_columns = ["AudioID", "column1", "column2"]  # Replace with your desired columns
table1_columns = ["AudioID", "column3", "column4"]  # Replace with your desired columns  
table2_columns = ["AudioID", "column5", "column6"]  # Replace with your desired columns

# Read Excel file and select specific columns
file_path = "/Workspace/Users/alex@grb.com/sample.xlsx"
df_excel = pd.read_excel(file_path, sheet_name="Feedback Data", nrows=5000)
df_excel_selected = df_excel[excel_columns]

# Convert pandas DataFrame to Spark DataFrame
spark_df_excel = spark.createDataFrame(df_excel_selected)

# Read Databricks tables and select specific columns
table1 = spark.table("hub_order.Table1").select(table1_columns)
table2 = spark.table("hub_order.Table2").select(table2_columns)

# Join Excel with Table1 on AudioID
result1 = spark_df_excel.join(table1, on="AudioID", how="inner")

# Join the result with Table2 on AudioID
final_result = result1.join(table2, on="AudioID", how="inner")

# Display results
print(f"Excel rows: {spark_df_excel.count()}")
print(f"Table1 rows: {table1.count()}")
print(f"Table2 rows: {table2.count()}")
print(f"Final joined rows: {final_result.count()}")

final_result.show()










# Install openpyxl (needed for reading .xlsx files with pandas)
%pip install openpyxl

import pandas as pd

# 1. Search for your uploaded file inside /FileStore
#    (You can adjust "sample.xlsx" if your filename is different)
file_name = "sample.xlsx"

# List all files in FileStore
files = dbutils.fs.ls("/FileStore/")

# Check if file exists at root
file_path = None
for f in files:
    if f.name == file_name:
        file_path = "/dbfs/FileStore/" + f.name
        break

# If not found at root, check /FileStore/tables/
if file_path is None:
    files = dbutils.fs.ls("/FileStore/tables/")
    for f in files:
        if f.name == file_name:
            file_path = "/dbfs/FileStore/tables/" + f.name
            break

if file_path is None:
    raise FileNotFoundError(f"{file_name} not found in /FileStore/ or /FileStore/tables/")

print("âœ… File found at:", file_path)

# 2. Load the Excel file
excel_file = pd.ExcelFile(file_path)

# 3. Print sheet info
print("Number of sheets:", len(excel_file.sheet_names))
print("Sheet names:", excel_file.sheet_names)

# 4. Read the first sheet as DataFrame
df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])

# 5. Show some data
display(df)
