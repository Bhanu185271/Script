
#Fixing the quadrants based on forecast
from pyspark.sql.window import Window
roundOffDecimals = 2

fixInitialQuadSDF = (
  marketSharePerXOitItemSDF.drop('LocSegment','Territory').alias('ms')
  #.join(other=metricsOitItemSDF.alias('f'), on= [granularity_level,'OitItem'], how='left')
#   .join(other=spark.table(f"""{db_name}.clientsListRawFull""").alias('cl'), on=['OitItem', 'PharmacyId'], how='left')
  .join(other=clients.alias('cl'), on=['OitItem', granularity_level], how='left')
  #.join(clients.alias('cl2'), (col('ms.Brand') == col('cl2.OitItem')) & (col(granularity_level) == col('cl2.' + granularity_level)), how='left')
  .join(other=commercial.withColumnRenamed('Quadrant', 'BaselineQuadrant').
        
  withColumn('Channel', when(col("Channel")=="Independent","Pharmacy").otherwise(col("Channel")))

        .withColumnRenamed('MinMktshare', 'MinVolShare')
        .withColumnRenamed('MaxMktshare', 'MaxVolShare').alias('cr'), 
        on=['OitItem', 'Channel', 'LocSegment', 'Territory'], how='inner')
  .where('(ms.capped_Baseline_Oititem >= QuadMinMktVol and ms.capped_Baseline_Oititem <= QuadMaxMktVol) and \
          (FcstOitItemMktShare >= QuadMinShare and FcstOitItemMktShare <= QuadMaxShare)')
  .select('ms.' + granularity_level, 'cl.NumOfPos', 'ms.OitItem', 'ms.Brand', 'cl.Channel', 'cl.LocSegment', 'cl.Territory',
          'OitItemToBrandWeight', 'ms.SPLY_Oititem', 'ms.SPLYGSK_Oititem','ms.capped_Baseline_Oititem','Fcst_gsk_vol_Oititem',
          'FcstOitItemMktShare','BaselineQuadrant', 'MoleculeCtcMktGrowth', 'BrandVolGrowthTarget',
          'MinVolShare', 'TargetShare', 'Target_MS_StepNum', 'MaxVolShare', 'MinVolPerPos', 'MaxVolPerPos', 'QuadMaxShare', 'QuadMinShare',
          'BenchmarkShare', 'BenchmarkCarryover','AdditionalProfit','MinGrossMargin',
          'MinDiscQuadMinMktShare', 'MinDiscQuadMaxMktShare', 'MaxDisc', 'MinDiscDiff',
#           'cr.QuadMinMktVol', 'cr.QuadMaxMktVol', 'cr.QuadMinShare', 'cr.QuadMaxShare',
          'cr.MinVolForMaxRangePerPos', 'cr.MinOffers', 'cr.ReferencePrice', 'cr.diminishing_gp_offers', 'cr.Offer_Period_Ref_Vol', 'cr.GP_Ref_Vol'
         ).distinct().alias('l')
  
  .join(other=oitItemPricesPerXSDF.alias('oip'), on= [granularity_level,'OitItem'], how='inner')
  .select('l.*', 'OitItemPTP','OitItemPTC','OitItemPTW','oip.OitItemNetPrice', 'oip.OitItemPrice', 'oip.OitItemNetCogs',\
          'oip.OitItemVAT', 'oip.OitItemVar_InvoiceValueBaseDiscount','oip.OitItem_PTW_Without_VAT', 'oip.GrossMargin')

).withColumn('rank', row_number().over(Window.partitionBy('OitItem', granularity_level, 'Channel', 'LocSegment', 'Territory', ).orderBy('BaselineQuadrant'))).where('rank = 1').drop('rank')

fixInitialQuadSDF.display()



























''''''''''''''''''''''''''''''''''''
Select 
concat(Brand,'-', Market,'-', Insight, '-',Type, '-',Value_1) as clm,  count(*) as count, Added_by, ENTRY_TIME
from hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config group by concat(Brand,'-', Market,'-', Insight, '-',Type, '-',Value_1), Added_by, ENTRY_TIME having count(*) > 1



DELETE FROM hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config
WHERE ENTRY_TIME IN (
    SELECT ENTRY_TIME
    FROM (
        SELECT 
            ENTRY_TIME,
            ROW_NUMBER() OVER (
                PARTITION BY Brand, Market, Insight, Type, Value_1, Added_by, ENTRY_TIME 
                ORDER BY ENTRY_TIME
            ) as rn
        FROM hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config
    ) t
    WHERE rn > 1
);


DELETE FROM hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config AS target
WHERE EXISTS (
    SELECT 1
    FROM (
        SELECT 
            Brand, Market, Insight, Type, Value_1, ENTRY_TIME,
            ROW_NUMBER() OVER (
                PARTITION BY Brand, Market, Insight, Type, Value_1
                ORDER BY ENTRY_TIME ASC  -- keeps the earliest record
            ) as rn
        FROM hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config
    ) dup
    WHERE dup.Brand = target.Brand
        AND dup.Market = target.Market
        AND dup.Insight = target.Insight
        AND dup.Type = target.Type
        AND dup.Value_1 = target.Value_1
        AND dup.ENTRY_TIME = target.ENTRY_TIME
        AND dup.rn > 1
);




















"""
Module to check authorization for admin routes
"""
from flask import request
import pkg_resources
import io
import yaml
from databricks import sql as dbsql
from Api import DatabricksToken as dt

cfile_global = pkg_resources.resource_filename('ApiData','/Global/Config/global_config.yml')

with io.open(cfile_global, 'r') as configfile_global:
    global_config_data = yaml.safe_load(configfile_global)

def check_role(fun):
    """
    This is function applied on admin routes to check
    whether the request databricks token is of an admin or not
    """
    def auth_decorator(*args, **kwargs):
        try:
            # db_token = request.headers.get('dbToken')
            client_id =  request.headers.get('Clientid')
            client_secret =  request.headers.get('Clientsecret')
            tenantId = global_config_data['tenantId']
            subscription_id = global_config_data['subscription_id']
            resource_group = global_config_data['resource_group']
            databricks_workspace = global_config_data['databricks_workspace']
            dbricks_location = global_config_data['dbricks_location']

            databricks_token = dt.DatabricksToken(client_id,client_secret,tenantId, subscription_id,
                                                        resource_group, databricks_workspace, dbricks_location)
                                
            connection = dbsql.connect( 
                                        server_hostname = global_config_data['server-hostname'],
                                        http_path = global_config_data['http-path'],
                                        access_token = databricks_token
                                        )
                                        
        

    # check if client_id is added in the access_table and retrieve role
            query_string = "SHOW TABLES IN admin"
            with connection.cursor() as cursor: 
                # Get the data 
                cursor.execute(query_string) 
                objects = cursor.fetchall()
            
            values = [value[1] for value in objects]
            
            if len(values) > 0:
                return fun(*args, **kwargs)
            else:
                return "This is an admin route please check your access."
        except Exception as exception:
            return 'Error in connection to Databricks due to '+str(exception)

    auth_decorator.__name__ = fun.__name__

    return auth_decorator
