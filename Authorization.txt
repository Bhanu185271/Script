# Fixing the quadrants based on forecast
from pyspark.sql.window import Window
from pyspark.sql.functions import col, when, row_number, coalesce

roundOffDecimals = 2

# TEST: Filter for Brand = 'abhi' to debug join issues
test_brand = 'abhi'

fixInitialQuadSDF = (
    marketSharePerXOitItemSDF.drop('LocSegment', 'Territory')
    .where(col('Brand') == test_brand)  # TEST FILTER
    .alias('ms')
    
    # Join 1: OitItem-level join with clients
    .join(
        other=clients.alias('cl'), 
        on=[
            col('ms.OitItem') == col('cl.OitItem'),
            col(f'ms.{granularity_level}') == col(f'cl.{granularity_level}')
        ], 
        how='left'
    )
    
    # Join 2: Brand-level join with clients (previously optional)
    .join(
        clients.alias('cl2'), 
        on=[
            col('ms.Brand') == col('cl2.OitItem'),
            col(f'ms.{granularity_level}') == col(f'cl2.{granularity_level}')
        ], 
        how='left'
    )
    
    # Fix 2: Ensure commercial join has proper column references
    # Using COALESCE to prioritize cl data, fallback to cl2 data
    .join(
        other=commercial
            .withColumnRenamed('Quadrant', 'BaselineQuadrant')
            .withColumn('Channel', 
                       when(col("Channel") == "Independent", "Pharmacy")
                       .otherwise(col("Channel")))
            .withColumnRenamed('MinMktshare', 'MinVolShare')
            .withColumnRenamed('MaxMktshare', 'MaxVolShare')
            .alias('cr'), 
        on=[
            col('ms.OitItem') == col('cr.OitItem'),
            # Use coalesced values from both client joins
            coalesce(col('cl.Channel'), col('cl2.Channel')) == col('cr.Channel'),
            coalesce(col('cl.LocSegment'), col('cl2.LocSegment')) == col('cr.LocSegment'),
            coalesce(col('cl.Territory'), col('cl2.Territory')) == col('cr.Territory')
        ], 
        how='inner'
    )
    
    # Fix 3: Add alias prefixes to filter columns
    .where(
        (col('ms.capped_Baseline_Oititem') >= col('cr.QuadMinMktVol')) &
        (col('ms.capped_Baseline_Oititem') <= col('cr.QuadMaxMktVol')) &
        (col('ms.FcstOitItemMktShare') >= col('cr.QuadMinShare')) &
        (col('ms.FcstOitItemMktShare') <= col('cr.QuadMaxShare'))
    )
    
    # Select with COALESCE to merge data from both client joins
    # Priority: cl (OitItem-level) > cl2 (Brand-level)
    .select(
        col(f'ms.{granularity_level}').alias(granularity_level),
        coalesce(col('cl.NumOfPos'), col('cl2.NumOfPos')).alias('NumOfPos'),
        'ms.OitItem',
        'ms.Brand',
        coalesce(col('cl.Channel'), col('cl2.Channel')).alias('Channel'),
        coalesce(col('cl.LocSegment'), col('cl2.LocSegment')).alias('LocSegment'),
        coalesce(col('cl.Territory'), col('cl2.Territory')).alias('Territory'),
        'ms.OitItemToBrandWeight',
        'ms.SPLY_Oititem',
        'ms.SPLYGSK_Oititem',
        'ms.capped_Baseline_Oititem',
        'ms.Fcst_gsk_vol_Oititem',
        'ms.FcstOitItemMktShare',
        'cr.BaselineQuadrant',
        'cr.MoleculeCtcMktGrowth',
        'cr.BrandVolGrowthTarget',
        'cr.MinVolShare',
        'cr.TargetShare',
        'cr.Target_MS_StepNum',
        'cr.MaxVolShare',
        'cr.MinVolPerPos',
        'cr.MaxVolPerPos',
        'cr.QuadMaxShare',
        'cr.QuadMinShare',
        'cr.BenchmarkShare',
        'cr.BenchmarkCarryover',
        'cr.AdditionalProfit',
        'cr.MinGrossMargin',
        'cr.MinDiscQuadMinMktShare',
        'cr.MinDiscQuadMaxMktShare',
        'cr.MaxDisc',
        'cr.MinDiscDiff',
        'cr.MinVolForMaxRangePerPos',
        'cr.MinOffers',
        'cr.ReferencePrice',
        'cr.diminishing_gp_offers',
        'cr.Offer_Period_Ref_Vol',
        'cr.GP_Ref_Vol'
    )
    .distinct()
    .alias('l')
    
    .join(
        other=oitItemPricesPerXSDF.alias('oip'), 
        on=[
            col(f'l.{granularity_level}') == col(f'oip.{granularity_level}'),
            col('l.OitItem') == col('oip.OitItem')
        ], 
        how='inner'
    )
    
    .select(
        'l.*',
        'oip.OitItemPTP',
        'oip.OitItemPTC',
        'oip.OitItemPTW',
        'oip.OitItemNetPrice',
        'oip.OitItemPrice',
        'oip.OitItemNetCogs',
        'oip.OitItemVAT',
        'oip.OitItemVar_InvoiceValueBaseDiscount',
        'oip.OitItem_PTW_Without_VAT',
        'oip.GrossMargin'
    )
    
    # Fix 4: Ensure window function uses proper column references
    .withColumn(
        'rank', 
        row_number().over(
            Window
            .partitionBy('OitItem', granularity_level, 'Channel', 'LocSegment', 'Territory')
            .orderBy('BaselineQuadrant')
        )
    )
    .where(col('rank') == 1)
    .drop('rank')
)

fixInitialQuadSDF.display()




















'''''''''''''''''''''''''''''''''''''''''''''''''''''''''
# Fixing the quadrants based on forecast
from pyspark.sql.window import Window
from pyspark.sql.functions import col, when, row_number

roundOffDecimals = 2

fixInitialQuadSDF = (
    marketSharePerXOitItemSDF.drop('LocSegment', 'Territory').alias('ms')
    
    # Fix 1: Use explicit column references with aliases
    .join(
        other=clients.alias('cl'), 
        on=[
            col('ms.OitItem') == col('cl.OitItem'),
            col(f'ms.{granularity_level}') == col(f'cl.{granularity_level}')
        ], 
        how='left'
    )
    
    # Optional: If you need brand-level join, uncomment and fix:
    # .join(
    #     clients.alias('cl2'), 
    #     on=[
    #         col('ms.Brand') == col('cl2.OitItem'),
    #         col(f'ms.{granularity_level}') == col(f'cl2.{granularity_level}')
    #     ], 
    #     how='left'
    # )
    
    # Fix 2: Ensure commercial join has proper column references
    .join(
        other=commercial
            .withColumnRenamed('Quadrant', 'BaselineQuadrant')
            .withColumn('Channel', 
                       when(col("Channel") == "Independent", "Pharmacy")
                       .otherwise(col("Channel")))
            .withColumnRenamed('MinMktshare', 'MinVolShare')
            .withColumnRenamed('MaxMktshare', 'MaxVolShare')
            .alias('cr'), 
        on=[
            col('ms.OitItem') == col('cr.OitItem'),
            col('cl.Channel') == col('cr.Channel'),
            col('cl.LocSegment') == col('cr.LocSegment'),
            col('cl.Territory') == col('cr.Territory')
        ], 
        how='inner'
    )
    
    # Fix 3: Add alias prefixes to filter columns
    .where(
        (col('ms.capped_Baseline_Oititem') >= col('cr.QuadMinMktVol')) &
        (col('ms.capped_Baseline_Oititem') <= col('cr.QuadMaxMktVol')) &
        (col('ms.FcstOitItemMktShare') >= col('cr.QuadMinShare')) &
        (col('ms.FcstOitItemMktShare') <= col('cr.QuadMaxShare'))
    )
    
    .select(
        col(f'ms.{granularity_level}').alias(granularity_level),
        'cl.NumOfPos',
        'ms.OitItem',
        'ms.Brand',
        'cl.Channel',
        'cl.LocSegment',
        'cl.Territory',
        'ms.OitItemToBrandWeight',
        'ms.SPLY_Oititem',
        'ms.SPLYGSK_Oititem',
        'ms.capped_Baseline_Oititem',
        'ms.Fcst_gsk_vol_Oititem',
        'ms.FcstOitItemMktShare',
        'cr.BaselineQuadrant',
        'cr.MoleculeCtcMktGrowth',
        'cr.BrandVolGrowthTarget',
        'cr.MinVolShare',
        'cr.TargetShare',
        'cr.Target_MS_StepNum',
        'cr.MaxVolShare',
        'cr.MinVolPerPos',
        'cr.MaxVolPerPos',
        'cr.QuadMaxShare',
        'cr.QuadMinShare',
        'cr.BenchmarkShare',
        'cr.BenchmarkCarryover',
        'cr.AdditionalProfit',
        'cr.MinGrossMargin',
        'cr.MinDiscQuadMinMktShare',
        'cr.MinDiscQuadMaxMktShare',
        'cr.MaxDisc',
        'cr.MinDiscDiff',
        'cr.MinVolForMaxRangePerPos',
        'cr.MinOffers',
        'cr.ReferencePrice',
        'cr.diminishing_gp_offers',
        'cr.Offer_Period_Ref_Vol',
        'cr.GP_Ref_Vol'
    )
    .distinct()
    .alias('l')
    
    .join(
        other=oitItemPricesPerXSDF.alias('oip'), 
        on=[
            col(f'l.{granularity_level}') == col(f'oip.{granularity_level}'),
            col('l.OitItem') == col('oip.OitItem')
        ], 
        how='inner'
    )
    
    .select(
        'l.*',
        'oip.OitItemPTP',
        'oip.OitItemPTC',
        'oip.OitItemPTW',
        'oip.OitItemNetPrice',
        'oip.OitItemPrice',
        'oip.OitItemNetCogs',
        'oip.OitItemVAT',
        'oip.OitItemVar_InvoiceValueBaseDiscount',
        'oip.OitItem_PTW_Without_VAT',
        'oip.GrossMargin'
    )
    
    # Fix 4: Ensure window function uses proper column references
    .withColumn(
        'rank', 
        row_number().over(
            Window
            .partitionBy('OitItem', granularity_level, 'Channel', 'LocSegment', 'Territory')
            .orderBy('BaselineQuadrant')
        )
    )
    .where(col('rank') == 1)
    .drop('rank')
)

fixInitialQuadSDF.display()























#Fixing the quadrants based on forecast
from pyspark.sql.window import Window
roundOffDecimals = 2

fixInitialQuadSDF = (
  marketSharePerXOitItemSDF.drop('LocSegment','Territory').alias('ms')
  #.join(other=metricsOitItemSDF.alias('f'), on= [granularity_level,'OitItem'], how='left')
#   .join(other=spark.table(f"""{db_name}.clientsListRawFull""").alias('cl'), on=['OitItem', 'PharmacyId'], how='left')
  .join(other=clients.alias('cl'), on=['OitItem', granularity_level], how='left')
  #.join(clients.alias('cl2'), (col('ms.Brand') == col('cl2.OitItem')) & (col(granularity_level) == col('cl2.' + granularity_level)), how='left')
  .join(other=commercial.withColumnRenamed('Quadrant', 'BaselineQuadrant').
        
  withColumn('Channel', when(col("Channel")=="Independent","Pharmacy").otherwise(col("Channel")))

        .withColumnRenamed('MinMktshare', 'MinVolShare')
        .withColumnRenamed('MaxMktshare', 'MaxVolShare').alias('cr'), 
        on=['OitItem', 'Channel', 'LocSegment', 'Territory'], how='inner')
  .where('(ms.capped_Baseline_Oititem >= QuadMinMktVol and ms.capped_Baseline_Oititem <= QuadMaxMktVol) and \
          (FcstOitItemMktShare >= QuadMinShare and FcstOitItemMktShare <= QuadMaxShare)')
  .select('ms.' + granularity_level, 'cl.NumOfPos', 'ms.OitItem', 'ms.Brand', 'cl.Channel', 'cl.LocSegment', 'cl.Territory',
          'OitItemToBrandWeight', 'ms.SPLY_Oititem', 'ms.SPLYGSK_Oititem','ms.capped_Baseline_Oititem','Fcst_gsk_vol_Oititem',
          'FcstOitItemMktShare','BaselineQuadrant', 'MoleculeCtcMktGrowth', 'BrandVolGrowthTarget',
          'MinVolShare', 'TargetShare', 'Target_MS_StepNum', 'MaxVolShare', 'MinVolPerPos', 'MaxVolPerPos', 'QuadMaxShare', 'QuadMinShare',
          'BenchmarkShare', 'BenchmarkCarryover','AdditionalProfit','MinGrossMargin',
          'MinDiscQuadMinMktShare', 'MinDiscQuadMaxMktShare', 'MaxDisc', 'MinDiscDiff',
#           'cr.QuadMinMktVol', 'cr.QuadMaxMktVol', 'cr.QuadMinShare', 'cr.QuadMaxShare',
          'cr.MinVolForMaxRangePerPos', 'cr.MinOffers', 'cr.ReferencePrice', 'cr.diminishing_gp_offers', 'cr.Offer_Period_Ref_Vol', 'cr.GP_Ref_Vol'
         ).distinct().alias('l')
  
  .join(other=oitItemPricesPerXSDF.alias('oip'), on= [granularity_level,'OitItem'], how='inner')
  .select('l.*', 'OitItemPTP','OitItemPTC','OitItemPTW','oip.OitItemNetPrice', 'oip.OitItemPrice', 'oip.OitItemNetCogs',\
          'oip.OitItemVAT', 'oip.OitItemVar_InvoiceValueBaseDiscount','oip.OitItem_PTW_Without_VAT', 'oip.GrossMargin')

).withColumn('rank', row_number().over(Window.partitionBy('OitItem', granularity_level, 'Channel', 'LocSegment', 'Territory', ).orderBy('BaselineQuadrant'))).where('rank = 1').drop('rank')

fixInitialQuadSDF.display()



























''''''''''''''''''''''''''''''''''''
Select 
concat(Brand,'-', Market,'-', Insight, '-',Type, '-',Value_1) as clm,  count(*) as count, Added_by, ENTRY_TIME
from hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config group by concat(Brand,'-', Market,'-', Insight, '-',Type, '-',Value_1), Added_by, ENTRY_TIME having count(*) > 1



DELETE FROM hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config
WHERE ENTRY_TIME IN (
    SELECT ENTRY_TIME
    FROM (
        SELECT 
            ENTRY_TIME,
            ROW_NUMBER() OVER (
                PARTITION BY Brand, Market, Insight, Type, Value_1, Added_by, ENTRY_TIME 
                ORDER BY ENTRY_TIME
            ) as rn
        FROM hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config
    ) t
    WHERE rn > 1
);


DELETE FROM hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config AS target
WHERE EXISTS (
    SELECT 1
    FROM (
        SELECT 
            Brand, Market, Insight, Type, Value_1, ENTRY_TIME,
            ROW_NUMBER() OVER (
                PARTITION BY Brand, Market, Insight, Type, Value_1
                ORDER BY ENTRY_TIME ASC  -- keeps the earliest record
            ) as rn
        FROM hive_metastore.fieldforce_navigator_uat.d_ffn_insight_config
    ) dup
    WHERE dup.Brand = target.Brand
        AND dup.Market = target.Market
        AND dup.Insight = target.Insight
        AND dup.Type = target.Type
        AND dup.Value_1 = target.Value_1
        AND dup.ENTRY_TIME = target.ENTRY_TIME
        AND dup.rn > 1
);




















"""
Module to check authorization for admin routes
"""
from flask import request
import pkg_resources
import io
import yaml
from databricks import sql as dbsql
from Api import DatabricksToken as dt

cfile_global = pkg_resources.resource_filename('ApiData','/Global/Config/global_config.yml')

with io.open(cfile_global, 'r') as configfile_global:
    global_config_data = yaml.safe_load(configfile_global)

def check_role(fun):
    """
    This is function applied on admin routes to check
    whether the request databricks token is of an admin or not
    """
    def auth_decorator(*args, **kwargs):
        try:
            # db_token = request.headers.get('dbToken')
            client_id =  request.headers.get('Clientid')
            client_secret =  request.headers.get('Clientsecret')
            tenantId = global_config_data['tenantId']
            subscription_id = global_config_data['subscription_id']
            resource_group = global_config_data['resource_group']
            databricks_workspace = global_config_data['databricks_workspace']
            dbricks_location = global_config_data['dbricks_location']

            databricks_token = dt.DatabricksToken(client_id,client_secret,tenantId, subscription_id,
                                                        resource_group, databricks_workspace, dbricks_location)
                                
            connection = dbsql.connect( 
                                        server_hostname = global_config_data['server-hostname'],
                                        http_path = global_config_data['http-path'],
                                        access_token = databricks_token
                                        )
                                        
        

    # check if client_id is added in the access_table and retrieve role
            query_string = "SHOW TABLES IN admin"
            with connection.cursor() as cursor: 
                # Get the data 
                cursor.execute(query_string) 
                objects = cursor.fetchall()
            
            values = [value[1] for value in objects]
            
            if len(values) > 0:
                return fun(*args, **kwargs)
            else:
                return "This is an admin route please check your access."
        except Exception as exception:
            return 'Error in connection to Databricks due to '+str(exception)

    auth_decorator.__name__ = fun.__name__

    return auth_decorator
